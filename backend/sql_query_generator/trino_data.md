# Trino Documentation
Generated on: 2025-03-29 07:15:09S
Generated by: pranay
Base URL: https://trino.io/docs/current/

## Table of Contents


# Trino Documentation Index


## Overview

### Use Cases Documentation
Source: https://trino.io/docs/current/overview/use-cases.html

Use cases# This section puts Trino into perspective, so that prospective administrators and end users know what to expect from Trino. What Trino is not# Since Trino is being called a database by many members of the community, it makes sense to begin with a definition of what Trino is not. Do not mistake the fact that Trino understands SQL with it providing the features of a standard database. Trino is not a general-purpose relational database. It is not a replacement for databases like MySQL, PostgreSQL or Oracle. Trino was not designed to handle Online Transaction Processing (OLTP). This is also true for many other databases designed and optimized for data warehousing or analytics. What Trino is# Trino is a tool designed to efficiently query vast amounts of data using distributed queries. If you work with terabytes or petabytes of data, you are likely using tools that interact with Hadoop and HDFS. Trino was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Trino is not limited to accessing HDFS. Trino can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Trino was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP).

---

### Trino Concepts Documentation
Source: https://trino.io/docs/current/overview/concepts.html

Trino concepts# Overview# To understand Trino, you must first understand the terms and concepts used throughout the Trino documentation. While it is easy to understand statements and queries, as an end-user you should have familiarity with concepts such as stages and splits to take full advantage of Trino to execute efficient queries. As a Trino administrator or a Trino contributor you should understand how Trino’s concepts of stages map to tasks and how tasks contain a set of drivers which process data. This section provides a solid definition for the core concepts referenced throughout Trino, and these sections are sorted from most general to most specific. Note The book Trino: The Definitive Guide and the research paper Presto: SQL on Everything can provide further information about Trino and the concepts in use. Architecture# Trino is a distributed query engine that processes data in parallel across multiple servers. There are two types of Trino servers, coordinators and workers. The following sections describe these servers and other components of Trino’s architecture. Cluster# A Trino cluster consists of several Trino nodes - one coordinator and zero or more workers. Users connect to the coordinator with their SQL query tool. The coordinator collaborates with the workers. The coordinator and the workers access the connected data sources. This access is configured in catalogs. Processing each query is a stateful operation. The workload is orchestrated by the coordinator and spread parallel across all workers in the cluster. Each node runs Trino in one JVM instance, and processing is parallelized further using threads. Node# Any Trino server in a specific Trino cluster is considered a node of the cluster. Technically this refers to the Java process running the Trino program, but node is often used to refer to the computer running the process due to the recommendation to run only one Trino process per computer. Coordinator# The Trino coordinator is the server that is responsible for parsing statements, planning queries, and managing Trino worker nodes. It is the “brain” of a Trino installation and is also the node to which a client connects to submit statements for execution. Every Trino installation must have a Trino coordinator alongside one or more Trino workers. For development or testing purposes, a single instance of Trino can be configured to perform both roles. The coordinator keeps track of the activity on each worker and coordinates the execution of a query. The coordinator creates a logical model of a query involving a series of stages, which is then translated into a series of connected tasks running on a cluster of Trino workers. Coordinators communicate with workers and clients using a REST API. Worker# A Trino worker is a server in a Trino installation, which is responsible for executing tasks and processing data. Worker nodes fetch data from connectors and exchange intermediate data with each other. The coordinator is responsible for fetching results from the workers and returning the final results to the client. When a Trino worker process starts up, it advertises itself to the discovery server in the coordinator, which makes it available to the Trino coordinator for task execution. Workers communicate with other workers and Trino coordinators using a REST API. Client# Clients allow you to connect to Trino, submit SQL queries, and receive the results. Clients can access all configured data sources using catalogs. Clients are full-featured client applications or client drivers and libraries that allow you to connect with any application supporting that driver, or even your own custom application or script. Clients applications include command line tools, desktop applications, web-based applications, and software-as-a-service solutions with features such as interactive SQL query authoring with editors, or rich user interfaces for graphical query creation, query running and result rendering, visualizations with charts and graphs, reporting, and dashboard creation. Client application that support other query languages or user interface components to build a query, must translate each request to SQL as supported by Trino. More details are available in the Trino client documentation. Plugin# Trino uses a plugin architecture to extend its capabilities and integrate with various data sources and other systems. Details about different types of plugins, installation, removal, and other aspects are available in the Plugin documentation. Data source# Trino is a query engine that you can use to query many different data sources. They include data lakes and lakehouses, numerous relational database management systems, key-value stores, and many other data stores. A comprehensive list with more details for each data source is available on the Trino website. Data sources provide the data for Trino to query. Configure a catalog with the required Trino connector for the specific data source to access the data. With Trino you are ready to use any supported client to query the data sources using SQL and the features of your client. Throughout this documentation, you’ll read terms such as connector, catalog, schema, and table. These fundamental concepts cover Trino’s model of a particular data source and are described in the following section. Connector# A connector adapts Trino to a data source such as a data lake using Hadoop/Hive or Apache Iceberg, or a relational database such as PostgreSQL. You can think of a connector the same way you think of a driver for a database. It is an implementation of Trino’s service provider interface (SPI), which allows Trino to interact with a resource using a standard API. Trino contains many built-in connectors: Connectors for data lakes and lakehouses including the Delta Lake, Hive, Hudi, and Iceberg connectors. Connectors for relational database management systems, including the MySQL, PostgreSQL, Oracle, and SQL Server connectors. Connectors for a variety of other systems, including the Cassandra, ClickHouse, OpenSearch, Pinot, Prometheus, SingleStore, and Snowflake connectors. A number of other utility connectors such as the JMX, System, and TPC-H connectors. Every catalog uses a specific connector. If you examine a catalog configuration file, you see that each contains a mandatory property connector.name with the value identifying the connector. Catalog# A Trino catalog is a collection of configuration properties used to access a specific data source, including the required connector and any other details such as credentials and URL. Catalogs are defined in properties files stored in the Trino configuration directory. The name of the properties file determines the name of the catalog. For example, the properties file etc/example.properties results in a catalog name example. You can configure and use many catalogs, with different or identical connectors, to access different data sources. For example, if you have two data lakes, you can configure two catalogs in a single Trino cluster that both use the Hive connector, allowing you to query data from both clusters, even within the same SQL query. You can also use a Hive connector for one catalog to access a data lake, and use the Iceberg connector for another catalog to access the data lakehouse. Or, you can configure different catalogs to access different PostgreSQL database. The combination of different catalogs is determined by your needs to access different data sources only. A catalog contains one or more schemas, which in turn contain objects such as tables, views, or materialized views. When addressing an objects such as tables in Trino, the fully-qualified name is always rooted in a catalog. For example, a fully-qualified table name of example.test_data.test refers to the test table in the test_data schema in the example catalog. Schema# Schemas are a way to organize tables. Together, a catalog and schema define a set of tables and other objects that can be queried. When accessing Hive or a relational database such as MySQL with Trino, a schema translates to the same concept in the target database. Other types of connectors may organize tables into schemas in a way that makes sense for the underlying data source. Table# A table is a set of unordered rows, which are organized into named columns with types. This is the same as in any relational database. Type mapping from source data to Trino is defined by the connector, varies across connectors, and is documented in the specific connector documentation, for example the type mapping in the PostgreSQL connector. Query execution model# Trino executes SQL statements and turns these statements into queries, that are executed across a distributed cluster of coordinator and workers. Statement# Trino executes ANSI-compatible SQL statements. When the Trino documentation refers to a statement, it is referring to statements as defined in the ANSI SQL standard, which consists of clauses, expressions, and predicates. Some readers might be curious why this section lists separate concepts for statements and queries. This is necessary because, in Trino, statements simply refer to the textual representation of a statement written in SQL. When a statement is executed, Trino creates a query along with a query plan that is then distributed across a series of Trino workers. Query# When Trino parses a statement, it converts it into a query and creates a distributed query plan, which is then realized as a series of interconnected stages running on Trino workers. When you retrieve information about a query in Trino, you receive a snapshot of every component that is involved in producing a result set in response to a statement. The difference between a statement and a query is simple. A statement can be thought of as the SQL text that is passed to Trino, while a query refers to the configuration and components instantiated to execute that statement. A query encompasses stages, tasks, splits, connectors, and other components and data sources working in concert to produce a result. Stage# When Trino executes a query, it does so by breaking up the execution into a hierarchy of stages. For example, if Trino needs to aggregate data from one billion rows stored in Hive, it does so by creating a root stage to aggregate the output of several other stages, all of which are designed to implement different sections of a distributed query plan. The hierarchy of stages that comprises a query resembles a tree. Every query has a root stage, which is responsible for aggregating the output from other stages. Stages are what the coordinator uses to model a distributed query plan, but stages themselves don’t run on Trino workers. Task# As mentioned in the previous section, stages model a particular section of a distributed query plan, but stages themselves don’t execute on Trino workers. To understand how a stage is executed, you need to understand that a stage is implemented as a series of tasks distributed over a network of Trino workers. Tasks are the “work horse” in the Trino architecture as a distributed query plan is deconstructed into a series of stages, which are then translated to tasks, which then act upon or process splits. A Trino task has inputs and outputs, and just as a stage can be executed in parallel by a series of tasks, a task is executing in parallel with a series of drivers. Split# Tasks operate on splits, which are sections of a larger data set. Stages at the lowest level of a distributed query plan retrieve data via splits from connectors, and intermediate stages at a higher level of a distributed query plan retrieve data from other stages. When Trino is scheduling a query, the coordinator queries a connector for a list of all splits that are available for a table. The coordinator keeps track of which machines are running which tasks, and what splits are being processed by which tasks. Driver# Tasks contain one or more parallel drivers. Drivers act upon data and combine operators to produce output that is then aggregated by a task and then delivered to another task in another stage. A driver is a sequence of operator instances, or you can think of a driver as a physical set of operators in memory. It is the lowest level of parallelism in the Trino architecture. A driver has one input and one output. Operator# An operator consumes, transforms and produces data. For example, a table scan fetches data from a connector and produces data that can be consumed by other operators, and a filter operator consumes data and produces a subset by applying a predicate over the input data. Exchange# Exchanges transfer data between Trino nodes for different stages of a query. Tasks produce data into an output buffer and consume data from other tasks using an exchange client.

---


## Functions and Operators

### Aggregate Functions Documentation
Source: https://trino.io/docs/current/functions/aggregate.html

Aggregate functions# Aggregate functions operate on a set of values to compute a single result. Except for count(), count_if(), max_by(), min_by() and approx_distinct(), all of these aggregate functions ignore null values and return null for no input rows or when all values are null. For example, sum() returns null rather than zero and avg() does not include null values in the count. The coalesce function can be used to convert null into zero. Ordering during aggregation# Some aggregate functions such as array_agg() produce different results depending on the order of input values. This ordering can be specified by writing an ORDER BY clause within the aggregate function: array_agg(x ORDER BY y DESC) array_agg(x ORDER BY x, y, z) Filtering during aggregation# The FILTER keyword can be used to remove rows from aggregation processing with a condition expressed using a WHERE clause. This is evaluated for each row before it is used in the aggregation and is supported for all aggregate functions. aggregate_function(...) FILTER (WHERE <condition>) A common and very useful example is to use FILTER to remove nulls from consideration when using array_agg: SELECT array_agg(name) FILTER (WHERE name IS NOT NULL) FROM region; As another example, imagine you want to add a condition on the count for Iris flowers, modifying the following query: SELECT species, count(*) AS count FROM iris GROUP BY species; species | count -----------+------- setosa | 50 virginica | 50 versicolor | 50 If you just use a normal WHERE statement you lose information: SELECT species, count(*) AS count FROM iris WHERE petal_length_cm > 4 GROUP BY species; species | count -----------+------- virginica | 50 versicolor | 34 Using a filter you retain all information: SELECT species, count(*) FILTER (where petal_length_cm > 4) AS count FROM iris GROUP BY species; species | count -----------+------- virginica | 50 setosa | 0 versicolor | 34 General aggregate functions# any_value(x) → [same as input]# Returns an arbitrary non-null value x, if one exists. x can be any valid expression. This allows you to return values from columns that are not directly part of the aggregation, inluding expressions using these columns, in a query. For example, the following query returns the customer name from the name column, and returns the sum of all total prices as customer spend. The aggregation however uses the rows grouped by the customer identifier custkey a required, since only that column is guaranteed to be unique: SELECT sum(o.totalprice) as spend, any_value(c.name) FROM tpch.tiny.orders o JOIN tpch.tiny.customer c ON o.custkey = c.custkey GROUP BY c.custkey; ORDER BY spend; arbitrary(x) → [same as input]# Returns an arbitrary non-null value of x, if one exists. Identical to any_value(). array_agg(x) → array<[same as input]># Returns an array created from the input x elements. avg(x) → double# Returns the average (arithmetic mean) of all input values. avg(time interval type) → time interval type Returns the average interval length of all input values. bool_and(boolean) → boolean# Returns TRUE if every input value is TRUE, otherwise FALSE. bool_or(boolean) → boolean# Returns TRUE if any input value is TRUE, otherwise FALSE. checksum(x) → varbinary# Returns an order-insensitive checksum of the given values. count(*) → bigint# Returns the number of input rows. count(x) → bigint Returns the number of non-null input values. count_if(x) → bigint# Returns the number of TRUE input values. This function is equivalent to count(CASE WHEN x THEN 1 END). every(boolean) → boolean# This is an alias for bool_and(). geometric_mean(x) → double# Returns the geometric mean of all input values. listagg(x, separator) → varchar# Returns the concatenated input values, separated by the separator string. Synopsis: LISTAGG( expression [, separator] [ON OVERFLOW overflow_behaviour]) WITHIN GROUP (ORDER BY sort_item, [...]) [FILTER (WHERE condition)] If separator is not specified, the empty string will be used as separator. In its simplest form the function looks like: SELECT listagg(value, ',') WITHIN GROUP (ORDER BY value) csv_value FROM (VALUES 'a', 'c', 'b') t(value); and results in: csv_value ----------- 'a,b,c' The overflow behaviour is by default to throw an error in case that the length of the output of the function exceeds 1048576 bytes: SELECT listagg(value, ',' ON OVERFLOW ERROR) WITHIN GROUP (ORDER BY value) csv_value FROM (VALUES 'a', 'b', 'c') t(value); There exists also the possibility to truncate the output WITH COUNT or WITHOUT COUNT of omitted non-null values in case that the length of the output of the function exceeds 1048576 bytes: SELECT listagg(value, ',' ON OVERFLOW TRUNCATE '.....' WITH COUNT) WITHIN GROUP (ORDER BY value) FROM (VALUES 'a', 'b', 'c') t(value); If not specified, the truncation filler string is by default '...'. This aggregation function can be also used in a scenario involving grouping: SELECT id, listagg(value, ',') WITHIN GROUP (ORDER BY o) csv_value FROM (VALUES (100, 1, 'a'), (200, 3, 'c'), (200, 2, 'b') ) t(id, o, value) GROUP BY id ORDER BY id; results in: id | csv_value -----+----------- 100 | a 200 | b,c This aggregation function supports filtering during aggregation for scenarios where the aggregation for the data not matching the filter condition still needs to show up in the output: SELECT country, listagg(city, ',') WITHIN GROUP (ORDER BY population DESC) FILTER (WHERE population >= 10_000_000) megacities FROM (VALUES ('India', 'Bangalore', 13_700_000), ('India', 'Chennai', 12_200_000), ('India', 'Ranchi', 1_547_000), ('Austria', 'Vienna', 1_897_000), ('Poland', 'Warsaw', 1_765_000) ) t(country, city, population) GROUP BY country ORDER BY country; results in: country | megacities ---------+------------------- Austria | NULL India | Bangalore,Chennai Poland | NULL The current implementation of listagg function does not support window frames. max(x) → [same as input]# Returns the maximum value of all input values. max(x, n) → array<[same as x]> Returns n largest values of all input values of x. max_by(x, y) → [same as x]# Returns the value of x associated with the maximum value of y over all input values. max_by(x, y, n) → array<[same as x]> Returns n values of x associated with the n largest of all input values of y in descending order of y. min(x) → [same as input]# Returns the minimum value of all input values. min(x, n) → array<[same as x]> Returns n smallest values of all input values of x. min_by(x, y) → [same as x]# Returns the value of x associated with the minimum value of y over all input values. min_by(x, y, n) → array<[same as x]> Returns n values of x associated with the n smallest of all input values of y in ascending order of y. sum(x) → [same as input]# Returns the sum of all input values. Bitwise aggregate functions# bitwise_and_agg(x) → bigint# Returns the bitwise AND of all input non-NULL values in 2’s complement representation. If all records inside the group are NULL, or if the group is empty, the function returns NULL. bitwise_or_agg(x) → bigint# Returns the bitwise OR of all input non-NULL values in 2’s complement representation. If all records inside the group are NULL, or if the group is empty, the function returns NULL. bitwise_xor_agg(x) → bigint# Returns the bitwise XOR of all input non-NULL values in 2’s complement representation. If all records inside the group are NULL, or if the group is empty, the function returns NULL. Map aggregate functions# histogram(x) → map<K,bigint># Returns a map containing the count of the number of times each input value occurs. map_agg(key, value) → map<K,V># Returns a map created from the input key / value pairs. map_union(x(K, V)) → map<K,V># Returns the union of all the input maps. If a key is found in multiple input maps, that key’s value in the resulting map comes from an arbitrary input map. For example, take the following histogram function that creates multiple maps from the Iris dataset: SELECT histogram(floor(petal_length_cm)) petal_data FROM memory.default.iris GROUP BY species; petal_data -- {4.0=6, 5.0=33, 6.0=11} -- {4.0=37, 5.0=2, 3.0=11} -- {1.0=50} You can combine these maps using map_union: SELECT map_union(petal_data) petal_data_union FROM ( SELECT histogram(floor(petal_length_cm)) petal_data FROM memory.default.iris GROUP BY species ); petal_data_union --{4.0=6, 5.0=2, 6.0=11, 1.0=50, 3.0=11} multimap_agg(key, value) → map<K,array(V)># Returns a multimap created from the input key / value pairs. Each key can be associated with multiple values. Approximate aggregate functions# approx_distinct(x) → bigint# Returns the approximate number of distinct input values. This function provides an approximation of count(DISTINCT x). Zero is returned if all input values are null. This function should produce a standard error of 2.3%, which is the standard deviation of the (approximately normal) error distribution over all possible sets. It does not guarantee an upper bound on the error for any specific input set. approx_distinct(x, e) → bigint Returns the approximate number of distinct input values. This function provides an approximation of count(DISTINCT x). Zero is returned if all input values are null. This function should produce a standard error of no more than e, which is the standard deviation of the (approximately normal) error distribution over all possible sets. It does not guarantee an upper bound on the error for any specific input set. The current implementation of this function requires that e be in the range of [0.0040625, 0.26000]. approx_most_frequent(buckets, value, capacity) → map<[same as value], bigint># Computes the top frequent values up to buckets elements approximately. Approximate estimation of the function enables us to pick up the frequent values with less memory. Larger capacity improves the accuracy of underlying algorithm with sacrificing the memory capacity. The returned value is a map containing the top elements with corresponding estimated frequency. The error of the function depends on the permutation of the values and its cardinality. We can set the capacity same as the cardinality of the underlying data to achieve the least error. buckets and capacity must be bigint. value can be numeric or string type. The function uses the stream summary data structure proposed in the paper Efficient Computation of Frequent and Top-k Elements in Data Streams by A. Metwalley, D. Agrawl and A. Abbadi. approx_percentile(x, percentage) → [same as x]# Returns the approximate percentile for all input values of x at the given percentage. The value of percentage must be between zero and one and must be constant for all input rows. approx_percentile(x, percentages) → array<[same as x]> Returns the approximate percentile for all input values of x at each of the specified percentages. Each element of the percentages array must be between zero and one, and the array must be constant for all input rows. approx_percentile(x, w, percentage) → [same as x] Returns the approximate weighed percentile for all input values of x using the per-item weight w at the percentage percentage. Weights must be greater or equal to 1. Integer-value weights can be thought of as a replication count for the value x in the percentile set. The value of percentage must be between zero and one and must be constant for all input rows. approx_percentile(x, w, percentages) → array<[same as x]> Returns the approximate weighed percentile for all input values of x using the per-item weight w at each of the given percentages specified in the array. Weights must be greater or equal to 1. Integer-value weights can be thought of as a replication count for the value x in the percentile set. Each element of the percentages array must be between zero and one, and the array must be constant for all input rows. approx_set(x) → HyperLogLog See HyperLogLog functions. merge(x) → HyperLogLog See HyperLogLog functions. merge(qdigest(T)) -> qdigest(T) See Quantile digest functions. merge(tdigest) → tdigest See T-Digest functions. numeric_histogram(buckets, value) → map<double, double> Computes an approximate histogram with up to buckets number of buckets for all values. This function is equivalent to the variant of numeric_histogram() that takes a weight, with a per-item weight of 1. numeric_histogram(buckets, value, weight) → map<double, double># Computes an approximate histogram with up to buckets number of buckets for all values with a per-item weight of weight. The algorithm is based loosely on: Yael Ben-Haim and Elad Tom-Tov, "A streaming parallel decision tree algorithm", J. Machine Learning Research 11 (2010), pp. 849--872. buckets must be a bigint. value and weight must be numeric. qdigest_agg(x) -> qdigest([same as x]) See Quantile digest functions. qdigest_agg(x, w) -> qdigest([same as x]) See Quantile digest functions. qdigest_agg(x, w, accuracy) -> qdigest([same as x]) See Quantile digest functions. tdigest_agg(x) → tdigest See T-Digest functions. tdigest_agg(x, w) → tdigest See T-Digest functions. Statistical aggregate functions# corr(y, x) → double# Returns correlation coefficient of input values. covar_pop(y, x) → double# Returns the population covariance of input values. covar_samp(y, x) → double# Returns the sample covariance of input values. kurtosis(x) → double# Returns the excess kurtosis of all input values. Unbiased estimate using the following expression: kurtosis(x) = n(n+1)/((n-1)(n-2)(n-3))sum[(x_i-mean)^4]/stddev(x)^4-3(n-1)^2/((n-2)(n-3)) regr_intercept(y, x) → double# Returns linear regression intercept of input values. y is the dependent value. x is the independent value. regr_slope(y, x) → double# Returns linear regression slope of input values. y is the dependent value. x is the independent value. skewness(x) → double# Returns the Fisher’s moment coefficient of skewness of all input values. stddev(x) → double# This is an alias for stddev_samp(). stddev_pop(x) → double# Returns the population standard deviation of all input values. stddev_samp(x) → double# Returns the sample standard deviation of all input values. variance(x) → double# This is an alias for var_samp(). var_pop(x) → double# Returns the population variance of all input values. var_samp(x) → double# Returns the sample variance of all input values. Lambda aggregate functions# reduce_agg(inputValue T, initialState S, inputFunction(S, T, S), combineFunction(S, S, S)) → S# Reduces all input values into a single value. inputFunction will be invoked for each non-null input value. In addition to taking the input value, inputFunction takes the current state, initially initialState, and returns the new state. combineFunction will be invoked to combine two states into a new state. The final state is returned: SELECT id, reduce_agg(value, 0, (a, b) -> a + b, (a, b) -> a + b) FROM ( VALUES (1, 3), (1, 4), (1, 5), (2, 6), (2, 7) ) AS t(id, value) GROUP BY id; -- (1, 12) -- (2, 13) SELECT id, reduce_agg(value, 1, (a, b) -> a * b, (a, b) -> a * b) FROM ( VALUES (1, 3), (1, 4), (1, 5), (2, 6), (2, 7) ) AS t(id, value) GROUP BY id; -- (1, 60) -- (2, 42) The state type must be a boolean, integer, floating-point, char, varchar or date/time/interval.

#### Code Examples

```
array_agg(x ORDER BY y DESC)
array_agg(x ORDER BY x, y, z)
```
```
aggregate_function(...) FILTER (WHERE <condition>)
```
```
SELECT array_agg(name) FILTER (WHERE name IS NOT NULL)
FROM region;
```
```
SELECT species,
       count(*) AS count
FROM iris
GROUP BY species;
```
```
species    | count
-----------+-------
setosa     |   50
virginica  |   50
versicolor |   50
```
```
SELECT species,
    count(*) AS count
FROM iris
WHERE petal_length_cm > 4
GROUP BY species;
```
```
species    | count
-----------+-------
virginica  |   50
versicolor |   34
```
```
SELECT species,
       count(*) FILTER (where petal_length_cm > 4) AS count
FROM iris
GROUP BY species;
```
```
species    | count
-----------+-------
virginica  |   50
setosa     |    0
versicolor |   34
```
```
SELECT sum(o.totalprice) as spend,
    any_value(c.name)
FROM tpch.tiny.orders o
JOIN tpch.tiny.customer c
ON o.custkey  = c.custkey
GROUP BY c.custkey;
ORDER BY spend;
```
```
LISTAGG( expression [, separator] [ON OVERFLOW overflow_behaviour])
    WITHIN GROUP (ORDER BY sort_item, [...]) [FILTER (WHERE condition)]
```
```
SELECT listagg(value, ',') WITHIN GROUP (ORDER BY value) csv_value
FROM (VALUES 'a', 'c', 'b') t(value);
```
```
csv_value
-----------
'a,b,c'
```
```
SELECT listagg(value, ',' ON OVERFLOW ERROR) WITHIN GROUP (ORDER BY value) csv_value
FROM (VALUES 'a', 'b', 'c') t(value);
```
```
SELECT listagg(value, ',' ON OVERFLOW TRUNCATE '.....' WITH COUNT) WITHIN GROUP (ORDER BY value)
FROM (VALUES 'a', 'b', 'c') t(value);
```
```
SELECT id, listagg(value, ',') WITHIN GROUP (ORDER BY o) csv_value
FROM (VALUES
    (100, 1, 'a'),
    (200, 3, 'c'),
    (200, 2, 'b')
) t(id, o, value)
GROUP BY id
ORDER BY id;
```
```
id  | csv_value
-----+-----------
 100 | a
 200 | b,c
```
```
SELECT 
    country,
    listagg(city, ',')
        WITHIN GROUP (ORDER BY population DESC)
        FILTER (WHERE population >= 10_000_000) megacities
FROM (VALUES 
    ('India', 'Bangalore', 13_700_000),
    ('India', 'Chennai', 12_200_000),
    ('India', 'Ranchi', 1_547_000),
    ('Austria', 'Vienna', 1_897_000),
    ('Poland', 'Warsaw', 1_765_000)
) t(country, city, population)
GROUP BY country
ORDER BY country;
```
```
country |    megacities     
---------+-------------------
 Austria | NULL              
 India   | Bangalore,Chennai 
 Poland  | NULL
```
```
SELECT histogram(floor(petal_length_cm)) petal_data
FROM memory.default.iris
GROUP BY species;

        petal_data
-- {4.0=6, 5.0=33, 6.0=11}
-- {4.0=37, 5.0=2, 3.0=11}
-- {1.0=50}
```
```
SELECT map_union(petal_data) petal_data_union
FROM (
       SELECT histogram(floor(petal_length_cm)) petal_data
       FROM memory.default.iris
       GROUP BY species
       );

             petal_data_union
--{4.0=6, 5.0=2, 6.0=11, 1.0=50, 3.0=11}
```
```
Yael Ben-Haim and Elad Tom-Tov, "A streaming parallel decision tree algorithm",
J. Machine Learning Research 11 (2010), pp. 849--872.
```
```
kurtosis(x) = n(n+1)/((n-1)(n-2)(n-3))sum[(x_i-mean)^4]/stddev(x)^4-3(n-1)^2/((n-2)(n-3))
```
```
SELECT id, reduce_agg(value, 0, (a, b) -> a + b, (a, b) -> a + b)
FROM (
    VALUES
        (1, 3),
        (1, 4),
        (1, 5),
        (2, 6),
        (2, 7)
) AS t(id, value)
GROUP BY id;
-- (1, 12)
-- (2, 13)

SELECT id, reduce_agg(value, 1, (a, b) -> a * b, (a, b) -> a * b)
FROM (
    VALUES
        (1, 3),
        (1, 4),
        (1, 5),
        (2, 6),
        (2, 7)
) AS t(id, value)
GROUP BY id;
-- (1, 60)
-- (2, 42)
```


---

### AI Functions Documentation
Source: https://trino.io/docs/current/functions/ai.html

AI functions# The AI functions allow you to invoke a large language model (LLM) to perform various textual tasks. Multiple LLM providers are supported, specifically OpenAI and Anthropic directly, and many others such as Llama, DeepSeek, Phi, Mistral, or Gemma using Ollama. The LLM must be provided outside Trino as an external service. Configuration# Because the AI functions require an external LLM service, they are not available by default. To enable them, you must configure a catalog properties file to register the functions invoking the configured LLM with the specified catalog name. Create a catalog properties file etc/catalog/llm.properties that references the ai connector: connector.name=ai The AI functions are available with the ai schema name. For the preceding example, the functions use the llm.ai catalog and schema prefix. To avoid needing to reference the functions with their fully qualified name, configure the sql.path SQL environment property in the config.properties file to include the catalog and schema prefix: sql.path=llm.ai Configure multiple catalogs to use the same functions with different LLM providers. In this case, the functions must be referenced using their fully qualified name, rather than relying on the SQL path. Providers# The AI functions invoke an external LLM. Access to the LLM API must be configured in the catalog. Performance, results, and cost of all AI function invocations are dependent on the LLM provider and the model used. You must specify a model that is suitable for textual analysis. AI functions provider configuration properties# Property name Description ai.provider Required name of the provider. Must be anthropic for using the Anthropic provider or openai for OpenAI or Ollama. ai.anthropic.endpoint URL for the Anthropic API endpoint. Defaults to https://api.anthropic.com. ai.anthropic.api-key API key value for Anthropic API access. Required with ai.provider set to anthropic. ai.openai.endpoint URL for the OpenAI API or Ollama endpoint. Defaults to https://api.openai.com. Set to the URL endpoint for Ollama when using models via Ollama and add any string for the ai.openai.api-key. ai.openai.api-key API key value for OpenAI API access. Required with ai.provider set to openai. Required and ignored with Ollama use. The AI functions connect to the providers over HTTP. Configure the connection using the ai prefix with the HTTP client properties. The following sections show minimal configurations for Anthropic, OpenAI, and Ollama use. Anthropic# The Anthropic provider uses the Anthropic API to perform the AI functions: ai.provider=anthropic ai.model=claude-3-5-sonnet-latest ai.anthropic.api-key=xxx Use secrets to avoid actual API key values in the catalog properties files. OpenAI# The OpenAI provider uses the OpenAI API to perform the AI functions: ai.provider=openai ai.model=gpt-4o-mini ai.openai.api-key=xxx Use secrets to avoid actual API key values in the catalog properties files. Ollama# The OpenAI provider can be used with Ollama to perform the AI functions, as Ollama is compatible with the OpenAI API: ai.provider=openai ai.model=llama3.3 ai.openai.endpoint=http://localhost:11434 ai.openai.api-key=none An API key must be specified, but is ignored by Ollama. Ollama allows you to use Llama, DeepSeek, Phi, Mistral, Gemma and other models on a self-hosted deployment or from a vendor. Model configuration# All providers support a number of different models. You must configure at least one model to use for the AI function. The model must be suitable for textual analysis. Provider and model choice impacts performance, results, and cost of all AI functions. Costs vary with AI function used based on the implementation prompt size, the length of the input, and the length of the output from the model, because model providers charge based input and output tokens. Optionally configure different models from the same provider for each functions as an override: AI function model configuration properties# Property name Description ai.model Required name of the model. Valid names vary by provider. Model must be suitable for textual analysis. The model is used for all functions, unless a specific model is configured for a function as override. ai.analyze-sentiment.model Optional override to use a different model for ai_analyze_sentiment(). ai.classify.model Optional override to use a different model for ai_classify(). ai.extract.model Optional override to use a different model for ai_extract(). ai.fix-grammar.model Optional override to use a different model for ai_fix_grammar(). ai.generate.model Optional override to use a different model for ai_gen(). ai.mask.model Optional override to use a different model for ai_mask(). ai.translate.model Optional override to use a different model for ai_translate(). Functions# The following functions are available in each catalog configured with the ai connector under the ai schema and use the configured LLM provider: ai_analyze_sentiment(text) → varchar# Analyzes the sentiment of the input text. The sentiment result is positive, negative, neutral, or mixed. SELECT ai_analyze_sentiment('I love Trino'); -- positive ai_classify(text, labels) → varchar# Classifies the input text according to the provided labels. SELECT ai_classify('Buy now!', ARRAY['spam', 'not spam']); -- spam ai_extract(text, labels)# Extracts values for the provided labels from the input text. SELECT ai_extract('John is 25 years old', ARRAY['name', 'age']); -- {name=John, age=25} ai_fix_grammar(text) → varchar# Corrects grammatical errors in the input text. SELECT ai_fix_grammar('I are happy. What you doing?'); -- I am happy. What are you doing? ai_gen(prompt) → varchar# Generates text based on the input prompt. SELECT ai_gen('Describe Trino in a few words'); -- Distributed SQL query engine. ai_mask(text, labels) → varchar# Masks the values for the provided labels in the input text by replacing them with the text [MASKED]. SELECT ai_mask( 'Contact me at 555-1234 or visit us at 123 Main St.', ARRAY['phone', 'address']); -- Contact me at [MASKED] or visit us at [MASKED]. ai_translate(text, language) → varchar# Translates the input text to the specified language. SELECT ai_translate('I like coffee', 'es'); -- Me gusta el café SELECT ai_translate('I like coffee', 'zh-TW'); -- 我喜歡咖啡

#### Code Examples

```
connector.name=ai
```
```
sql.path=llm.ai
```
```
ai.provider=anthropic
ai.model=claude-3-5-sonnet-latest
ai.anthropic.api-key=xxx
```
```
ai.provider=openai
ai.model=gpt-4o-mini
ai.openai.api-key=xxx
```
```
ai.provider=openai
ai.model=llama3.3
ai.openai.endpoint=http://localhost:11434
ai.openai.api-key=none
```
```
SELECT ai_analyze_sentiment('I love Trino');
-- positive
```
```
SELECT ai_classify('Buy now!', ARRAY['spam', 'not spam']);
-- spam
```
```
SELECT ai_extract('John is 25 years old', ARRAY['name', 'age']);
-- {name=John, age=25}
```
```
SELECT ai_fix_grammar('I are happy. What you doing?');
-- I am happy. What are you doing?
```
```
SELECT ai_gen('Describe Trino in a few words');
-- Distributed SQL query engine.
```
```
SELECT ai_mask(
    'Contact me at 555-1234 or visit us at 123 Main St.',
    ARRAY['phone', 'address']);
-- Contact me at [MASKED] or visit us at [MASKED].
```
```
SELECT ai_translate('I like coffee', 'es');
-- Me gusta el café

SELECT ai_translate('I like coffee', 'zh-TW');
-- 我喜歡咖啡
```


---

### Array Functions Documentation
Source: https://trino.io/docs/current/functions/array.html

Array functions and operators# Array functions and operators use the ARRAY type. Create an array with the data type constructor. Create an array of integer numbers: SELECT ARRAY[1, 2, 4]; -- [1, 2, 4] Create an array of character values: SELECT ARRAY['foo', 'bar', 'bazz']; -- [foo, bar, bazz] Array elements must use the same type or it must be possible to coerce values to a common type. The following example uses integer and decimal values and the resulting array contains decimals: SELECT ARRAY[1, 1.2, 4]; -- [1.0, 1.2, 4.0] Null values are allowed: SELECT ARRAY[1, 2, NULL, -4, NULL]; -- [1, 2, NULL, -4, NULL] Subscript operator: []# The [] operator is used to access an element of an array and is indexed starting from one: SELECT my_array[1] AS first_element The following example constructs an array and then accesses the second element: SELECT ARRAY[1, 1.2, 4][2]; -- 1.2 Concatenation operator: ||# The || operator is used to concatenate an array with an array or an element of the same type: SELECT ARRAY[1] || ARRAY[2]; -- [1, 2] SELECT ARRAY[1] || 2; -- [1, 2] SELECT 2 || ARRAY[1]; -- [2, 1] Array functions# all_match(array(T), function(T, boolean)) → boolean# Returns whether all elements of an array match the given predicate. Returns true if all the elements match the predicate (a special case is when the array is empty); false if one or more elements don’t match; NULL if the predicate function returns NULL for one or more elements and true for all other elements. any_match(array(T), function(T, boolean)) → boolean# Returns whether any elements of an array match the given predicate. Returns true if one or more elements match the predicate; false if none of the elements matches (a special case is when the array is empty); NULL if the predicate function returns NULL for one or more elements and false for all other elements. array_distinct(x) → array# Remove duplicate values from the array x. array_intersect(x, y) → array# Returns an array of the elements in the intersection of x and y, without duplicates. array_union(x, y) → array# Returns an array of the elements in the union of x and y, without duplicates. array_except(x, y) → array# Returns an array of elements in x but not in y, without duplicates. array_histogram(x) → map<K, bigint># Returns a map where the keys are the unique elements in the input array x and the values are the number of times that each element appears in x. Null values are ignored. SELECT array_histogram(ARRAY[42, 7, 42, NULL]); -- {42=2, 7=1} Returns an empty map if the input array has no non-null elements. SELECT array_histogram(ARRAY[NULL, NULL]); -- {} array_join(x, delimiter) → varchar# Concatenates the elements of the given array using the delimiter. Null elements are omitted in the result. array_join(x, delimiter, null_replacement) → varchar Concatenates the elements of the given array using the delimiter and an optional string to replace nulls. array_max(x) → x# Returns the maximum value of input array. array_min(x) → x# Returns the minimum value of input array. array_position(x, element) → bigint# Returns the position of the first occurrence of the element in array x (or 0 if not found). array_remove(x, element) → array# Remove all elements that equal element from array x. array_sort(x) → array# Sorts and returns the array x. The elements of x must be orderable. Null elements will be placed at the end of the returned array. array_sort(array(T), function(T, T, int)) -> array(T) Sorts and returns the array based on the given comparator function. The comparator will take two nullable arguments representing two nullable elements of the array. It returns -1, 0, or 1 as the first nullable element is less than, equal to, or greater than the second nullable element. If the comparator function returns other values (including NULL), the query will fail and raise an error. SELECT array_sort(ARRAY[3, 2, 5, 1, 2], (x, y) -> IF(x < y, 1, IF(x = y, 0, -1))); -- [5, 3, 2, 2, 1] SELECT array_sort(ARRAY['bc', 'ab', 'dc'], (x, y) -> IF(x < y, 1, IF(x = y, 0, -1))); -- ['dc', 'bc', 'ab'] SELECT array_sort(ARRAY[3, 2, null, 5, null, 1, 2], -- sort null first with descending order (x, y) -> CASE WHEN x IS NULL THEN -1 WHEN y IS NULL THEN 1 WHEN x < y THEN 1 WHEN x = y THEN 0 ELSE -1 END); -- [null, null, 5, 3, 2, 2, 1] SELECT array_sort(ARRAY[3, 2, null, 5, null, 1, 2], -- sort null last with descending order (x, y) -> CASE WHEN x IS NULL THEN 1 WHEN y IS NULL THEN -1 WHEN x < y THEN 1 WHEN x = y THEN 0 ELSE -1 END); -- [5, 3, 2, 2, 1, null, null] SELECT array_sort(ARRAY['a', 'abcd', 'abc'], -- sort by string length (x, y) -> IF(length(x) < length(y), -1, IF(length(x) = length(y), 0, 1))); -- ['a', 'abc', 'abcd'] SELECT array_sort(ARRAY[ARRAY[2, 3, 1], ARRAY[4, 2, 1, 4], ARRAY[1, 2]], -- sort by array length (x, y) -> IF(cardinality(x) < cardinality(y), -1, IF(cardinality(x) = cardinality(y), 0, 1))); -- [[1, 2], [2, 3, 1], [4, 2, 1, 4]] arrays_overlap(x, y) → boolean# Tests if arrays x and y have any non-null elements in common. Returns null if there are no non-null elements in common but either array contains null. cardinality(x) → bigint# Returns the cardinality (size) of the array x. concat(array1, array2, ..., arrayN) → array Concatenates the arrays array1, array2, ..., arrayN. This function provides the same functionality as the SQL-standard concatenation operator (||). combinations(array(T), n) -> array(array(T))# Returns n-element sub-groups of input array. If the input array has no duplicates, combinations returns n-element subsets. SELECT combinations(ARRAY['foo', 'bar', 'baz'], 2); -- [['foo', 'bar'], ['foo', 'baz'], ['bar', 'baz']] SELECT combinations(ARRAY[1, 2, 3], 2); -- [[1, 2], [1, 3], [2, 3]] SELECT combinations(ARRAY[1, 2, 2], 2); -- [[1, 2], [1, 2], [2, 2]] Order of sub-groups is deterministic but unspecified. Order of elements within a sub-group deterministic but unspecified. n must be not be greater than 5, and the total size of sub-groups generated must be smaller than 100,000. contains(x, element) → boolean# Returns true if the array x contains the element. contains_sequence(x, seq) → boolean# Return true if array x contains all of array seq as a subsequence (all values in the same consecutive order). element_at(array(E), index) → E# Returns element of array at given index. If index > 0, this function provides the same functionality as the SQL-standard subscript operator ([]), except that the function returns NULL when accessing an index larger than array length, whereas the subscript operator would fail in such a case. If index < 0, element_at accesses elements from the last to the first. filter(array(T), function(T, boolean)) -> array(T)# Constructs an array from those elements of array for which function returns true: SELECT filter(ARRAY[], x -> true); -- [] SELECT filter(ARRAY[5, -6, NULL, 7], x -> x > 0); -- [5, 7] SELECT filter(ARRAY[5, NULL, 7, NULL], x -> x IS NOT NULL); -- [5, 7] flatten(x) → array# Flattens an array(array(T)) to an array(T) by concatenating the contained arrays. ngrams(array(T), n) -> array(array(T))# Returns n-grams (sub-sequences of adjacent n elements) for the array. The order of the n-grams in the result is unspecified. SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 2); -- [['foo', 'bar'], ['bar', 'baz'], ['baz', 'foo']] SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 3); -- [['foo', 'bar', 'baz'], ['bar', 'baz', 'foo']] SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 4); -- [['foo', 'bar', 'baz', 'foo']] SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 5); -- [['foo', 'bar', 'baz', 'foo']] SELECT ngrams(ARRAY[1, 2, 3, 4], 2); -- [[1, 2], [2, 3], [3, 4]] none_match(array(T), function(T, boolean)) → boolean# Returns whether no elements of an array match the given predicate. Returns true if none of the elements matches the predicate (a special case is when the array is empty); false if one or more elements match; NULL if the predicate function returns NULL for one or more elements and false for all other elements. reduce(array(T), initialState S, inputFunction(S, T, S), outputFunction(S, R)) → R# Returns a single value reduced from array. inputFunction will be invoked for each element in array in order. In addition to taking the element, inputFunction takes the current state, initially initialState, and returns the new state. outputFunction will be invoked to turn the final state into the result value. It may be the identity function (i -> i). SELECT reduce(ARRAY[], 0, (s, x) -> s + x, s -> s); -- 0 SELECT reduce(ARRAY[5, 20, 50], 0, (s, x) -> s + x, s -> s); -- 75 SELECT reduce(ARRAY[5, 20, NULL, 50], 0, (s, x) -> s + x, s -> s); -- NULL SELECT reduce(ARRAY[5, 20, NULL, 50], 0, (s, x) -> s + coalesce(x, 0), s -> s); -- 75 SELECT reduce(ARRAY[5, 20, NULL, 50], 0, (s, x) -> IF(x IS NULL, s, s + x), s -> s); -- 75 SELECT reduce(ARRAY[2147483647, 1], BIGINT '0', (s, x) -> s + x, s -> s); -- 2147483648 -- calculates arithmetic average SELECT reduce(ARRAY[5, 6, 10, 20], CAST(ROW(0.0, 0) AS ROW(sum DOUBLE, count INTEGER)), (s, x) -> CAST(ROW(x + s.sum, s.count + 1) AS ROW(sum DOUBLE, count INTEGER)), s -> IF(s.count = 0, NULL, s.sum / s.count)); -- 10.25 repeat(element, count) → array# Repeat element for count times. reverse(x) → array Returns an array which has the reversed order of array x. sequence(start, stop)# Generate a sequence of integers from start to stop, incrementing by 1 if start is less than or equal to stop, otherwise -1. sequence(start, stop, step) Generate a sequence of integers from start to stop, incrementing by step. sequence(start, stop) Generate a sequence of dates from start date to stop date, incrementing by 1 day if start date is less than or equal to stop date, otherwise -1 day. sequence(start, stop, step) Generate a sequence of dates from start to stop, incrementing by step. The type of step can be either INTERVAL DAY TO SECOND or INTERVAL YEAR TO MONTH. sequence(start, stop, step) Generate a sequence of timestamps from start to stop, incrementing by step. The type of step can be either INTERVAL DAY TO SECOND or INTERVAL YEAR TO MONTH. shuffle(x) → array# Generate a random permutation of the given array x. slice(x, start, length) → array# Subsets array x starting from index start (or starting from the end if start is negative) with a length of length. trim_array(x, n) → array# Remove n elements from the end of array: SELECT trim_array(ARRAY[1, 2, 3, 4], 1); -- [1, 2, 3] SELECT trim_array(ARRAY[1, 2, 3, 4], 2); -- [1, 2] transform(array(T), function(T, U)) -> array(U)# Returns an array that is the result of applying function to each element of array: SELECT transform(ARRAY[], x -> x + 1); -- [] SELECT transform(ARRAY[5, 6], x -> x + 1); -- [6, 7] SELECT transform(ARRAY[5, NULL, 6], x -> coalesce(x, 0) + 1); -- [6, 1, 7] SELECT transform(ARRAY['x', 'abc', 'z'], x -> x || '0'); -- ['x0', 'abc0', 'z0'] SELECT transform(ARRAY[ARRAY[1, NULL, 2], ARRAY[3, NULL]], a -> filter(a, x -> x IS NOT NULL)); -- [[1, 2], [3]] euclidean_distance(array(double), array(double)) → double# Calculates the euclidean distance: SELECT euclidean_distance(ARRAY[1.0, 2.0], ARRAY[3.0, 4.0]); -- 2.8284271247461903 dot_product(array(double), array(double)) → double# Calculates the dot product: SELECT dot_product(ARRAY[1.0, 2.0], ARRAY[3.0, 4.0]); -- 11.0 zip(array1, array2[, ...]) -> array(row)# Merges the given arrays, element-wise, into a single array of rows. The M-th element of the N-th argument will be the N-th field of the M-th output element. If the arguments have an uneven length, missing values are filled with NULL. SELECT zip(ARRAY[1, 2], ARRAY['1b', null, '3b']); -- [ROW(1, '1b'), ROW(2, null), ROW(null, '3b')] zip_with(array(T), array(U), function(T, U, R)) -> array(R)# Merges the two given arrays, element-wise, into a single array using function. If one array is shorter, nulls are appended at the end to match the length of the longer array, before applying function. SELECT zip_with(ARRAY[1, 3, 5], ARRAY['a', 'b', 'c'], (x, y) -> (y, x)); -- [ROW('a', 1), ROW('b', 3), ROW('c', 5)] SELECT zip_with(ARRAY[1, 2], ARRAY[3, 4], (x, y) -> x + y); -- [4, 6] SELECT zip_with(ARRAY['a', 'b', 'c'], ARRAY['d', 'e', 'f'], (x, y) -> concat(x, y)); -- ['ad', 'be', 'cf'] SELECT zip_with(ARRAY['a'], ARRAY['d', null, 'f'], (x, y) -> coalesce(x, y)); -- ['a', null, 'f']

#### Code Examples

```
SELECT ARRAY[1, 2, 4];
-- [1, 2, 4]
```
```
SELECT ARRAY['foo', 'bar', 'bazz'];
-- [foo, bar, bazz]
```
```
SELECT ARRAY[1, 1.2, 4];
-- [1.0, 1.2, 4.0]
```
```
SELECT ARRAY[1, 2, NULL, -4, NULL];
-- [1, 2, NULL, -4, NULL]
```
```
SELECT my_array[1] AS first_element
```
```
SELECT ARRAY[1, 1.2, 4][2];
-- 1.2
```
```
SELECT ARRAY[1] || ARRAY[2];
-- [1, 2]

SELECT ARRAY[1] || 2;
-- [1, 2]

SELECT 2 || ARRAY[1];
-- [2, 1]
```
```
SELECT array_histogram(ARRAY[42, 7, 42, NULL]);
-- {42=2, 7=1}
```
```
SELECT array_histogram(ARRAY[NULL, NULL]);
-- {}
```
```
SELECT array_sort(ARRAY[3, 2, 5, 1, 2],
                  (x, y) -> IF(x < y, 1, IF(x = y, 0, -1)));
-- [5, 3, 2, 2, 1]

SELECT array_sort(ARRAY['bc', 'ab', 'dc'],
                  (x, y) -> IF(x < y, 1, IF(x = y, 0, -1)));
-- ['dc', 'bc', 'ab']


SELECT array_sort(ARRAY[3, 2, null, 5, null, 1, 2],
                  -- sort null first with descending order
                  (x, y) -> CASE WHEN x IS NULL THEN -1
                                 WHEN y IS NULL THEN 1
                                 WHEN x < y THEN 1
                                 WHEN x = y THEN 0
                                 ELSE -1 END);
-- [null, null, 5, 3, 2, 2, 1]

SELECT array_sort(ARRAY[3, 2, null, 5, null, 1, 2],
                  -- sort null last with descending order
                  (x, y) -> CASE WHEN x IS NULL THEN 1
                                 WHEN y IS NULL THEN -1
                                 WHEN x < y THEN 1
                                 WHEN x = y THEN 0
                                 ELSE -1 END);
-- [5, 3, 2, 2, 1, null, null]

SELECT array_sort(ARRAY['a', 'abcd', 'abc'],
                  -- sort by string length
                  (x, y) -> IF(length(x) < length(y), -1,
                               IF(length(x) = length(y), 0, 1)));
-- ['a', 'abc', 'abcd']

SELECT array_sort(ARRAY[ARRAY[2, 3, 1], ARRAY[4, 2, 1, 4], ARRAY[1, 2]],
                  -- sort by array length
                  (x, y) -> IF(cardinality(x) < cardinality(y), -1,
                               IF(cardinality(x) = cardinality(y), 0, 1)));
-- [[1, 2], [2, 3, 1], [4, 2, 1, 4]]
```
```
SELECT combinations(ARRAY['foo', 'bar', 'baz'], 2);
-- [['foo', 'bar'], ['foo', 'baz'], ['bar', 'baz']]

SELECT combinations(ARRAY[1, 2, 3], 2);
-- [[1, 2], [1, 3], [2, 3]]

SELECT combinations(ARRAY[1, 2, 2], 2);
-- [[1, 2], [1, 2], [2, 2]]
```
```
SELECT filter(ARRAY[], x -> true);
-- []

SELECT filter(ARRAY[5, -6, NULL, 7], x -> x > 0);
-- [5, 7]

SELECT filter(ARRAY[5, NULL, 7, NULL], x -> x IS NOT NULL);
-- [5, 7]
```
```
SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 2);
-- [['foo', 'bar'], ['bar', 'baz'], ['baz', 'foo']]

SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 3);
-- [['foo', 'bar', 'baz'], ['bar', 'baz', 'foo']]

SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 4);
-- [['foo', 'bar', 'baz', 'foo']]

SELECT ngrams(ARRAY['foo', 'bar', 'baz', 'foo'], 5);
-- [['foo', 'bar', 'baz', 'foo']]

SELECT ngrams(ARRAY[1, 2, 3, 4], 2);
-- [[1, 2], [2, 3], [3, 4]]
```
```
SELECT reduce(ARRAY[], 0,
              (s, x) -> s + x,
              s -> s);
-- 0

SELECT reduce(ARRAY[5, 20, 50], 0,
              (s, x) -> s + x,
              s -> s);
-- 75

SELECT reduce(ARRAY[5, 20, NULL, 50], 0,
              (s, x) -> s + x,
              s -> s);
-- NULL

SELECT reduce(ARRAY[5, 20, NULL, 50], 0,
              (s, x) -> s + coalesce(x, 0),
              s -> s);
-- 75

SELECT reduce(ARRAY[5, 20, NULL, 50], 0,
              (s, x) -> IF(x IS NULL, s, s + x),
              s -> s);
-- 75

SELECT reduce(ARRAY[2147483647, 1], BIGINT '0',
              (s, x) -> s + x,
              s -> s);
-- 2147483648

-- calculates arithmetic average
SELECT reduce(ARRAY[5, 6, 10, 20],
              CAST(ROW(0.0, 0) AS ROW(sum DOUBLE, count INTEGER)),
              (s, x) -> CAST(ROW(x + s.sum, s.count + 1) AS
                             ROW(sum DOUBLE, count INTEGER)),
              s -> IF(s.count = 0, NULL, s.sum / s.count));
-- 10.25
```
```
SELECT trim_array(ARRAY[1, 2, 3, 4], 1);
-- [1, 2, 3]

SELECT trim_array(ARRAY[1, 2, 3, 4], 2);
-- [1, 2]
```
```
SELECT transform(ARRAY[], x -> x + 1);
-- []

SELECT transform(ARRAY[5, 6], x -> x + 1);
-- [6, 7]

SELECT transform(ARRAY[5, NULL, 6], x -> coalesce(x, 0) + 1);
-- [6, 1, 7]

SELECT transform(ARRAY['x', 'abc', 'z'], x -> x || '0');
-- ['x0', 'abc0', 'z0']

SELECT transform(ARRAY[ARRAY[1, NULL, 2], ARRAY[3, NULL]],
                 a -> filter(a, x -> x IS NOT NULL));
-- [[1, 2], [3]]
```
```
SELECT euclidean_distance(ARRAY[1.0, 2.0], ARRAY[3.0, 4.0]);
-- 2.8284271247461903
```
```
SELECT dot_product(ARRAY[1.0, 2.0], ARRAY[3.0, 4.0]);
-- 11.0
```
```
SELECT zip(ARRAY[1, 2], ARRAY['1b', null, '3b']);
-- [ROW(1, '1b'), ROW(2, null), ROW(null, '3b')]
```
```
SELECT zip_with(ARRAY[1, 3, 5], ARRAY['a', 'b', 'c'],
                (x, y) -> (y, x));
-- [ROW('a', 1), ROW('b', 3), ROW('c', 5)]

SELECT zip_with(ARRAY[1, 2], ARRAY[3, 4],
                (x, y) -> x + y);
-- [4, 6]

SELECT zip_with(ARRAY['a', 'b', 'c'], ARRAY['d', 'e', 'f'],
                (x, y) -> concat(x, y));
-- ['ad', 'be', 'cf']

SELECT zip_with(ARRAY['a'], ARRAY['d', null, 'f'],
                (x, y) -> coalesce(x, y));
-- ['a', null, 'f']
```


---

### Binary Functions Documentation
Source: https://trino.io/docs/current/functions/binary.html

Binary functions and operators# Binary operators# The || operator performs concatenation. Binary functions# concat(binary1, ..., binaryN) → varbinary Returns the concatenation of binary1, binary2, ..., binaryN. This function provides the same functionality as the SQL-standard concatenation operator (||). length(binary) → bigint Returns the length of binary in bytes. lpad(binary, size, padbinary) → varbinary Left pads binary to size bytes with padbinary. If size is less than the length of binary, the result is truncated to size characters. size must not be negative and padbinary must be non-empty. rpad(binary, size, padbinary) → varbinary Right pads binary to size bytes with padbinary. If size is less than the length of binary, the result is truncated to size characters. size must not be negative and padbinary must be non-empty. substr(binary, start) → varbinary Returns the rest of binary from the starting position start, measured in bytes. Positions start with 1. A negative starting position is interpreted as being relative to the end of the string. substr(binary, start, length) → varbinary Returns a substring from binary of length length from the starting position start, measured in bytes. Positions start with 1. A negative starting position is interpreted as being relative to the end of the string. reverse(binary) → varbinary Returns binary with the bytes in reverse order. Base64 encoding functions# The Base64 functions implement the encoding specified in RFC 4648. from_base64(string) → varbinary# Decodes binary data from the base64 encoded string. to_base64(binary) → varchar# Encodes binary into a base64 string representation. from_base64url(string) → varbinary# Decodes binary data from the base64 encoded string using the URL safe alphabet. to_base64url(binary) → varchar# Encodes binary into a base64 string representation using the URL safe alphabet. from_base32(string) → varbinary# Decodes binary data from the base32 encoded string. to_base32(binary) → varchar# Encodes binary into a base32 string representation. Hex encoding functions# from_hex(string) → varbinary# Decodes binary data from the hex encoded string. to_hex(binary) → varchar# Encodes binary into a hex string representation. Integer encoding functions# from_big_endian_32(binary) → integer# Decodes the 32-bit two’s complement big-endian binary. The input must be exactly 4 bytes. to_big_endian_32(integer) → varbinary# Encodes integer into a 32-bit two’s complement big-endian format. from_big_endian_64(binary) → bigint# Decodes the 64-bit two’s complement big-endian binary. The input must be exactly 8 bytes. to_big_endian_64(bigint) → varbinary# Encodes bigint into a 64-bit two’s complement big-endian format. Floating-point encoding functions# from_ieee754_32(binary) → real# Decodes the 32-bit big-endian binary in IEEE 754 single-precision floating-point format. The input must be exactly 4 bytes. to_ieee754_32(real) → varbinary# Encodes real into a 32-bit big-endian binary according to IEEE 754 single-precision floating-point format. from_ieee754_64(binary) → double# Decodes the 64-bit big-endian binary in IEEE 754 double-precision floating-point format. The input must be exactly 8 bytes. to_ieee754_64(double) → varbinary# Encodes double into a 64-bit big-endian binary according to IEEE 754 double-precision floating-point format. Hashing functions# crc32(binary) → bigint# Computes the CRC-32 of binary. For general purpose hashing, use xxhash64(), as it is much faster and produces a better quality hash. md5(binary) → varbinary# Computes the MD5 hash of binary. sha1(binary) → varbinary# Computes the SHA1 hash of binary. sha256(binary) → varbinary# Computes the SHA256 hash of binary. sha512(binary) → varbinary# Computes the SHA512 hash of binary. spooky_hash_v2_32(binary) → varbinary# Computes the 32-bit SpookyHashV2 hash of binary. spooky_hash_v2_64(binary) → varbinary# Computes the 64-bit SpookyHashV2 hash of binary. xxhash64(binary) → varbinary# Computes the xxHash64 hash of binary. murmur3(binary) → varbinary# Computes the 128-bit MurmurHash3 hash of binary. SELECT murmur3(from_base64('aaaaaa')); -- ba 58 55 63 55 69 b4 2f 49 20 37 2c a0 e3 96 ef HMAC functions# hmac_md5(binary, key) → varbinary# Computes HMAC with MD5 of binary with the given key. hmac_sha1(binary, key) → varbinary# Computes HMAC with SHA1 of binary with the given key. hmac_sha256(binary, key) → varbinary# Computes HMAC with SHA256 of binary with the given key. hmac_sha512(binary, key) → varbinary# Computes HMAC with SHA512 of binary with the given key.

#### Code Examples

```
SELECT murmur3(from_base64('aaaaaa'));
-- ba 58 55 63 55 69 b4 2f 49 20 37 2c a0 e3 96 ef
```


---

### Bitwise Functions Documentation
Source: https://trino.io/docs/current/functions/bitwise.html

Bitwise functions# bit_count(x, bits) → bigint# Count the number of bits set in x (treated as bits-bit signed integer) in 2’s complement representation: SELECT bit_count(9, 64); -- 2 SELECT bit_count(9, 8); -- 2 SELECT bit_count(-7, 64); -- 62 SELECT bit_count(-7, 8); -- 6 bitwise_and(x, y) → bigint# Returns the bitwise AND of x and y in 2’s complement representation. Bitwise AND of 19 (binary: 10011) and 25 (binary: 11001) results in 17 (binary: 10001): SELECT bitwise_and(19,25); -- 17 bitwise_not(x) → bigint# Returns the bitwise NOT of x in 2’s complement representation (NOT x = -x - 1): SELECT bitwise_not(-12); -- 11 SELECT bitwise_not(19); -- -20 SELECT bitwise_not(25); -- -26 bitwise_or(x, y) → bigint# Returns the bitwise OR of x and y in 2’s complement representation. Bitwise OR of 19 (binary: 10011) and 25 (binary: 11001) results in 27 (binary: 11011): SELECT bitwise_or(19,25); -- 27 bitwise_xor(x, y) → bigint# Returns the bitwise XOR of x and y in 2’s complement representation. Bitwise XOR of 19 (binary: 10011) and 25 (binary: 11001) results in 10 (binary: 01010): SELECT bitwise_xor(19,25); -- 10 bitwise_left_shift(value, shift) → [same as value]# Returns the left shifted value of value. Shifting 1 (binary: 001) by two bits results in 4 (binary: 00100): SELECT bitwise_left_shift(1, 2); -- 4 Shifting 5 (binary: 0101) by two bits results in 20 (binary: 010100): SELECT bitwise_left_shift(5, 2); -- 20 Shifting a value by 0 always results in the original value: SELECT bitwise_left_shift(20, 0); -- 20 SELECT bitwise_left_shift(42, 0); -- 42 Shifting 0 by a shift always results in 0: SELECT bitwise_left_shift(0, 1); -- 0 SELECT bitwise_left_shift(0, 2); -- 0 bitwise_right_shift(value, shift) → [same as value]# Returns the logical right shifted value of value. Shifting 8 (binary: 1000) by three bits results in 1 (binary: 001): SELECT bitwise_right_shift(8, 3); -- 1 Shifting 9 (binary: 1001) by one bit results in 4 (binary: 100): SELECT bitwise_right_shift(9, 1); -- 4 Shifting a value by 0 always results in the original value: SELECT bitwise_right_shift(20, 0); -- 20 SELECT bitwise_right_shift(42, 0); -- 42 Shifting a value by 64 or more bits results in 0: SELECT bitwise_right_shift( 12, 64); -- 0 SELECT bitwise_right_shift(-45, 64); -- 0 Shifting 0 by a shift always results in 0: SELECT bitwise_right_shift(0, 1); -- 0 SELECT bitwise_right_shift(0, 2); -- 0 bitwise_right_shift_arithmetic(value, shift) → [same as value]# Returns the arithmetic right shifted value of value. Returns the same values as bitwise_right_shift() when shifting by less than 64 bits. Shifting by 64 or more bits results in 0 for a positive and -1 for a negative value: SELECT bitwise_right_shift_arithmetic( 12, 64); -- 0 SELECT bitwise_right_shift_arithmetic(-45, 64); -- -1 See also bitwise_and_agg() and bitwise_or_agg().

#### Code Examples

```
SELECT bit_count(9, 64); -- 2
SELECT bit_count(9, 8); -- 2
SELECT bit_count(-7, 64); -- 62
SELECT bit_count(-7, 8); -- 6
```
```
SELECT bitwise_and(19,25); -- 17
```
```
SELECT bitwise_not(-12); --  11
SELECT bitwise_not(19);  -- -20
SELECT bitwise_not(25);  -- -26
```
```
SELECT bitwise_or(19,25); -- 27
```
```
SELECT bitwise_xor(19,25); -- 10
```
```
SELECT bitwise_left_shift(1, 2); -- 4
```
```
SELECT bitwise_left_shift(5, 2); -- 20
```
```
SELECT bitwise_left_shift(20, 0); -- 20
SELECT bitwise_left_shift(42, 0); -- 42
```
```
SELECT bitwise_left_shift(0, 1); -- 0
SELECT bitwise_left_shift(0, 2); -- 0
```
```
SELECT bitwise_right_shift(8, 3); -- 1
```
```
SELECT bitwise_right_shift(9, 1); -- 4
```
```
SELECT bitwise_right_shift(20, 0); -- 20
SELECT bitwise_right_shift(42, 0); -- 42
```
```
SELECT bitwise_right_shift( 12, 64); -- 0
SELECT bitwise_right_shift(-45, 64); -- 0
```
```
SELECT bitwise_right_shift(0, 1); -- 0
SELECT bitwise_right_shift(0, 2); -- 0
```
```
SELECT bitwise_right_shift_arithmetic( 12, 64); --  0
SELECT bitwise_right_shift_arithmetic(-45, 64); -- -1
```


---

### Color Functions Documentation
Source: https://trino.io/docs/current/functions/color.html

Color functions# bar(x, width) → varchar# Renders a single bar in an ANSI bar chart using a default low_color of red and a high_color of green. For example, if x of 25% and width of 40 are passed to this function. A 10-character red bar will be drawn followed by 30 spaces to create a bar of 40 characters. bar(x, width, low_color, high_color) → varchar Renders a single line in an ANSI bar chart of the specified width. The parameter x is a double value between 0 and 1. Values of x that fall outside the range [0, 1] will be truncated to either a 0 or a 1 value. The low_color and high_color capture the color to use for either end of the horizontal bar chart. For example, if x is 0.5, width is 80, low_color is 0xFF0000, and high_color is 0x00FF00 this function will return a 40 character bar that varies from red (0xFF0000) and yellow (0xFFFF00) and the remainder of the 80 character bar will be padded with spaces. color(string) → color# Returns a color capturing a decoded RGB value from a 4-character string of the format “#000”. The input string should be varchar containing a CSS-style short rgb string or one of black, red, green, yellow, blue, magenta, cyan, white. color(x, low, high, low_color, high_color) → color Returns a color interpolated between low_color and high_color using the double parameters x, low, and high to calculate a fraction which is then passed to the color(fraction, low_color, high_color) function shown below. If x falls outside the range defined by low and high its value is truncated to fit within this range. color(x, low_color, high_color) → color Returns a color interpolated between low_color and high_color according to the double argument x between 0 and 1. The parameter x is a double value between 0 and 1. Values of x that fall outside the range [0, 1] will be truncated to either a 0 or a 1 value. render(x, color) → varchar# Renders value x using the specific color using ANSI color codes. x can be either a double, bigint, or varchar. render(b) → varchar Accepts boolean value b and renders a green true or a red false using ANSI color codes. rgb(red, green, blue) → color# Returns a color value capturing the RGB value of three component color values supplied as int parameters ranging from 0 to 255: red, green, blue.

---

### Comparison Functions Documentation
Source: https://trino.io/docs/current/functions/comparison.html

Comparison functions and operators# Comparison operators# Operator Description < Less than > Greater than <= Less than or equal to >= Greater than or equal to = Equal <> Not equal != Not equal (non-standard but popular syntax) Range operator: BETWEEN# The BETWEEN operator tests if a value is within a specified range. It uses the syntax value BETWEEN min AND max: SELECT 3 BETWEEN 2 AND 6; The preceding statement is equivalent to the following statement: SELECT 3 >= 2 AND 3 <= 6; To test if a value does not fall within the specified range use NOT BETWEEN: SELECT 3 NOT BETWEEN 2 AND 6; The statement shown above is equivalent to the following statement: SELECT 3 < 2 OR 3 > 6; A NULL in a BETWEEN or NOT BETWEEN statement is evaluated using the standard NULL evaluation rules applied to the equivalent expression above: SELECT NULL BETWEEN 2 AND 4; -- null SELECT 2 BETWEEN NULL AND 6; -- null SELECT 2 BETWEEN 3 AND NULL; -- false SELECT 8 BETWEEN NULL AND 6; -- false The BETWEEN and NOT BETWEEN operators can also be used to evaluate any orderable type. For example, a VARCHAR: SELECT 'Paul' BETWEEN 'John' AND 'Ringo'; -- true Note that the value, min, and max parameters to BETWEEN and NOT BETWEEN must be the same type. For example, Trino produces an error if you ask it if John is between 2.3 and 35.2. IS NULL and IS NOT NULL# The IS NULL and IS NOT NULL operators test whether a value is null (undefined). Both operators work for all data types. Using NULL with IS NULL evaluates to true: SELECT NULL IS NULL; -- true But any other constant does not: SELECT 3.0 IS NULL; -- false IS DISTINCT FROM and IS NOT DISTINCT FROM# In SQL a NULL value signifies an unknown value, so any comparison involving a NULL produces NULL. The IS DISTINCT FROM and IS NOT DISTINCT FROM operators treat NULL as a known value and both operators guarantee either a true or false outcome even in the presence of NULL input: SELECT NULL IS DISTINCT FROM NULL; -- false SELECT NULL IS NOT DISTINCT FROM NULL; -- true In the preceding example a NULL value is not considered distinct from NULL. When you are comparing values which may include NULL use these operators to guarantee either a TRUE or FALSE result. The following truth table demonstrate the handling of NULL in IS DISTINCT FROM and IS NOT DISTINCT FROM: a b a = b a <> b a DISTINCT b a NOT DISTINCT b 1 1 TRUE FALSE FALSE TRUE 1 2 FALSE TRUE TRUE FALSE 1 NULL NULL NULL TRUE FALSE NULL NULL NULL NULL FALSE TRUE GREATEST and LEAST# These functions are not in the SQL standard, but are a common extension. Like most other functions in Trino, they return null if any argument is null. Note that in some other databases, such as PostgreSQL, they only return null if all arguments are null. The following types are supported: DOUBLE BIGINT VARCHAR TIMESTAMP TIMESTAMP WITH TIME ZONE DATE greatest(value1, value2, ..., valueN) → [same as input]# Returns the largest of the provided values. least(value1, value2, ..., valueN) → [same as input]# Returns the smallest of the provided values. Quantified comparison predicates: ALL, ANY and SOME# The ALL, ANY and SOME quantifiers can be used together with comparison operators in the following way: expression operator quantifier ( subquery ) For example: SELECT 'hello' = ANY (VALUES 'hello', 'world'); -- true SELECT 21 < ALL (VALUES 19, 20, 21); -- false SELECT 42 >= SOME (SELECT 41 UNION ALL SELECT 42 UNION ALL SELECT 43); -- true Following are the meanings of some quantifier and comparison operator combinations: Expression Meaning A = ALL (...) Evaluates to true when A is equal to all values. A <> ALL (...) Evaluates to true when A doesn’t match any value. A < ALL (...) Evaluates to true when A is smaller than the smallest value. A = ANY (...) Evaluates to true when A is equal to any of the values. This form is equivalent to A IN (...). A <> ANY (...) Evaluates to true when A doesn’t match one or more values. A < ANY (...) Evaluates to true when A is smaller than the biggest value. ANY and SOME have the same meaning and can be used interchangeably. Pattern comparison: LIKE# The LIKE operator can be used to compare values with a pattern: ... column [NOT] LIKE 'pattern' ESCAPE 'character'; Matching characters is case sensitive, and the pattern supports two symbols for matching: _ matches any single character % matches zero or more characters Typically it is often used as a condition in WHERE statements. An example is a query to find all continents starting with E, which returns Europe: SELECT * FROM (VALUES 'America', 'Asia', 'Africa', 'Europe', 'Australia', 'Antarctica') AS t (continent) WHERE continent LIKE 'E%'; You can negate the result by adding NOT, and get all other continents, all not starting with E: SELECT * FROM (VALUES 'America', 'Asia', 'Africa', 'Europe', 'Australia', 'Antarctica') AS t (continent) WHERE continent NOT LIKE 'E%'; If you only have one specific character to match, you can use the _ symbol for each character. The following query uses two underscores and produces only Asia as result: SELECT * FROM (VALUES 'America', 'Asia', 'Africa', 'Europe', 'Australia', 'Antarctica') AS t (continent) WHERE continent LIKE 'A__A'; The wildcard characters _ and % must be escaped to allow you to match them as literals. This can be achieved by specifying the ESCAPE character to use: SELECT 'South_America' LIKE 'South\_America' ESCAPE '\'; The above query returns true since the escaped underscore symbol matches. If you need to match the used escape character as well, you can escape it. If you want to match for the chosen escape character, you simply escape itself. For example, you can use \\ to match for \. Row comparison: IN# The IN operator can be used in a WHERE clause to compare column values with a list of values. The list of values can be supplied by a subquery or directly as static values in an array: ... WHERE column [NOT] IN ('value1','value2'); ... WHERE column [NOT] IN ( subquery ); Use the optional NOT keyword to negate the condition. The following example shows a simple usage with a static array: SELECT * FROM region WHERE name IN ('AMERICA', 'EUROPE'); The values in the clause are used for multiple comparisons that are combined as a logical OR. The preceding query is equivalent to the following query: SELECT * FROM region WHERE name = 'AMERICA' OR name = 'EUROPE'; You can negate the comparisons by adding NOT, and get all other regions except the values in list: SELECT * FROM region WHERE name NOT IN ('AMERICA', 'EUROPE'); When using a subquery to determine the values to use in the comparison, the subquery must return a single column and one or more rows. For example, the following query returns nation name of countries in regions starting with the letter A, specifically Africa, America, and Asia: SELECT nation.name FROM nation WHERE regionkey IN ( SELECT regionkey FROM region WHERE starts_with(name, 'A') ) ORDER by nation.name; Examples# The following example queries showcase aspects of using comparison functions and operators related to implied ordering of values, implicit casting, and different types. Ordering: SELECT 'M' BETWEEN 'A' AND 'Z'; -- true SELECT 'A' < 'B'; -- true SELECT 'A' < 'a'; -- true SELECT TRUE > FALSE; -- true SELECT 'M' BETWEEN 'A' AND 'Z'; -- true SELECT 'm' BETWEEN 'A' AND 'Z'; -- false The following queries show a subtle difference between char and varchar types. The length parameter for varchar is an optional maximum length parameter and comparison is based on the data only, ignoring the length: SELECT cast('Test' as varchar(20)) = cast('Test' as varchar(25)); --true SELECT cast('Test' as varchar(20)) = cast('Test ' as varchar(25)); --false The length parameter for char defines a fixed length character array. Comparison with different length automatically includes a cast to the same larger length. The cast is performed as automatic padding with spaces, and therefore both queries in the following return true: SELECT cast('Test' as char(20)) = cast('Test' as char(25)); -- true SELECT cast('Test' as char(20)) = cast('Test ' as char(25)); -- true The following queries show how date types are ordered, and how date is implicitly cast to timestamp with zero time values: SELECT DATE '2024-08-22' < DATE '2024-08-31'; SELECT DATE '2024-08-22' < TIMESTAMP '2024-08-22 8:00:00';

#### Code Examples

```
SELECT 3 BETWEEN 2 AND 6;
```
```
SELECT 3 >= 2 AND 3 <= 6;
```
```
SELECT 3 NOT BETWEEN 2 AND 6;
```
```
SELECT 3 < 2 OR 3 > 6;
```
```
SELECT NULL BETWEEN 2 AND 4; -- null

SELECT 2 BETWEEN NULL AND 6; -- null

SELECT 2 BETWEEN 3 AND NULL; -- false

SELECT 8 BETWEEN NULL AND 6; -- false
```
```
SELECT 'Paul' BETWEEN 'John' AND 'Ringo'; -- true
```
```
SELECT NULL IS NULL; -- true
```
```
SELECT 3.0 IS NULL; -- false
```
```
SELECT NULL IS DISTINCT FROM NULL; -- false

SELECT NULL IS NOT DISTINCT FROM NULL; -- true
```
```
expression operator quantifier ( subquery )
```
```
SELECT 'hello' = ANY (VALUES 'hello', 'world'); -- true

SELECT 21 < ALL (VALUES 19, 20, 21); -- false

SELECT 42 >= SOME (SELECT 41 UNION ALL SELECT 42 UNION ALL SELECT 43); -- true
```
```
... column [NOT] LIKE 'pattern' ESCAPE 'character';
```
```
SELECT * FROM (VALUES 'America', 'Asia', 'Africa', 'Europe', 'Australia', 'Antarctica') AS t (continent)
WHERE continent LIKE 'E%';
```
```
SELECT * FROM (VALUES 'America', 'Asia', 'Africa', 'Europe', 'Australia', 'Antarctica') AS t (continent)
WHERE continent NOT LIKE 'E%';
```
```
SELECT * FROM (VALUES 'America', 'Asia', 'Africa', 'Europe', 'Australia', 'Antarctica') AS t (continent)
WHERE continent LIKE 'A__A';
```
```
SELECT 'South_America' LIKE 'South\_America' ESCAPE '\';
```
```
... WHERE column [NOT] IN ('value1','value2');
... WHERE column [NOT] IN ( subquery );
```
```
SELECT * FROM region WHERE name IN ('AMERICA', 'EUROPE');
```
```
SELECT * FROM region WHERE name = 'AMERICA' OR name = 'EUROPE';
```
```
SELECT * FROM region WHERE name NOT IN ('AMERICA', 'EUROPE');
```
```
SELECT nation.name
FROM nation
WHERE regionkey IN (
  SELECT regionkey
  FROM region
  WHERE starts_with(name, 'A')
)
ORDER by nation.name;
```
```
SELECT 'M' BETWEEN 'A' AND 'Z'; -- true
SELECT 'A' < 'B'; -- true
SELECT 'A' < 'a'; -- true
SELECT TRUE > FALSE; -- true
SELECT 'M' BETWEEN 'A' AND 'Z'; -- true
SELECT 'm' BETWEEN 'A' AND 'Z'; -- false
```
```
SELECT cast('Test' as varchar(20)) = cast('Test' as varchar(25)); --true
SELECT cast('Test' as varchar(20)) = cast('Test   ' as varchar(25)); --false
```
```
SELECT cast('Test' as char(20)) = cast('Test' as char(25)); -- true
SELECT cast('Test' as char(20)) = cast('Test   ' as char(25)); -- true
```
```
SELECT DATE '2024-08-22' < DATE '2024-08-31';
SELECT DATE '2024-08-22' < TIMESTAMP '2024-08-22 8:00:00';
```


---

### Conditional Functions Documentation
Source: https://trino.io/docs/current/functions/conditional.html

Conditional expressions# CASE# The standard SQL CASE expression has two forms. The “simple” form searches each value expression from left to right until it finds one that equals expression: CASE expression WHEN value THEN result [ WHEN ... ] [ ELSE result ] END The result for the matching value is returned. If no match is found, the result from the ELSE clause is returned if it exists, otherwise null is returned. Example: SELECT a, CASE a WHEN 1 THEN 'one' WHEN 2 THEN 'two' ELSE 'many' END The “searched” form evaluates each boolean condition from left to right until one is true and returns the matching result: CASE WHEN condition THEN result [ WHEN ... ] [ ELSE result ] END If no conditions are true, the result from the ELSE clause is returned if it exists, otherwise null is returned. Example: SELECT a, b, CASE WHEN a = 1 THEN 'aaa' WHEN b = 2 THEN 'bbb' ELSE 'ccc' END SQL UDFs can use CASE statements that use a slightly different syntax from the CASE expressions. Specifically note the requirements for terminating each clause with a semicolon ; and the usage of END CASE. IF# The IF expression has two forms, one supplying only a true_value and the other supplying both a true_value and a false_value: if(condition, true_value)# Evaluates and returns true_value if condition is true, otherwise null is returned and true_value is not evaluated. if(condition, true_value, false_value) Evaluates and returns true_value if condition is true, otherwise evaluates and returns false_value. The following IF and CASE expressions are equivalent: SELECT orderkey, totalprice, IF(totalprice >= 150000, 'High Value', 'Low Value') FROM tpch.sf1.orders; SELECT orderkey, totalprice, CASE WHEN totalprice >= 150000 THEN 'High Value' ELSE 'Low Value' END FROM tpch.sf1.orders; SQL UDFs can use IF statements that use a slightly different syntax from IF expressions. Specifically note the requirement for terminating each clause with a semicolon ; and the usage of END IF. COALESCE# coalesce(value1, value2[, ...])# Returns the first non-null value in the argument list. Like a CASE expression, arguments are only evaluated if necessary. NULLIF# nullif(value1, value2)# Returns null if value1 equals value2, otherwise returns value1. TRY# try(expression)# Evaluate an expression and handle certain types of errors by returning NULL. In cases where it is preferable that queries produce NULL or default values instead of failing when corrupt or invalid data is encountered, the TRY function may be useful. To specify default values, the TRY function can be used in conjunction with the COALESCE function. The following errors are handled by TRY: Division by zero Invalid cast or function argument Numeric value out of range Examples# Source table with some invalid data: SELECT * FROM shipping; origin_state | origin_zip | packages | total_cost --------------+------------+----------+------------ California | 94131 | 25 | 100 California | P332a | 5 | 72 California | 94025 | 0 | 155 New Jersey | 08544 | 225 | 490 (4 rows) Query failure without TRY: SELECT CAST(origin_zip AS BIGINT) FROM shipping; Query failed: Cannot cast 'P332a' to BIGINT NULL values with TRY: SELECT TRY(CAST(origin_zip AS BIGINT)) FROM shipping; origin_zip ------------ 94131 NULL 94025 08544 (4 rows) Query failure without TRY: SELECT total_cost / packages AS per_package FROM shipping; Query failed: Division by zero Default values with TRY and COALESCE: SELECT COALESCE(TRY(total_cost / packages), 0) AS per_package FROM shipping; per_package ------------- 4 14 0 19 (4 rows)

#### Code Examples

```
CASE expression
    WHEN value THEN result
    [ WHEN ... ]
    [ ELSE result ]
END
```
```
SELECT a,
       CASE a
           WHEN 1 THEN 'one'
           WHEN 2 THEN 'two'
           ELSE 'many'
       END
```
```
CASE
    WHEN condition THEN result
    [ WHEN ... ]
    [ ELSE result ]
END
```
```
SELECT a, b,
       CASE
           WHEN a = 1 THEN 'aaa'
           WHEN b = 2 THEN 'bbb'
           ELSE 'ccc'
       END
```
```
SELECT
  orderkey,
  totalprice,
  IF(totalprice >= 150000, 'High Value', 'Low Value')
FROM tpch.sf1.orders;
```
```
SELECT
  orderkey,
  totalprice,
  CASE
    WHEN totalprice >= 150000 THEN 'High Value'
    ELSE 'Low Value'
  END
FROM tpch.sf1.orders;
```
```
SELECT * FROM shipping;
```
```
origin_state | origin_zip | packages | total_cost
--------------+------------+----------+------------
 California   |      94131 |       25 |        100
 California   |      P332a |        5 |         72
 California   |      94025 |        0 |        155
 New Jersey   |      08544 |      225 |        490
(4 rows)
```
```
SELECT CAST(origin_zip AS BIGINT) FROM shipping;
```
```
Query failed: Cannot cast 'P332a' to BIGINT
```
```
SELECT TRY(CAST(origin_zip AS BIGINT)) FROM shipping;
```
```
origin_zip
------------
      94131
 NULL
      94025
      08544
(4 rows)
```
```
SELECT total_cost / packages AS per_package FROM shipping;
```
```
Query failed: Division by zero
```
```
SELECT COALESCE(TRY(total_cost / packages), 0) AS per_package FROM shipping;
```
```
per_package
-------------
          4
         14
          0
         19
(4 rows)
```


---

### Conversion Functions Documentation
Source: https://trino.io/docs/current/functions/conversion.html

Conversion functions# Trino will implicitly convert numeric and character values to the correct type if such a conversion is possible. Trino will not convert between character and numeric types. For example, a query that expects a varchar will not automatically convert a bigint value to an equivalent varchar. When necessary, values can be explicitly cast to a particular type. Conversion functions# cast(value AS type) → type# Explicitly cast a value as a type. This can be used to cast a varchar to a numeric value type and vice versa. try_cast(value AS type) → type# Like cast(), but returns null if the cast fails. Formatting# format(format, args...) → varchar# Returns a formatted string using the specified format string and arguments: SELECT format('%s%%', 123); -- '123%' SELECT format('%.5f', pi()); -- '3.14159' SELECT format('%03d', 8); -- '008' SELECT format('%,.2f', 1234567.89); -- '1,234,567.89' SELECT format('%-7s,%7s', 'hello', 'world'); -- 'hello , world' SELECT format('%2$s %3$s %1$s', 'a', 'b', 'c'); -- 'b c a' SELECT format('%1$tA, %1$tB %1$te, %1$tY', date '2006-07-04'); -- 'Tuesday, July 4, 2006' format_number(number) → varchar# Returns a formatted string using a unit symbol: SELECT format_number(123456); -- '123K' SELECT format_number(1000000); -- '1M' Data size# The parse_data_size function supports the following units: Unit Description Value B Bytes 1 kB Kilobytes 1024 MB Megabytes 10242 GB Gigabytes 10243 TB Terabytes 10244 PB Petabytes 10245 EB Exabytes 10246 ZB Zettabytes 10247 YB Yottabytes 10248 parse_data_size(string)# Parses string of format value unit into a number, where value is the fractional number of unit values: SELECT parse_data_size('1B'); -- 1 SELECT parse_data_size('1kB'); -- 1024 SELECT parse_data_size('1MB'); -- 1048576 SELECT parse_data_size('2.3MB'); -- 2411724 Miscellaneous# typeof(expr) → varchar# Returns the name of the type of the provided expression: SELECT typeof(123); -- integer SELECT typeof('cat'); -- varchar(3) SELECT typeof(cos(2) + 1.5); -- double

#### Code Examples

```
SELECT format('%s%%', 123);
-- '123%'

SELECT format('%.5f', pi());
-- '3.14159'

SELECT format('%03d', 8);
-- '008'

SELECT format('%,.2f', 1234567.89);
-- '1,234,567.89'

SELECT format('%-7s,%7s', 'hello', 'world');
-- 'hello  ,  world'

SELECT format('%2$s %3$s %1$s', 'a', 'b', 'c');
-- 'b c a'

SELECT format('%1$tA, %1$tB %1$te, %1$tY', date '2006-07-04');
-- 'Tuesday, July 4, 2006'
```
```
SELECT format_number(123456); -- '123K'
SELECT format_number(1000000); -- '1M'
```
```
SELECT parse_data_size('1B'); -- 1
SELECT parse_data_size('1kB'); -- 1024
SELECT parse_data_size('1MB'); -- 1048576
SELECT parse_data_size('2.3MB'); -- 2411724
```
```
SELECT typeof(123); -- integer
SELECT typeof('cat'); -- varchar(3)
SELECT typeof(cos(2) + 1.5); -- double
```


---

### Date and Time Functions Documentation
Source: https://trino.io/docs/current/functions/datetime.html

Date and time functions and operators# These functions and operators operate on date and time data types. Date and time operators# Operator Example Result + date '2012-08-08' + interval '2' day 2012-08-10 + time '01:00' + interval '3' hour 04:00:00.000 + timestamp '2012-08-08 01:00' + interval '29' hour 2012-08-09 06:00:00.000 + timestamp '2012-10-31 01:00' + interval '1' month 2012-11-30 01:00:00.000 + interval '2' day + interval '3' hour 2 03:00:00.000 + interval '3' year + interval '5' month 3-5 - date '2012-08-08' - interval '2' day 2012-08-06 - time '01:00' - interval '3' hour 22:00:00.000 - timestamp '2012-08-08 01:00' - interval '29' hour 2012-08-06 20:00:00.000 - timestamp '2012-10-31 01:00' - interval '1' month 2012-09-30 01:00:00.000 - interval '2' day - interval '3' hour 1 21:00:00.000 - interval '3' year - interval '5' month 2-7 Time zone conversion# The AT TIME ZONE operator sets the time zone of a timestamp: SELECT timestamp '2012-10-31 01:00 UTC'; -- 2012-10-31 01:00:00.000 UTC SELECT timestamp '2012-10-31 01:00 UTC' AT TIME ZONE 'America/Los_Angeles'; -- 2012-10-30 18:00:00.000 America/Los_Angeles Date and time functions# current_date# Returns the current date as of the start of the query. current_time# Returns the current time with time zone as of the start of the query. current_timestamp# Returns the current timestamp with time zone as of the start of the query, with 3 digits of subsecond precision, current_timestamp(p) Returns the current timestamp with time zone as of the start of the query, with p digits of subsecond precision: SELECT current_timestamp(6); -- 2020-06-24 08:25:31.759993 America/Los_Angeles current_timezone() → varchar# Returns the current time zone in the format defined by IANA (e.g., America/Los_Angeles) or as fixed offset from UTC (e.g., +08:35) date(x) → date# This is an alias for CAST(x AS date). last_day_of_month(x) → date# Returns the last day of the month. from_iso8601_timestamp(string) → timestamp(3) with time zone# Parses the ISO 8601 formatted date string, optionally with time and time zone, into a timestamp(3) with time zone. The time defaults to 00:00:00.000, and the time zone defaults to the session time zone: SELECT from_iso8601_timestamp('2020-05-11'); -- 2020-05-11 00:00:00.000 America/Vancouver SELECT from_iso8601_timestamp('2020-05-11T11:15:05'); -- 2020-05-11 11:15:05.000 America/Vancouver SELECT from_iso8601_timestamp('2020-05-11T11:15:05.055+01:00'); -- 2020-05-11 11:15:05.055 +01:00 from_iso8601_timestamp_nanos(string) → timestamp(9) with time zone# Parses the ISO 8601 formatted date and time string. The time zone defaults to the session time zone: SELECT from_iso8601_timestamp_nanos('2020-05-11T11:15:05'); -- 2020-05-11 11:15:05.000000000 America/Vancouver SELECT from_iso8601_timestamp_nanos('2020-05-11T11:15:05.123456789+01:00'); -- 2020-05-11 11:15:05.123456789 +01:00 from_iso8601_date(string) → date# Parses the ISO 8601 formatted date string into a date. The date can be a calendar date, a week date using ISO week numbering, or year and day of year combined: SELECT from_iso8601_date('2020-05-11'); -- 2020-05-11 SELECT from_iso8601_date('2020-W10'); -- 2020-03-02 SELECT from_iso8601_date('2020-123'); -- 2020-05-02 at_timezone(timestamp(p) with time zone, zone) → timestamp(p) with time zone# Converts a timestamp(p) with time zone to a time zone specified in zone. In the following example, the input timezone is GMT, which is seven hours ahead of America/Los_Angeles in November 2022: SELECT at_timezone(TIMESTAMP '2022-11-01 09:08:07.321 GMT', 'America/Los_Angeles') -- 2022-11-01 02:08:07.321 America/Los_Angeles with_timezone(timestamp(p), zone) → timestamp(p) with time zone# Returns the timestamp specified in timestamp with the time zone specified in zone with precision p: SELECT current_timezone() -- America/New_York SELECT with_timezone(TIMESTAMP '2022-11-01 09:08:07.321', 'America/Los_Angeles') -- 2022-11-01 09:08:07.321 America/Los_Angeles from_unixtime(unixtime) → timestamp(3) with time zone# Returns the UNIX timestamp unixtime as a timestamp with time zone. unixtime is the number of seconds since 1970-01-01 00:00:00 UTC. from_unixtime(unixtime, zone) → timestamp(3) with time zone Returns the UNIX timestamp unixtime as a timestamp with time zone using zone for the time zone. unixtime is the number of seconds since 1970-01-01 00:00:00 UTC. from_unixtime(unixtime, hours, minutes) → timestamp(3) with time zone Returns the UNIX timestamp unixtime as a timestamp with time zone using hours and minutes for the time zone offset. unixtime is the number of seconds since 1970-01-01 00:00:00 in double data type. from_unixtime_nanos(unixtime) → timestamp(9) with time zone# Returns the UNIX timestamp unixtime as a timestamp with time zone. unixtime is the number of nanoseconds since 1970-01-01 00:00:00.000000000 UTC: SELECT from_unixtime_nanos(100); -- 1970-01-01 00:00:00.000000100 UTC SELECT from_unixtime_nanos(DECIMAL '1234'); -- 1970-01-01 00:00:00.000001234 UTC SELECT from_unixtime_nanos(DECIMAL '1234.499'); -- 1970-01-01 00:00:00.000001234 UTC SELECT from_unixtime_nanos(DECIMAL '-1234'); -- 1969-12-31 23:59:59.999998766 UTC localtime# Returns the current time as of the start of the query. localtimestamp# Returns the current timestamp as of the start of the query, with 3 digits of subsecond precision. localtimestamp(p) Returns the current timestamp as of the start of the query, with p digits of subsecond precision: SELECT localtimestamp(6); -- 2020-06-10 15:55:23.383628 now() → timestamp(3) with time zone# This is an alias for current_timestamp. to_iso8601(x) → varchar# Formats x as an ISO 8601 string. x can be date, timestamp, or timestamp with time zone. to_milliseconds(interval) → bigint# Returns the day-to-second interval as milliseconds. to_unixtime(timestamp) → double# Returns timestamp as a UNIX timestamp. Note The following SQL-standard functions do not use parenthesis: current_date current_time current_timestamp localtime localtimestamp Truncation function# The date_trunc function supports the following units: Unit Example Truncated Value millisecond 2001-08-22 03:04:05.321 second 2001-08-22 03:04:05.000 minute 2001-08-22 03:04:00.000 hour 2001-08-22 03:00:00.000 day 2001-08-22 00:00:00.000 week 2001-08-20 00:00:00.000 month 2001-08-01 00:00:00.000 quarter 2001-07-01 00:00:00.000 year 2001-01-01 00:00:00.000 The above examples use the timestamp 2001-08-22 03:04:05.321 as the input. date_trunc(unit, x) → [same as input]# Returns x truncated to unit: SELECT date_trunc('day' , TIMESTAMP '2022-10-20 05:10:00'); -- 2022-10-20 00:00:00.000 SELECT date_trunc('month' , TIMESTAMP '2022-10-20 05:10:00'); -- 2022-10-01 00:00:00.000 SELECT date_trunc('year', TIMESTAMP '2022-10-20 05:10:00'); -- 2022-01-01 00:00:00.000 Interval functions# The functions in this section support the following interval units: Unit Description millisecond Milliseconds second Seconds minute Minutes hour Hours day Days week Weeks month Months quarter Quarters of a year year Years date_add(unit, value, timestamp) → [same as input]# Adds an interval value of type unit to timestamp. Subtraction can be performed by using a negative value: SELECT date_add('second', 86, TIMESTAMP '2020-03-01 00:00:00'); -- 2020-03-01 00:01:26.000 SELECT date_add('hour', 9, TIMESTAMP '2020-03-01 00:00:00'); -- 2020-03-01 09:00:00.000 SELECT date_add('day', -1, TIMESTAMP '2020-03-01 00:00:00 UTC'); -- 2020-02-29 00:00:00.000 UTC date_diff(unit, timestamp1, timestamp2) → bigint# Returns timestamp2 - timestamp1 expressed in terms of unit: SELECT date_diff('second', TIMESTAMP '2020-03-01 00:00:00', TIMESTAMP '2020-03-02 00:00:00'); -- 86400 SELECT date_diff('hour', TIMESTAMP '2020-03-01 00:00:00 UTC', TIMESTAMP '2020-03-02 00:00:00 UTC'); -- 24 SELECT date_diff('day', DATE '2020-03-01', DATE '2020-03-02'); -- 1 SELECT date_diff('second', TIMESTAMP '2020-06-01 12:30:45.000000000', TIMESTAMP '2020-06-02 12:30:45.123456789'); -- 86400 SELECT date_diff('millisecond', TIMESTAMP '2020-06-01 12:30:45.000000000', TIMESTAMP '2020-06-02 12:30:45.123456789'); -- 86400123 Duration function# The parse_duration function supports the following units: Unit Description ns Nanoseconds us Microseconds ms Milliseconds s Seconds m Minutes h Hours d Days parse_duration(string) → interval# Parses string of format value unit into an interval, where value is fractional number of unit values: SELECT parse_duration('42.8ms'); -- 0 00:00:00.043 SELECT parse_duration('3.81 d'); -- 3 19:26:24.000 SELECT parse_duration('5m'); -- 0 00:05:00.000 human_readable_seconds(double) → varchar# Formats the double value of seconds into a human readable string containing weeks, days, hours, minutes, and seconds: SELECT human_readable_seconds(96); -- 1 minute, 36 seconds SELECT human_readable_seconds(3762); -- 1 hour, 2 minutes, 42 seconds SELECT human_readable_seconds(56363463); -- 93 weeks, 1 day, 8 hours, 31 minutes, 3 seconds MySQL date functions# The functions in this section use a format string that is compatible with the MySQL date_parse and str_to_date functions. The following table, based on the MySQL manual, describes the format specifiers: Specifier Description %a Abbreviated weekday name (Sun .. Sat) %b Abbreviated month name (Jan .. Dec) %c Month, numeric (1 .. 12), this specifier does not support 0 as a month. %D Day of the month with English suffix (0th, 1st, 2nd, 3rd, …) %d Day of the month, numeric (01 .. 31), this specifier does not support 0 as a month or day. %e Day of the month, numeric (1 .. 31), this specifier does not support 0 as a day. %f Fraction of second (6 digits for printing: 000000 .. 999000; 1 - 9 digits for parsing: 0 .. 999999999), timestamp is truncated to milliseconds. %H Hour (00 .. 23) %h Hour (01 .. 12) %I Hour (01 .. 12) %i Minutes, numeric (00 .. 59) %j Day of year (001 .. 366) %k Hour (0 .. 23) %l Hour (1 .. 12) %M Month name (January .. December) %m Month, numeric (01 .. 12), this specifier does not support 0 as a month. %p AM or PM %r Time of day, 12-hour (equivalent to %h:%i:%s %p) %S Seconds (00 .. 59) %s Seconds (00 .. 59) %T Time of day, 24-hour (equivalent to %H:%i:%s) %U Week (00 .. 53), where Sunday is the first day of the week %u Week (00 .. 53), where Monday is the first day of the week %V Week (01 .. 53), where Sunday is the first day of the week; used with %X %v Week (01 .. 53), where Monday is the first day of the week; used with %x %W Weekday name (Sunday .. Saturday) %w Day of the week (0 .. 6), where Sunday is the first day of the week, this specifier is not supported,consider using day_of_week() (it uses 1-7 instead of 0-6). %X Year for the week where Sunday is the first day of the week, numeric, four digits; used with %V %x Year for the week, where Monday is the first day of the week, numeric, four digits; used with %v %Y Year, numeric, four digits %y Year, numeric (two digits), when parsing, two-digit year format assumes range 1970 .. 2069, so “70” will result in year 1970 but “69” will produce 2069. %% A literal % character %x x, for any x not listed above Warning The following specifiers are not currently supported: %D %U %u %V %w %X date_format(timestamp, format) → varchar# Formats timestamp as a string using format: SELECT date_format(TIMESTAMP '2022-10-20 05:10:00', '%m-%d-%Y %H'); -- 10-20-2022 05 date_parse(string, format) → timestamp(3)# Parses string into a timestamp using format: SELECT date_parse('2022/10/20/05', '%Y/%m/%d/%H'); -- 2022-10-20 05:00:00.000 Java date functions# The functions in this section use a format string that is compatible with JodaTime’s DateTimeFormat pattern format. format_datetime(timestamp, format) → varchar# Formats timestamp as a string using format. parse_datetime(string, format) → timestamp with time zone# Parses string into a timestamp with time zone using format. Extraction function# The extract function supports the following fields: Field Description YEAR year() QUARTER quarter() MONTH month() WEEK week() DAY day() DAY_OF_MONTH day() DAY_OF_WEEK day_of_week() DOW day_of_week() DAY_OF_YEAR day_of_year() DOY day_of_year() YEAR_OF_WEEK year_of_week() YOW year_of_week() HOUR hour() MINUTE minute() SECOND second() TIMEZONE_HOUR timezone_hour() TIMEZONE_MINUTE timezone_minute() The types supported by the extract function vary depending on the field to be extracted. Most fields support all date and time types. extract(field FROM x) → bigint# Returns field from x: SELECT extract(YEAR FROM TIMESTAMP '2022-10-20 05:10:00'); -- 2022 Note This SQL-standard function uses special syntax for specifying the arguments. Convenience extraction functions# day(x) → bigint# Returns the day of the month from x. day_of_month(x) → bigint# This is an alias for day(). day_of_week(x) → bigint# Returns the ISO day of the week from x. The value ranges from 1 (Monday) to 7 (Sunday). day_of_year(x) → bigint# Returns the day of the year from x. The value ranges from 1 to 366. dow(x) → bigint# This is an alias for day_of_week(). doy(x) → bigint# This is an alias for day_of_year(). hour(x) → bigint# Returns the hour of the day from x. The value ranges from 0 to 23. millisecond(x) → bigint# Returns the millisecond of the second from x. minute(x) → bigint# Returns the minute of the hour from x. month(x) → bigint# Returns the month of the year from x. quarter(x) → bigint# Returns the quarter of the year from x. The value ranges from 1 to 4. second(x) → bigint# Returns the second of the minute from x. timezone_hour(timestamp) → bigint# Returns the hour of the time zone offset from timestamp. timezone_minute(timestamp) → bigint# Returns the minute of the time zone offset from timestamp. week(x) → bigint# Returns the ISO week of the year from x. The value ranges from 1 to 53. week_of_year(x) → bigint# This is an alias for week(). year(x) → bigint# Returns the year from x. year_of_week(x) → bigint# Returns the year of the ISO week from x. yow(x) → bigint# This is an alias for year_of_week(). timezone(timestamp(p) with time zone) → varchar# Returns the timezone identifier from timestamp(p) with time zone. The format of the returned identifier is identical to the format used in the input timestamp: SELECT timezone(TIMESTAMP '2024-01-01 12:00:00 Asia/Tokyo'); -- Asia/Tokyo SELECT timezone(TIMESTAMP '2024-01-01 12:00:00 +01:00'); -- +01:00 SELECT timezone(TIMESTAMP '2024-02-29 12:00:00 UTC'); -- UTC timezone(time(p) with time zone) → varchar Returns the timezone identifier from a time(p) with time zone. The format of the returned identifier is identical to the format used in the input time: SELECT timezone(TIME '12:00:00+09:00'); -- +09:00

#### Code Examples

```
SELECT timestamp '2012-10-31 01:00 UTC';
-- 2012-10-31 01:00:00.000 UTC

SELECT timestamp '2012-10-31 01:00 UTC' AT TIME ZONE 'America/Los_Angeles';
-- 2012-10-30 18:00:00.000 America/Los_Angeles
```
```
SELECT current_timestamp(6);
-- 2020-06-24 08:25:31.759993 America/Los_Angeles
```
```
SELECT from_iso8601_timestamp('2020-05-11');
-- 2020-05-11 00:00:00.000 America/Vancouver

SELECT from_iso8601_timestamp('2020-05-11T11:15:05');
-- 2020-05-11 11:15:05.000 America/Vancouver

SELECT from_iso8601_timestamp('2020-05-11T11:15:05.055+01:00');
-- 2020-05-11 11:15:05.055 +01:00
```
```
SELECT from_iso8601_timestamp_nanos('2020-05-11T11:15:05');
-- 2020-05-11 11:15:05.000000000 America/Vancouver

SELECT from_iso8601_timestamp_nanos('2020-05-11T11:15:05.123456789+01:00');
-- 2020-05-11 11:15:05.123456789 +01:00
```
```
SELECT from_iso8601_date('2020-05-11');
-- 2020-05-11

SELECT from_iso8601_date('2020-W10');
-- 2020-03-02

SELECT from_iso8601_date('2020-123');
-- 2020-05-02
```
```
SELECT at_timezone(TIMESTAMP '2022-11-01 09:08:07.321 GMT', 'America/Los_Angeles')
-- 2022-11-01 02:08:07.321 America/Los_Angeles
```
```
SELECT current_timezone()
-- America/New_York

SELECT with_timezone(TIMESTAMP '2022-11-01 09:08:07.321', 'America/Los_Angeles')
-- 2022-11-01 09:08:07.321 America/Los_Angeles
```
```
SELECT from_unixtime_nanos(100);
-- 1970-01-01 00:00:00.000000100 UTC

SELECT from_unixtime_nanos(DECIMAL '1234');
-- 1970-01-01 00:00:00.000001234 UTC

SELECT from_unixtime_nanos(DECIMAL '1234.499');
-- 1970-01-01 00:00:00.000001234 UTC

SELECT from_unixtime_nanos(DECIMAL '-1234');
-- 1969-12-31 23:59:59.999998766 UTC
```
```
SELECT localtimestamp(6);
-- 2020-06-10 15:55:23.383628
```
```
SELECT date_trunc('day' , TIMESTAMP '2022-10-20 05:10:00');
-- 2022-10-20 00:00:00.000

SELECT date_trunc('month' , TIMESTAMP '2022-10-20 05:10:00');
-- 2022-10-01 00:00:00.000

SELECT date_trunc('year', TIMESTAMP '2022-10-20 05:10:00');
-- 2022-01-01 00:00:00.000
```
```
SELECT date_add('second', 86, TIMESTAMP '2020-03-01 00:00:00');
-- 2020-03-01 00:01:26.000

SELECT date_add('hour', 9, TIMESTAMP '2020-03-01 00:00:00');
-- 2020-03-01 09:00:00.000

SELECT date_add('day', -1, TIMESTAMP '2020-03-01 00:00:00 UTC');
-- 2020-02-29 00:00:00.000 UTC
```
```
SELECT date_diff('second', TIMESTAMP '2020-03-01 00:00:00', TIMESTAMP '2020-03-02 00:00:00');
-- 86400

SELECT date_diff('hour', TIMESTAMP '2020-03-01 00:00:00 UTC', TIMESTAMP '2020-03-02 00:00:00 UTC');
-- 24

SELECT date_diff('day', DATE '2020-03-01', DATE '2020-03-02');
-- 1

SELECT date_diff('second', TIMESTAMP '2020-06-01 12:30:45.000000000', TIMESTAMP '2020-06-02 12:30:45.123456789');
-- 86400

SELECT date_diff('millisecond', TIMESTAMP '2020-06-01 12:30:45.000000000', TIMESTAMP '2020-06-02 12:30:45.123456789');
-- 86400123
```
```
SELECT parse_duration('42.8ms');
-- 0 00:00:00.043

SELECT parse_duration('3.81 d');
-- 3 19:26:24.000

SELECT parse_duration('5m');
-- 0 00:05:00.000
```
```
SELECT human_readable_seconds(96);
-- 1 minute, 36 seconds

SELECT human_readable_seconds(3762);
-- 1 hour, 2 minutes, 42 seconds

SELECT human_readable_seconds(56363463);
-- 93 weeks, 1 day, 8 hours, 31 minutes, 3 seconds
```
```
SELECT date_format(TIMESTAMP '2022-10-20 05:10:00', '%m-%d-%Y %H');
-- 10-20-2022 05
```
```
SELECT date_parse('2022/10/20/05', '%Y/%m/%d/%H');
-- 2022-10-20 05:00:00.000
```
```
SELECT extract(YEAR FROM TIMESTAMP '2022-10-20 05:10:00');
-- 2022
```
```
SELECT timezone(TIMESTAMP '2024-01-01 12:00:00 Asia/Tokyo'); -- Asia/Tokyo
SELECT timezone(TIMESTAMP '2024-01-01 12:00:00 +01:00'); -- +01:00
SELECT timezone(TIMESTAMP '2024-02-29 12:00:00 UTC'); -- UTC
```
```
SELECT timezone(TIME '12:00:00+09:00'); -- +09:00
```


---

### Decimal Functions Documentation
Source: https://trino.io/docs/current/functions/decimal.html

Decimal functions and operators# Decimal literals# Use the DECIMAL 'xxxxxxx.yyyyyyy' syntax to define a decimal literal. The precision of a decimal type for a literal will be equal to the number of digits in the literal (including trailing and leading zeros). The scale will be equal to the number of digits in the fractional part (including trailing zeros). Example literal Data type DECIMAL '0' DECIMAL(1) DECIMAL '12345' DECIMAL(5) DECIMAL '0000012345.1234500000' DECIMAL(20, 10) Binary arithmetic decimal operators# Standard mathematical operators are supported. The table below explains precision and scale calculation rules for result. Assuming x is of type DECIMAL(xp, xs) and y is of type DECIMAL(yp, ys). Operation Result type precision Result type scale x + y and x - y min(38, 1 + max(xs, ys) + max(xp - xs, yp - ys) ) max(xs, ys) x * y min(38, xp + yp) xs + ys x / y min(38, xp + ys-xs + max(0, ys-xs) ) max(xs, ys) x % y min(xp - xs, yp - ys) + max(xs, bs) max(xs, ys) If the mathematical result of the operation is not exactly representable with the precision and scale of the result data type, then an exception condition is raised: Value is out of range. When operating on decimal types with different scale and precision, the values are first coerced to a common super type. For types near the largest representable precision (38), this can result in Value is out of range errors when one of the operands doesn’t fit in the common super type. For example, the common super type of decimal(38, 0) and decimal(38, 1) is decimal(38, 1), but certain values that fit in decimal(38, 0) cannot be represented as a decimal(38, 1). Comparison operators# All standard Comparison functions and operators work for the decimal type. Unary decimal operators# The - operator performs negation. The type of result is same as type of argument.

#### Code Examples

```
min(38,
    1 +
    max(xs, ys) +
    max(xp - xs, yp - ys)
)
```
```
min(38, xp + yp)
```
```
min(38,
    xp + ys-xs
    + max(0, ys-xs)
    )
```
```
min(xp - xs, yp - ys) +
max(xs, bs)
```


---

### Geospatial Functions Documentation
Source: https://trino.io/docs/current/functions/geospatial.html

Geospatial functions# Trino Geospatial functions that begin with the ST_ prefix support the SQL/MM specification and are compliant with the Open Geospatial Consortium’s (OGC) OpenGIS Specifications. As such, many Trino Geospatial functions require, or more accurately, assume that geometries that are operated on are both simple and valid. For example, it does not make sense to calculate the area of a polygon that has a hole defined outside of the polygon, or to construct a polygon from a non-simple boundary line. Trino Geospatial functions support the Well-Known Text (WKT) and Well-Known Binary (WKB) form of spatial objects: POINT (0 0) LINESTRING (0 0, 1 1, 1 2) POLYGON ((0 0, 4 0, 4 4, 0 4, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1)) MULTIPOINT (0 0, 1 2) MULTILINESTRING ((0 0, 1 1, 1 2), (2 3, 3 2, 5 4)) MULTIPOLYGON (((0 0, 4 0, 4 4, 0 4, 0 0), (1 1, 2 1, 2 2, 1 2, 1 1)), ((-1 -1, -1 -2, -2 -2, -2 -1, -1 -1))) GEOMETRYCOLLECTION (POINT(2 3), LINESTRING (2 3, 3 4)) Use ST_GeometryFromText() and ST_GeomFromBinary() functions to create geometry objects from WKT or WKB. The SphericalGeography type provides native support for spatial features represented on geographic coordinates (sometimes called geodetic coordinates, or lat/lon, or lon/lat). Geographic coordinates are spherical coordinates expressed in angular units (degrees). The basis for the Geometry type is a plane. The shortest path between two points on the plane is a straight line. That means calculations on geometries (areas, distances, lengths, intersections, etc) can be calculated using cartesian mathematics and straight line vectors. The basis for the SphericalGeography type is a sphere. The shortest path between two points on the sphere is a great circle arc. That means that calculations on geographies (areas, distances, lengths, intersections, etc) must be calculated on the sphere, using more complicated mathematics. More accurate measurements that take the actual spheroidal shape of the world into account are not supported. Values returned by the measurement functions ST_Distance() and ST_Length() are in the unit of meters; values returned by ST_Area() are in square meters. Use to_spherical_geography() function to convert a geometry object to geography object. For example, ST_Distance(ST_Point(-71.0882, 42.3607), ST_Point(-74.1197, 40.6976)) returns 3.4577 in the unit of the passed-in values on the euclidean plane, while ST_Distance(to_spherical_geography(ST_Point(-71.0882, 42.3607)), to_spherical_geography(ST_Point(-74.1197, 40.6976))) returns 312822.179 in meters. Constructors# ST_AsBinary(Geometry) → varbinary# Returns the WKB representation of the geometry. ST_AsText(Geometry) → varchar# Returns the WKT representation of the geometry. For empty geometries, ST_AsText(ST_LineFromText('LINESTRING EMPTY')) will produce 'MULTILINESTRING EMPTY' and ST_AsText(ST_Polygon('POLYGON EMPTY')) will produce 'MULTIPOLYGON EMPTY'. ST_GeometryFromText(varchar) → Geometry# Returns a geometry type object from WKT representation. ST_GeomFromBinary(varbinary) → Geometry# Returns a geometry type object from WKB or EWKB representation. ST_GeomFromKML(varchar) → Geometry# Returns a geometry type object from KML representation. geometry_from_hadoop_shape(varbinary) → Geometry# Returns a geometry type object from Spatial Framework for Hadoop representation. ST_LineFromText(varchar) → LineString# Returns a geometry type linestring object from WKT representation. ST_LineString(array(Point)) → LineString# Returns a LineString formed from an array of points. If there are fewer than two non-empty points in the input array, an empty LineString will be returned. Array elements must not be NULL or the same as the previous element. The returned geometry may not be simple, e.g. may self-intersect or may contain duplicate vertexes depending on the input. ST_MultiPoint(array(Point)) → MultiPoint# Returns a MultiPoint geometry object formed from the specified points. Returns NULL if input array is empty. Array elements must not be NULL or empty. The returned geometry may not be simple and may contain duplicate points if input array has duplicates. ST_Point(lon: double, lat: double) → Point# Returns a geometry type point object with the given coordinate values. ST_Polygon(varchar) → Polygon# Returns a geometry type polygon object from WKT representation. to_spherical_geography(Geometry) → SphericalGeography# Converts a Geometry object to a SphericalGeography object on the sphere of the Earth’s radius. This function is only applicable to POINT, MULTIPOINT, LINESTRING, MULTILINESTRING, POLYGON, MULTIPOLYGON geometries defined in 2D space, or GEOMETRYCOLLECTION of such geometries. For each point of the input geometry, it verifies that point.x is within [-180.0, 180.0] and point.y is within [-90.0, 90.0], and uses them as (longitude, latitude) degrees to construct the shape of the SphericalGeography result. to_geometry(SphericalGeography) → Geometry# Converts a SphericalGeography object to a Geometry object. Relationship tests# ST_Contains(geometryA: Geometry, geometryB: Geometry) → boolean# Returns true if and only if no points of the second geometry lie in the exterior of the first geometry, and at least one point of the interior of the first geometry lies in the interior of the second geometry. ST_Crosses(first: Geometry, second: Geometry) → boolean# Returns true if the supplied geometries have some, but not all, interior points in common. ST_Disjoint(first: Geometry, second: Geometry) → boolean# Returns true if the give geometries do not spatially intersect – if they do not share any space together. ST_Equals(first: Geometry, second: Geometry) → boolean# Returns true if the given geometries represent the same geometry. ST_Intersects(first: Geometry, second: Geometry) → boolean# Returns true if the given geometries spatially intersect in two dimensions (share any portion of space) and false if they do not (they are disjoint). ST_Overlaps(first: Geometry, second: Geometry) → boolean# Returns true if the given geometries share space, are of the same dimension, but are not completely contained by each other. ST_Relate(first: Geometry, second: Geometry) → boolean# Returns true if first geometry is spatially related to second geometry. ST_Touches(first: Geometry, second: Geometry) → boolean# Returns true if the given geometries have at least one point in common, but their interiors do not intersect. ST_Within(first: Geometry, second: Geometry) → boolean# Returns true if first geometry is completely inside second geometry. Operations# geometry_nearest_points(first: Geometry, second: Geometry)# Returns the points on each geometry nearest the other. If either geometry is empty, return NULL. Otherwise, return a row of two Points that have the minimum distance of any two points on the geometries. The first Point will be from the first Geometry argument, the second from the second Geometry argument. If there are multiple pairs with the minimum distance, one pair is chosen arbitrarily. geometry_union(array(Geometry)) → Geometry# Returns a geometry that represents the point set union of the input geometries. Performance of this function, in conjunction with array_agg() to first aggregate the input geometries, may be better than geometry_union_agg(), at the expense of higher memory utilization. ST_Boundary(Geometry) → Geometry# Returns the closure of the combinatorial boundary of this geometry. ST_Buffer(Geometry, distance) → Geometry# Returns the geometry that represents all points whose distance from the specified geometry is less than or equal to the specified distance. If the points of the geometry are extremely close together (delta < 1e-8), this might return an empty geometry. ST_Difference(first: Geometry, second: Geometry) → Geometry# Returns the geometry value that represents the point set difference of the given geometries. ST_Envelope(Geometry) → Geometry# Returns the bounding rectangular polygon of a geometry. ST_EnvelopeAsPts(Geometry)# Returns an array of two points: the lower left and upper right corners of the bounding rectangular polygon of a geometry. Returns NULL if input geometry is empty. ST_ExteriorRing(Geometry) → Geometry# Returns a line string representing the exterior ring of the input polygon. ST_Intersection(first: Geometry, second: Geometry) → Geometry# Returns the geometry value that represents the point set intersection of two geometries. ST_SymDifference(first: Geometry, second: Geometry) → Geometry# Returns the geometry value that represents the point set symmetric difference of two geometries. ST_Union(first: Geometry, second: Geometry) → Geometry# Returns a geometry that represents the point set union of the input geometries. See also: geometry_union(), geometry_union_agg() Accessors# ST_Area(Geometry) → double# Returns the 2D Euclidean area of a geometry. For Point and LineString types, returns 0.0. For GeometryCollection types, returns the sum of the areas of the individual geometries. ST_Area(SphericalGeography) → double Returns the area of a polygon or multi-polygon in square meters using a spherical model for Earth. ST_Centroid(Geometry) → Geometry# Returns the point value that is the mathematical centroid of a geometry. ST_ConvexHull(Geometry) → Geometry# Returns the minimum convex geometry that encloses all input geometries. ST_CoordDim(Geometry) → bigint# Returns the coordinate dimension of the geometry. ST_Dimension(Geometry) → bigint# Returns the inherent dimension of this geometry object, which must be less than or equal to the coordinate dimension. ST_Distance(first: Geometry, second: Geometry) → double Returns the 2-dimensional cartesian minimum distance (based on spatial ref) between two geometries in projected units. ST_Distance(first: SphericalGeography, second: SphericalGeography) → double# Returns the great-circle distance in meters between two SphericalGeography points. ST_GeometryN(Geometry, index) → Geometry# Returns the geometry element at a given index (indices start at 1). If the geometry is a collection of geometries (e.g., GEOMETRYCOLLECTION or MULTI*), returns the geometry at a given index. If the given index is less than 1 or greater than the total number of elements in the collection, returns NULL. Use ST_NumGeometries() to find out the total number of elements. Singular geometries (e.g., POINT, LINESTRING, POLYGON), are treated as collections of one element. Empty geometries are treated as empty collections. ST_InteriorRingN(Geometry, index) → Geometry# Returns the interior ring element at the specified index (indices start at 1). If the given index is less than 1 or greater than the total number of interior rings in the input geometry, returns NULL. The input geometry must be a polygon. Use ST_NumInteriorRing() to find out the total number of elements. ST_GeometryType(Geometry) → varchar# Returns the type of the geometry. ST_IsClosed(Geometry) → boolean# Returns true if the linestring’s start and end points are coincident. ST_IsEmpty(Geometry) → boolean# Returns true if this Geometry is an empty geometrycollection, polygon, point etc. ST_IsSimple(Geometry) → boolean# Returns true if this Geometry has no anomalous geometric points, such as self intersection or self tangency. ST_IsRing(Geometry) → boolean# Returns true if and only if the line is closed and simple. ST_IsValid(Geometry) → boolean# Returns true if and only if the input geometry is well formed. Use geometry_invalid_reason() to determine why the geometry is not well formed. ST_Length(Geometry) → double# Returns the length of a linestring or multi-linestring using Euclidean measurement on a two dimensional plane (based on spatial ref) in projected units. ST_Length(SphericalGeography) → double Returns the length of a linestring or multi-linestring on a spherical model of the Earth. This is equivalent to the sum of great-circle distances between adjacent points on the linestring. ST_PointN(LineString, index) → Point# Returns the vertex of a linestring at a given index (indices start at 1). If the given index is less than 1 or greater than the total number of elements in the collection, returns NULL. Use ST_NumPoints() to find out the total number of elements. ST_Points(Geometry)# Returns an array of points in a linestring. ST_XMax(Geometry) → double# Returns X maxima of a bounding box of a geometry. ST_YMax(Geometry) → double# Returns Y maxima of a bounding box of a geometry. ST_XMin(Geometry) → double# Returns X minima of a bounding box of a geometry. ST_YMin(Geometry) → double# Returns Y minima of a bounding box of a geometry. ST_StartPoint(Geometry) → point# Returns the first point of a LineString geometry as a Point. This is a shortcut for ST_PointN(geometry, 1). simplify_geometry(Geometry, double) → Geometry# Returns a “simplified” version of the input geometry using the Douglas-Peucker algorithm. Will avoid creating derived geometries (polygons in particular) that are invalid. ST_EndPoint(Geometry) → point# Returns the last point of a LineString geometry as a Point. This is a shortcut for ST_PointN(geometry, ST_NumPoints(geometry)). ST_X(Point) → double# Returns the X coordinate of the point. ST_Y(Point) → double# Returns the Y coordinate of the point. ST_InteriorRings(Geometry)# Returns an array of all interior rings found in the input geometry, or an empty array if the polygon has no interior rings. Returns NULL if the input geometry is empty. The input geometry must be a polygon. ST_NumGeometries(Geometry) → bigint# Returns the number of geometries in the collection. If the geometry is a collection of geometries (e.g., GEOMETRYCOLLECTION or MULTI*), returns the number of geometries, for single geometries returns 1, for empty geometries returns 0. ST_Geometries(Geometry)# Returns an array of geometries in the specified collection. Returns a one-element array if the input geometry is not a multi-geometry. Returns NULL if input geometry is empty. ST_NumPoints(Geometry) → bigint# Returns the number of points in a geometry. This is an extension to the SQL/MM ST_NumPoints function which only applies to point and linestring. ST_NumInteriorRing(Geometry) → bigint# Returns the cardinality of the collection of interior rings of a polygon. line_interpolate_point(LineString, double) → Geometry# Returns a Point interpolated along a LineString at the fraction given. The fraction must be between 0 and 1, inclusive. line_interpolate_points(LineString, double, repeated)# Returns an array of Points interpolated along a LineString. The fraction must be between 0 and 1, inclusive. line_locate_point(LineString, Point) → double# Returns a float between 0 and 1 representing the location of the closest point on the LineString to the given Point, as a fraction of total 2d line length. Returns NULL if a LineString or a Point is empty or NULL. geometry_invalid_reason(Geometry) → varchar# Returns the reason for why the input geometry is not valid. Returns NULL if the input is valid. great_circle_distance(latitude1, longitude1, latitude2, longitude2) → double# Returns the great-circle distance between two points on Earth’s surface in kilometers. to_geojson_geometry(SphericalGeography) → varchar# Returns the GeoJSON encoded defined by the input spherical geography. from_geojson_geometry(varchar) → SphericalGeography# Returns the spherical geography type object from the GeoJSON representation stripping non geometry key/values. Feature and FeatureCollection are not supported. Aggregations# convex_hull_agg(Geometry) → Geometry# Returns the minimum convex geometry that encloses all input geometries. geometry_union_agg(Geometry) → Geometry# Returns a geometry that represents the point set union of all input geometries. Bing tiles# These functions convert between geometries and Bing tiles. bing_tile(x, y, zoom_level) → BingTile# Creates a Bing tile object from XY coordinates and a zoom level. Zoom levels from 1 to 23 are supported. bing_tile(quadKey) → BingTile Creates a Bing tile object from a quadkey. bing_tile_at(latitude, longitude, zoom_level) → BingTile# Returns a Bing tile at a given zoom level containing a point at a given latitude and longitude. Latitude must be within [-85.05112878, 85.05112878] range. Longitude must be within [-180, 180] range. Zoom levels from 1 to 23 are supported. bing_tiles_around(latitude, longitude, zoom_level)# Returns a collection of Bing tiles that surround the point specified by the latitude and longitude arguments at a given zoom level. bing_tiles_around(latitude, longitude, zoom_level, radius_in_km) Returns a minimum set of Bing tiles at specified zoom level that cover a circle of specified radius in km around a specified (latitude, longitude) point. bing_tile_coordinates(tile) → row<x, y># Returns the XY coordinates of a given Bing tile. bing_tile_polygon(tile) → Geometry# Returns the polygon representation of a given Bing tile. bing_tile_quadkey(tile) → varchar# Returns the quadkey of a given Bing tile. bing_tile_zoom_level(tile) → tinyint# Returns the zoom level of a given Bing tile. geometry_to_bing_tiles(geometry, zoom_level)# Returns the minimum set of Bing tiles that fully covers a given geometry at a given zoom level. Zoom levels from 1 to 23 are supported. Encoded polylines# These functions convert between geometries and encoded polylines. to_encoded_polyline(Geometry) → varchar# Encodes a linestring or multipoint to a polyline. from_encoded_polyline(varchar) → Geometry# Decodes a polyline to a linestring.

---

### HyperLogLog Functions Documentation
Source: https://trino.io/docs/current/functions/hyperloglog.html

HyperLogLog functions# Trino implements the approx_distinct() function using the HyperLogLog data structure. Data structures# Trino implements HyperLogLog data sketches as a set of 32-bit buckets which store a maximum hash. They can be stored sparsely (as a map from bucket ID to bucket), or densely (as a contiguous memory block). The HyperLogLog data structure starts as the sparse representation, switching to dense when it is more efficient. The P4HyperLogLog structure is initialized densely and remains dense for its lifetime. HyperLogLog implicitly casts to P4HyperLogLog, while one can explicitly cast HyperLogLog to P4HyperLogLog: cast(hll AS P4HyperLogLog) Serialization# Data sketches can be serialized to and deserialized from varbinary. This allows them to be stored for later use. Combined with the ability to merge multiple sketches, this allows one to calculate approx_distinct() of the elements of a partition of a query, then for the entirety of a query with very little cost. For example, calculating the HyperLogLog for daily unique users will allow weekly or monthly unique users to be calculated incrementally by combining the dailies. This is similar to computing weekly revenue by summing daily revenue. Uses of approx_distinct() with GROUPING SETS can be converted to use HyperLogLog. Examples: CREATE TABLE visit_summaries ( visit_date date, hll varbinary ); INSERT INTO visit_summaries SELECT visit_date, cast(approx_set(user_id) AS varbinary) FROM user_visits GROUP BY visit_date; SELECT cardinality(merge(cast(hll AS HyperLogLog))) AS weekly_unique_users FROM visit_summaries WHERE visit_date >= current_date - interval '7' day; Functions# approx_set(x) → HyperLogLog# Returns the HyperLogLog sketch of the input data set of x. This data sketch underlies approx_distinct() and can be stored and used later by calling cardinality(). cardinality(hll) → bigint This will perform approx_distinct() on the data summarized by the hll HyperLogLog data sketch. empty_approx_set() → HyperLogLog# Returns an empty HyperLogLog. merge(HyperLogLog) → HyperLogLog# Returns the HyperLogLog of the aggregate union of the individual hll HyperLogLog structures.

#### Code Examples

```
cast(hll AS P4HyperLogLog)
```
```
CREATE TABLE visit_summaries (
  visit_date date,
  hll varbinary
);

INSERT INTO visit_summaries
SELECT visit_date, cast(approx_set(user_id) AS varbinary)
FROM user_visits
GROUP BY visit_date;

SELECT cardinality(merge(cast(hll AS HyperLogLog))) AS weekly_unique_users
FROM visit_summaries
WHERE visit_date >= current_date - interval '7' day;
```


---

### IP Address Functions Documentation
Source: https://trino.io/docs/current/functions/ipaddress.html

IP Address Functions# contains(network, address) → boolean Returns true if the address exists in the CIDR network: SELECT contains('10.0.0.0/8', IPADDRESS '10.255.255.255'); -- true SELECT contains('10.0.0.0/8', IPADDRESS '11.255.255.255'); -- false SELECT contains('2001:0db8:0:0:0:ff00:0042:8329/128', IPADDRESS '2001:0db8:0:0:0:ff00:0042:8329'); -- true SELECT contains('2001:0db8:0:0:0:ff00:0042:8329/128', IPADDRESS '2001:0db8:0:0:0:ff00:0042:8328'); -- false

#### Code Examples

```
SELECT contains('10.0.0.0/8', IPADDRESS '10.255.255.255'); -- true
SELECT contains('10.0.0.0/8', IPADDRESS '11.255.255.255'); -- false

SELECT contains('2001:0db8:0:0:0:ff00:0042:8329/128', IPADDRESS '2001:0db8:0:0:0:ff00:0042:8329'); -- true
SELECT contains('2001:0db8:0:0:0:ff00:0042:8329/128', IPADDRESS '2001:0db8:0:0:0:ff00:0042:8328'); -- false
```


---

### JSON Functions Documentation
Source: https://trino.io/docs/current/functions/json.html

JSON functions and operators# The SQL standard describes functions and operators to process JSON data. They allow you to access JSON data according to its structure, generate JSON data, and store it persistently in SQL tables. Importantly, the SQL standard imposes that there is no dedicated data type to represent JSON data in SQL. Instead, JSON data is represented as character or binary strings. Although Trino supports JSON type, it is not used or produced by the following functions. Trino supports three functions for querying JSON data: json_exists, json_query, and json_value. Each of them is based on the same mechanism of exploring and processing JSON input using JSON path. Trino also supports two functions for generating JSON data – json_array, and json_object. JSON path language# The JSON path language is a special language, used exclusively by certain SQL operators to specify the query to perform on the JSON input. Although JSON path expressions are embedded in SQL queries, their syntax significantly differs from SQL. The semantics of predicates, operators, etc. in JSON path expressions generally follow the semantics of SQL. The JSON path language is case-sensitive for keywords and identifiers. JSON path syntax and semantics# JSON path expressions are recursive structures. Although the name “path” suggests a linear sequence of operations going step by step deeper into the JSON structure, a JSON path expression is in fact a tree. It can access the input JSON item multiple times, in multiple ways, and combine the results. Moreover, the result of a JSON path expression is not a single item, but an ordered sequence of items. Each of the sub-expressions takes one or more input sequences, and returns a sequence as the result. Note In the lax mode, most path operations first unnest all JSON arrays in the input sequence. Any divergence from this rule is mentioned in the following listing. Path modes are explained in JSON path modes. The JSON path language features are divided into: literals, variables, arithmetic binary expressions, arithmetic unary expressions, and a group of operators collectively known as accessors. literals# numeric literals They include exact and approximate numbers, and are interpreted as if they were SQL values. -1, 1.2e3, NaN string literals They are enclosed in double quotes. "Some text" boolean literals true, false null literal It has the semantics of the JSON null, not of SQL null. See Comparison rules. null variables# context variable It refers to the currently processed input of the JSON function. $ named variable It refers to a named parameter by its name. $param current item variable It is used inside the filter expression to refer to the currently processed item from the input sequence. @ last subscript variable It refers to the last index of the innermost enclosing array. Array indexes in JSON path expressions are zero-based. last arithmetic binary expressions# The JSON path language supports five arithmetic binary operators: <path1> + <path2> <path1> - <path2> <path1> * <path2> <path1> / <path2> <path1> % <path2> Both operands, <path1> and <path2>, are evaluated to sequences of items. For arithmetic binary operators, each input sequence must contain a single numeric item. The arithmetic operation is performed according to SQL semantics, and it returns a sequence containing a single element with the result. The operators follow the same precedence rules as in SQL arithmetic operations, and parentheses can be used for grouping. arithmetic unary expressions# + <path> - <path> The operand <path> is evaluated to a sequence of items. Every item must be a numeric value. The unary plus or minus is applied to every item in the sequence, following SQL semantics, and the results form the returned sequence. member accessor# The member accessor returns the value of the member with the specified key for each JSON object in the input sequence. <path>.key <path>."key" The condition when a JSON object does not have such a member is called a structural error. In the lax mode, it is suppressed, and the faulty object is excluded from the result. Let <path> return a sequence of three JSON objects: {"customer" : 100, "region" : "AFRICA"}, {"region" : "ASIA"}, {"customer" : 300, "region" : "AFRICA", "comment" : null} the expression <path>.customer succeeds in the first and the third object, but the second object lacks the required member. In strict mode, path evaluation fails. In lax mode, the second object is silently skipped, and the resulting sequence is 100, 300. All items in the input sequence must be JSON objects. Note Trino does not support JSON objects with duplicate keys. wildcard member accessor# Returns values from all key-value pairs for each JSON object in the input sequence. All the partial results are concatenated into the returned sequence. <path>.* Let <path> return a sequence of three JSON objects: {"customer" : 100, "region" : "AFRICA"}, {"region" : "ASIA"}, {"customer" : 300, "region" : "AFRICA", "comment" : null} The results is: 100, "AFRICA", "ASIA", 300, "AFRICA", null All items in the input sequence must be JSON objects. The order of values returned from a single JSON object is arbitrary. The sub-sequences from all JSON objects are concatenated in the same order in which the JSON objects appear in the input sequence. descendant member accessor# Returns the values associated with the specified key in all JSON objects on all levels of nesting in the input sequence. <path>..key <path>.."key" The order of returned values is that of preorder depth first search. First, the enclosing object is visited, and then all child nodes are visited. This method does not perform array unwrapping in the lax mode. The results are the same in the lax and strict modes. The method traverses into JSON arrays and JSON objects. Non-structural JSON items are skipped. Let <path> be a sequence containing a JSON object: { "id" : 1, "notes" : [{"type" : 1, "comment" : "foo"}, {"type" : 2, "comment" : null}], "comment" : ["bar", "baz"] } <path>..comment --> ["bar", "baz"], "foo", null array accessor# Returns the elements at the specified indexes for each JSON array in the input sequence. Indexes are zero-based. <path>[ <subscripts> ] The <subscripts> list contains one or more subscripts. Each subscript specifies a single index or a range (ends inclusive): <path>[<path1>, <path2> to <path3>, <path4>,...] In lax mode, any non-array items resulting from the evaluation of the input sequence are wrapped into single-element arrays. Note that this is an exception to the rule of automatic array wrapping. Each array in the input sequence is processed in the following way: The variable last is set to the last index of the array. All subscript indexes are computed in order of declaration. For a singleton subscript <path1>, the result must be a singleton numeric item. For a range subscript <path2> to <path3>, two numeric items are expected. The specified array elements are added in order to the output sequence. Let <path> return a sequence of three JSON arrays: [0, 1, 2], ["a", "b", "c", "d"], [null, null] The following expression returns a sequence containing the last element from every array: <path>[last] --> 2, "d", null The following expression returns the third and fourth element from every array: <path>[2 to 3] --> 2, "c", "d" Note that the first array does not have the fourth element, and the last array does not have the third or fourth element. Accessing non-existent elements is a structural error. In strict mode, it causes the path expression to fail. In lax mode, such errors are suppressed, and only the existing elements are returned. Another example of a structural error is an improper range specification such as 5 to 3. Note that the subscripts may overlap, and they do not need to follow the element order. The order in the returned sequence follows the subscripts: <path>[1, 0, 0] --> 1, 0, 0, "b", "a", "a", null, null, null wildcard array accessor# Returns all elements of each JSON array in the input sequence. <path>[*] In lax mode, any non-array items resulting from the evaluation of the input sequence are wrapped into single-element arrays. Note that this is an exception to the rule of automatic array wrapping. The output order follows the order of the original JSON arrays. Also, the order of elements within the arrays is preserved. Let <path> return a sequence of three JSON arrays: [0, 1, 2], ["a", "b", "c", "d"], [null, null] <path>[*] --> 0, 1, 2, "a", "b", "c", "d", null, null filter# Retrieves the items from the input sequence which satisfy the predicate. <path>?( <predicate> ) JSON path predicates are syntactically similar to boolean expressions in SQL. However, the semantics are different in many aspects: They operate on sequences of items. They have their own error handling (they never fail). They behave different depending on the lax or strict mode. The predicate evaluates to true, false, or unknown. Note that some predicate expressions involve nested JSON path expression. When evaluating the nested path, the variable @ refers to the currently examined item from the input sequence. The following predicate expressions are supported: Conjunction <predicate1> && <predicate2> Disjunction <predicate1> || <predicate2> Negation ! <predicate> exists predicate exists( <path> ) Returns true if the nested path evaluates to a non-empty sequence, and false when the nested path evaluates to an empty sequence. If the path evaluation throws an error, returns unknown. starts with predicate <path> starts with "Some text" <path> starts with $variable The nested <path> must evaluate to a sequence of textual items, and the other operand must evaluate to a single textual item. If evaluating of either operand throws an error, the result is unknown. All items from the sequence are checked for starting with the right operand. The result is true if a match is found, otherwise false. However, if any of the comparisons throws an error, the result in the strict mode is unknown. The result in the lax mode depends on whether the match or the error was found first. is unknown predicate ( <predicate> ) is unknown Returns true if the nested predicate evaluates to unknown, and false otherwise. Comparisons <path1> == <path2> <path1> <> <path2> <path1> != <path2> <path1> < <path2> <path1> > <path2> <path1> <= <path2> <path1> >= <path2> Both operands of a comparison evaluate to sequences of items. If either evaluation throws an error, the result is unknown. Items from the left and right sequence are then compared pairwise. Similarly to the starts with predicate, the result is true if any of the comparisons returns true, otherwise false. However, if any of the comparisons throws an error, for example because the compared types are not compatible, the result in the strict mode is unknown. The result in the lax mode depends on whether the true comparison or the error was found first. Comparison rules# Null values in the context of comparison behave different than SQL null: null == null –> true null != null, null < null, … –> false null compared to a scalar value –> false null compared to a JSON array or a JSON object –> false When comparing two scalar values, true or false is returned if the comparison is successfully performed. The semantics of the comparison is the same as in SQL. In case of an error, e.g. comparing text and number, unknown is returned. Comparing a scalar value with a JSON array or a JSON object, and comparing JSON arrays/objects is an error, so unknown is returned. Examples of filter# Let <path> return a sequence of three JSON objects: {"customer" : 100, "region" : "AFRICA"}, {"region" : "ASIA"}, {"customer" : 300, "region" : "AFRICA", "comment" : null} <path>?(@.region != "ASIA") --> {"customer" : 100, "region" : "AFRICA"}, {"customer" : 300, "region" : "AFRICA", "comment" : null} <path>?(!exists(@.customer)) --> {"region" : "ASIA"} The following accessors are collectively referred to as item methods. double()# Converts numeric or text values into double values. <path>.double() Let <path> return a sequence -1, 23e4, "5.6": <path>.double() --> -1e0, 23e4, 5.6e0 ceiling(), floor(), and abs()# Gets the ceiling, the floor or the absolute value for every numeric item in the sequence. The semantics of the operations is the same as in SQL. Let <path> return a sequence -1.5, -1, 1.3: <path>.ceiling() --> -1.0, -1, 2.0 <path>.floor() --> -2.0, -1, 1.0 <path>.abs() --> 1.5, 1, 1.3 keyvalue()# Returns a collection of JSON objects including one object per every member of the original object for every JSON object in the sequence. <path>.keyvalue() The returned objects have three members: “name”, which is the original key, “value”, which is the original bound value, “id”, which is the unique number, specific to an input object. Let <path> be a sequence of three JSON objects: {"customer" : 100, "region" : "AFRICA"}, {"region" : "ASIA"}, {"customer" : 300, "region" : "AFRICA", "comment" : null} <path>.keyvalue() --> {"name" : "customer", "value" : 100, "id" : 0}, {"name" : "region", "value" : "AFRICA", "id" : 0}, {"name" : "region", "value" : "ASIA", "id" : 1}, {"name" : "customer", "value" : 300, "id" : 2}, {"name" : "region", "value" : "AFRICA", "id" : 2}, {"name" : "comment", "value" : null, "id" : 2} It is required that all items in the input sequence are JSON objects. The order of the returned values follows the order of the original JSON objects. However, within objects, the order of returned entries is arbitrary. type()# Returns a textual value containing the type name for every item in the sequence. <path>.type() This method does not perform array unwrapping in the lax mode. The returned values are: "null" for JSON null, "number" for a numeric item, "string" for a textual item, "boolean" for a boolean item, "date" for an item of type date, "time without time zone" for an item of type time, "time with time zone" for an item of type time with time zone, "timestamp without time zone" for an item of type timestamp, "timestamp with time zone" for an item of type timestamp with time zone, "array" for JSON array, "object" for JSON object, size()# Returns a numeric value containing the size for every JSON array in the sequence. <path>.size() This method does not perform array unwrapping in the lax mode. Instead, all non-array items are wrapped in singleton JSON arrays, so their size is 1. It is required that all items in the input sequence are JSON arrays. Let <path> return a sequence of three JSON arrays: [0, 1, 2], ["a", "b", "c", "d"], [null, null] <path>.size() --> 3, 4, 2 Limitations# The SQL standard describes the datetime() JSON path item method and the like_regex() JSON path predicate. Trino does not support them. JSON path modes# The JSON path expression can be evaluated in two modes: strict and lax. In the strict mode, it is required that the input JSON data strictly fits the schema required by the path expression. In the lax mode, the input JSON data can diverge from the expected schema. The following table shows the differences between the two modes. Condition strict mode lax mode Performing an operation which requires a non-array on an array, e.g.: $.key requires a JSON object $.floor() requires a numeric value ERROR The array is automatically unnested, and the operation is performed on each array element. Performing an operation which requires an array on an non-array, e.g.: $[0], $[*], $.size() ERROR The non-array item is automatically wrapped in a singleton array, and the operation is performed on the array. A structural error: accessing a non-existent element of an array or a non-existent member of a JSON object, e.g.: $[-1] (array index out of bounds) $.key, where the input JSON object does not have a member key ERROR The error is suppressed, and the operation results in an empty sequence. Examples of the lax mode behavior# Let <path> return a sequence of three items, a JSON array, a JSON object, and a scalar numeric value: [1, "a", null], {"key1" : 1.0, "key2" : true}, -2e3 The following example shows the wildcard array accessor in the lax mode. The JSON array returns all its elements, while the JSON object and the number are wrapped in singleton arrays and then unnested, so effectively they appear unchanged in the output sequence: <path>[*] --> 1, "a", null, {"key1" : 1.0, "key2" : true}, -2e3 When calling the size() method, the JSON object and the number are also wrapped in singleton arrays: <path>.size() --> 3, 1, 1 In some cases, the lax mode cannot prevent failure. In the following example, even though the JSON array is unwrapped prior to calling the floor() method, the item "a" causes type mismatch. <path>.floor() --> ERROR json_exists# The json_exists function determines whether a JSON value satisfies a JSON path specification. JSON_EXISTS( json_input [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ], json_path [ PASSING json_argument [, ...] ] [ { TRUE | FALSE | UNKNOWN | ERROR } ON ERROR ] ) The json_path is evaluated using the json_input as the context variable ($), and the passed arguments as the named variables ($variable_name). The returned value is true if the path returns a non-empty sequence, and false if the path returns an empty sequence. If an error occurs, the returned value depends on the ON ERROR clause. The default value returned ON ERROR is FALSE. The ON ERROR clause is applied for the following kinds of errors: Input conversion errors, such as malformed JSON JSON path evaluation errors, e.g. division by zero json_input is a character string or a binary string. It should contain a single JSON item. For a binary string, you can specify encoding. json_path is a string literal, containing the path mode specification, and the path expression, following the syntax rules described in JSON path syntax and semantics. 'strict ($.price + $.tax)?(@ > 99.9)' 'lax $[0 to 1].floor()?(@ > 10)' In the PASSING clause you can pass arbitrary expressions to be used by the path expression. PASSING orders.totalprice AS O_PRICE, orders.tax % 10 AS O_TAX The passed parameters can be referenced in the path expression by named variables, prefixed with $. 'lax $?(@.price > $O_PRICE || @.tax > $O_TAX)' Additionally to SQL values, you can pass JSON values, specifying the format and optional encoding: PASSING orders.json_desc FORMAT JSON AS o_desc, orders.binary_record FORMAT JSON ENCODING UTF16 AS o_rec Note that the JSON path language is case-sensitive, while the unquoted SQL identifiers are upper-cased. Therefore, it is recommended to use quoted identifiers in the PASSING clause: 'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS KeyName --> ERROR; no passed value found 'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS "KeyName" --> correct Examples# Let customers be a table containing two columns: id:bigint, description:varchar. id description 101 ‘{“comment” : “nice”, “children” : [10, 13, 16]}’ 102 ‘{“comment” : “problematic”, “children” : [8, 11]}’ 103 ‘{“comment” : “knows best”, “children” : [2]}’ The following query checks which customers have children above the age of 10: SELECT id, json_exists( description, 'lax $.children[*]?(@ > 10)' ) AS children_above_ten FROM customers id children_above_ten 101 true 102 true 103 false In the following query, the path mode is strict. We check the third child for each customer. This should cause a structural error for the customers who do not have three or more children. This error is handled according to the ON ERROR clause. SELECT id, json_exists( description, 'strict $.children[2]?(@ > 10)' UNKNOWN ON ERROR ) AS child_3_above_ten FROM customers id child_3_above_ten 101 true 102 NULL 103 NULL json_query# The json_query function extracts a JSON value from a JSON value. JSON_QUERY( json_input [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ], json_path [ PASSING json_argument [, ...] ] [ RETURNING type [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ] ] [ WITHOUT [ ARRAY ] WRAPPER | WITH [ { CONDITIONAL | UNCONDITIONAL } ] [ ARRAY ] WRAPPER ] [ { KEEP | OMIT } QUOTES [ ON SCALAR STRING ] ] [ { ERROR | NULL | EMPTY ARRAY | EMPTY OBJECT } ON EMPTY ] [ { ERROR | NULL | EMPTY ARRAY | EMPTY OBJECT } ON ERROR ] ) The constant string json_path is evaluated using the json_input as the context variable ($), and the passed arguments as the named variables ($variable_name). The returned value is a JSON item returned by the path. By default, it is represented as a character string (varchar). In the RETURNING clause, you can specify other character string type or varbinary. With varbinary, you can also specify the desired encoding. json_input is a character string or a binary string. It should contain a single JSON item. For a binary string, you can specify encoding. json_path is a string literal, containing the path mode specification, and the path expression, following the syntax rules described in JSON path syntax and semantics. 'strict $.keyvalue()?(@.name == $cust_id)' 'lax $[5 to last]' In the PASSING clause you can pass arbitrary expressions to be used by the path expression. PASSING orders.custkey AS CUST_ID The passed parameters can be referenced in the path expression by named variables, prefixed with $. 'strict $.keyvalue()?(@.value == $CUST_ID)' Additionally to SQL values, you can pass JSON values, specifying the format and optional encoding: PASSING orders.json_desc FORMAT JSON AS o_desc, orders.binary_record FORMAT JSON ENCODING UTF16 AS o_rec Note that the JSON path language is case-sensitive, while the unquoted SQL identifiers are upper-cased. Therefore, it is recommended to use quoted identifiers in the PASSING clause: 'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS KeyName --> ERROR; no passed value found 'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS "KeyName" --> correct The ARRAY WRAPPER clause lets you modify the output by wrapping the results in a JSON array. WITHOUT ARRAY WRAPPER is the default option. WITH CONDITIONAL ARRAY WRAPPER wraps every result which is not a singleton JSON array or JSON object. WITH UNCONDITIONAL ARRAY WRAPPER wraps every result. The QUOTES clause lets you modify the result for a scalar string by removing the double quotes being part of the JSON string representation. Examples# Let customers be a table containing two columns: id:bigint, description:varchar. id description 101 ‘{“comment” : “nice”, “children” : [10, 13, 16]}’ 102 ‘{“comment” : “problematic”, “children” : [8, 11]}’ 103 ‘{“comment” : “knows best”, “children” : [2]}’ The following query gets the children array for each customer: SELECT id, json_query( description, 'lax $.children' ) AS children FROM customers id children 101 ‘[10,13,16]’ 102 ‘[8,11]’ 103 ‘[2]’ The following query gets the collection of children for each customer. Note that the json_query function can only output a single JSON item. If you don’t use array wrapper, you get an error for every customer with multiple children. The error is handled according to the ON ERROR clause. SELECT id, json_query( description, 'lax $.children[*]' WITHOUT ARRAY WRAPPER NULL ON ERROR ) AS children FROM customers id children 101 NULL 102 NULL 103 ‘2’ The following query gets the last child for each customer, wrapped in a JSON array: SELECT id, json_query( description, 'lax $.children[last]' WITH ARRAY WRAPPER ) AS last_child FROM customers id last_child 101 ‘[16]’ 102 ‘[11]’ 103 ‘[2]’ The following query gets all children above the age of 12 for each customer, wrapped in a JSON array. The second and the third customer don’t have children of this age. Such case is handled according to the ON EMPTY clause. The default value returned ON EMPTY is NULL. In the following example, EMPTY ARRAY ON EMPTY is specified. SELECT id, json_query( description, 'strict $.children[*]?(@ > 12)' WITH ARRAY WRAPPER EMPTY ARRAY ON EMPTY ) AS children FROM customers id children 101 ‘[13,16]’ 102 ‘[]’ 103 ‘[]’ The following query shows the result of the QUOTES clause. Note that KEEP QUOTES is the default. SELECT id, json_query(description, 'strict $.comment' KEEP QUOTES) AS quoted_comment, json_query(description, 'strict $.comment' OMIT QUOTES) AS unquoted_comment FROM customers id quoted_comment unquoted_comment 101 ‘“nice”’ ‘nice’ 102 ‘“problematic”’ ‘problematic’ 103 ‘“knows best”’ ‘knows best’ If an error occurs, the returned value depends on the ON ERROR clause. The default value returned ON ERROR is NULL. One example of error is multiple items returned by the path. Other errors caught and handled according to the ON ERROR clause are: Input conversion errors, such as malformed JSON JSON path evaluation errors, e.g. division by zero Output conversion errors json_value# The json_value function extracts a scalar SQL value from a JSON value. JSON_VALUE( json_input [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ], json_path [ PASSING json_argument [, ...] ] [ RETURNING type ] [ { ERROR | NULL | DEFAULT expression } ON EMPTY ] [ { ERROR | NULL | DEFAULT expression } ON ERROR ] ) The json_path is evaluated using the json_input as the context variable ($), and the passed arguments as the named variables ($variable_name). The returned value is the SQL scalar returned by the path. By default, it is converted to string (varchar). In the RETURNING clause, you can specify other desired type: a character string type, numeric, boolean or datetime type. json_input is a character string or a binary string. It should contain a single JSON item. For a binary string, you can specify encoding. json_path is a string literal, containing the path mode specification, and the path expression, following the syntax rules described in JSON path syntax and semantics. 'strict $.price + $tax' 'lax $[last].abs().floor()' In the PASSING clause you can pass arbitrary expressions to be used by the path expression. PASSING orders.tax AS O_TAX The passed parameters can be referenced in the path expression by named variables, prefixed with $. 'strict $[last].price + $O_TAX' Additionally to SQL values, you can pass JSON values, specifying the format and optional encoding: PASSING orders.json_desc FORMAT JSON AS o_desc, orders.binary_record FORMAT JSON ENCODING UTF16 AS o_rec Note that the JSON path language is case-sensitive, while the unquoted SQL identifiers are upper-cased. Therefore, it is recommended to use quoted identifiers in the PASSING clause: 'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS KeyName --> ERROR; no passed value found 'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS "KeyName" --> correct If the path returns an empty sequence, the ON EMPTY clause is applied. The default value returned ON EMPTY is NULL. You can also specify the default value: DEFAULT -1 ON EMPTY If an error occurs, the returned value depends on the ON ERROR clause. The default value returned ON ERROR is NULL. One example of error is multiple items returned by the path. Other errors caught and handled according to the ON ERROR clause are: Input conversion errors, such as malformed JSON JSON path evaluation errors, e.g. division by zero Returned scalar not convertible to the desired type Examples# Let customers be a table containing two columns: id:bigint, description:varchar. id description 101 ‘{“comment” : “nice”, “children” : [10, 13, 16]}’ 102 ‘{“comment” : “problematic”, “children” : [8, 11]}’ 103 ‘{“comment” : “knows best”, “children” : [2]}’ The following query gets the comment for each customer as char(12): SELECT id, json_value( description, 'lax $.comment' RETURNING char(12) ) AS comment FROM customers id comment 101 ‘nice ‘ 102 ‘problematic ‘ 103 ‘knows best ‘ The following query gets the first child’s age for each customer as tinyint: SELECT id, json_value( description, 'lax $.children[0]' RETURNING tinyint ) AS child FROM customers id child 101 10 102 8 103 2 The following query gets the third child’s age for each customer. In the strict mode, this should cause a structural error for the customers who do not have the third child. This error is handled according to the ON ERROR clause. SELECT id, json_value( description, 'strict $.children[2]' DEFAULT 'err' ON ERROR ) AS child FROM customers id child 101 ‘16’ 102 ‘err’ 103 ‘err’ After changing the mode to lax, the structural error is suppressed, and the customers without a third child produce empty sequence. This case is handled according to the ON EMPTY clause. SELECT id, json_value( description, 'lax $.children[2]' DEFAULT 'missing' ON EMPTY ) AS child FROM customers id child 101 ‘16’ 102 ‘missing’ 103 ‘missing’ json_table# The json_table clause extracts a table from a JSON value. Use this clause to transform JSON data into a relational format, making it easier to query and analyze. Use json_table in the FROM clause of a SELECT statement to create a table from JSON data. JSON_TABLE( json_input, json_path [ AS path_name ] [ PASSING value AS parameter_name [, ...] ] COLUMNS ( column_definition [, ...] ) [ PLAN ( json_table_specific_plan ) | PLAN DEFAULT ( json_table_default_plan ) ] [ { ERROR | EMPTY } ON ERROR ] ) The COLUMNS clause supports the following column_definition arguments: column_name FOR ORDINALITY | column_name type [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ] [ PATH json_path ] [ { WITHOUT | WITH { CONDITIONAL | UNCONDITIONAL } } [ ARRAY ] WRAPPER ] [ { KEEP | OMIT } QUOTES [ ON SCALAR STRING ] ] [ { ERROR | NULL | EMPTY { [ARRAY] | OBJECT } | DEFAULT expression } ON EMPTY ] [ { ERROR | NULL | DEFAULT expression } ON ERROR ] | NESTED [ PATH ] json_path [ AS path_name ] COLUMNS ( column_definition [, ...] ) json_input is a character string or a binary string. It must contain a single JSON item. json_path is a string literal containing the path mode specification and the path expression. It follows the syntax rules described in JSON path syntax and semantics. 'strict ($.price + $.tax)?(@ > 99.9)' 'lax $[0 to 1].floor()?(@ > 10)' In the PASSING clause, pass values as named parameters that the json_path expression can reference. PASSING orders.totalprice AS o_price, orders.tax % 10 AS o_tax Use named parameters to reference the values in the path expression. Prefix named parameters with $. 'lax $?(@.price > $o_price || @.tax > $o_tax)' You can also pass JSON values in the PASSING clause. Use FORMAT JSON to specify the format and ENCODING to specify the encoding: PASSING orders.json_desc FORMAT JSON AS o_desc, orders.binary_record FORMAT JSON ENCODING UTF16 AS o_rec The json_path value is case-sensitive. The SQL identifiers are uppercase. Use quoted identifiers in the PASSING clause: 'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS KeyName --> ERROR; no passed value found 'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS "KeyName" --> correct The PLAN clause specifies how to join columns from different paths. Use OUTER or INNER to define how to join parent paths with their child paths. Use CROSS or UNION to join siblings. COLUMNS defines the schema of your table. Each column_definition specifies how to extract and format your json_input value into a relational column. PLAN is an optional clause to control how to process and join nested JSON data. ON ERROR specifies how to handle processing errors. ERROR ON ERROR throws an error. EMPTY ON ERROR returns an empty result set. column_name specifies a column name. FOR ORDINALITY adds a row number column to the output table, starting at 1. Specify the column name in the column definition: row_num FOR ORDINALITY NESTED PATH extracts data from nested levels of a json_input value. Each NESTED PATH clause can contain column_definition values. The json_table function returns a result set that you can use like any other table in your queries. You can join the result set with other tables or combine multiple arrays from your JSON data. You can also process nested JSON objects without parsing the data multiple times. Use json_table as a lateral join to process JSON data from another table. Examples# The following query uses json_table to extract values from a JSON array and return them as rows in a table with three columns: SELECT * FROM json_table( '[ {"id":1,"name":"Africa","wikiDataId":"Q15"}, {"id":2,"name":"Americas","wikiDataId":"Q828"}, {"id":3,"name":"Asia","wikiDataId":"Q48"}, {"id":4,"name":"Europe","wikiDataId":"Q51"} ]', 'strict $' COLUMNS ( NESTED PATH 'strict $[*]' COLUMNS ( id integer PATH 'strict $.id', name varchar PATH 'strict $.name', wiki_data_id varchar PATH 'strict $."wikiDataId"' ) ) ); id child wiki_data_id 1 Africa Q1 2 Americas Q828 3 Asia Q48 4 Europe Q51 The following query uses json_table to extract values from an array of nested JSON objects. It flattens the nested JSON data into a single table. The example query processes an array of continent names, where each continent contains an array of countries and their populations. The NESTED PATH 'lax $[*]' clause iterates through the continent objects, while the NESTED PATH 'lax $.countries[*]' iterates through each country within each continent. This creates a flat table structure with four rows combining each continent with each of its countries. Continent values repeat for each of their countries. SELECT * FROM json_table( '[ {"continent": "Asia", "countries": [ {"name": "Japan", "population": 125.7}, {"name": "Thailand", "population": 71.6} ]}, {"continent": "Europe", "countries": [ {"name": "France", "population": 67.4}, {"name": "Germany", "population": 83.2} ]} ]', 'lax $' COLUMNS ( NESTED PATH 'lax $[*]' COLUMNS ( continent varchar PATH 'lax $.continent', NESTED PATH 'lax $.countries[*]' COLUMNS ( country varchar PATH 'lax $.name', population double PATH 'lax $.population' ) ) )); continent country population Asia Japan 125.7 Asia Thailand 71.6 Europe France 67.4 Europe Germany 83.2 The following query uses PLAN to specify an OUTER join between a parent path and a child path: SELECT * FROM JSON_TABLE( '[]', 'lax $' AS "root_path" COLUMNS( a varchar(1) PATH 'lax "A"', NESTED PATH 'lax $[*]' AS "nested_path" COLUMNS (b varchar(1) PATH 'lax "B"')) PLAN ("root_path" OUTER "nested_path") ); a b A null The following query uses PLAN to specify an INNER join between a parent path and a child path: SELECT * FROM JSON_TABLE( '[]', 'lax $' AS "root_path" COLUMNS( a varchar(1) PATH 'lax "A"', NESTED PATH 'lax $[*]' AS "nested_path" COLUMNS (b varchar(1) PATH 'lax "B"')) PLAN ("root_path" INNER "nested_path") ); a b null null json_array# The json_array function creates a JSON array containing given elements. JSON_ARRAY( [ array_element [, ...] [ { NULL ON NULL | ABSENT ON NULL } ] ], [ RETURNING type [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ] ] ) Argument types# The array elements can be arbitrary expressions. Each passed value is converted into a JSON item according to its type, and optional FORMAT and ENCODING specification. You can pass SQL values of types boolean, numeric, and character string. They are converted to corresponding JSON literals: SELECT json_array(true, 12e-1, 'text') --> '[true,1.2,"text"]' Additionally to SQL values, you can pass JSON values. They are character or binary strings with a specified format and optional encoding: SELECT json_array( '[ "text" ] ' FORMAT JSON, X'5B0035005D00' FORMAT JSON ENCODING UTF16 ) --> '[["text"],[5]]' You can also nest other JSON-returning functions. In that case, the FORMAT option is implicit: SELECT json_array( json_query('{"key" : [ "value" ]}', 'lax $.key') ) --> '[["value"]]' Other passed values are cast to varchar, and they become JSON text literals: SELECT json_array( DATE '2001-01-31', UUID '12151fd2-7586-11e9-8f9e-2a86e4085a59' ) --> '["2001-01-31","12151fd2-7586-11e9-8f9e-2a86e4085a59"]' You can omit the arguments altogether to get an empty array: SELECT json_array() --> '[]' Null handling# If a value passed for an array element is null, it is treated according to the specified null treatment option. If ABSENT ON NULL is specified, the null element is omitted in the result. If NULL ON NULL is specified, JSON null is added to the result. ABSENT ON NULL is the default configuration: SELECT json_array(true, null, 1) --> '[true,1]' SELECT json_array(true, null, 1 ABSENT ON NULL) --> '[true,1]' SELECT json_array(true, null, 1 NULL ON NULL) --> '[true,null,1]' Returned type# The SQL standard imposes that there is no dedicated data type to represent JSON data in SQL. Instead, JSON data is represented as character or binary strings. By default, the json_array function returns varchar containing the textual representation of the JSON array. With the RETURNING clause, you can specify other character string type: SELECT json_array(true, 1 RETURNING VARCHAR(100)) --> '[true,1]' You can also specify to use varbinary and the required encoding as return type. The default encoding is UTF8: SELECT json_array(true, 1 RETURNING VARBINARY) --> X'5b 74 72 75 65 2c 31 5d' SELECT json_array(true, 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF8) --> X'5b 74 72 75 65 2c 31 5d' SELECT json_array(true, 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF16) --> X'5b 00 74 00 72 00 75 00 65 00 2c 00 31 00 5d 00' SELECT json_array(true, 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF32) --> X'5b 00 00 00 74 00 00 00 72 00 00 00 75 00 00 00 65 00 00 00 2c 00 00 00 31 00 00 00 5d 00 00 00' json_object# The json_object function creates a JSON object containing given key-value pairs. JSON_OBJECT( [ key_value [, ...] [ { NULL ON NULL | ABSENT ON NULL } ] ], [ { WITH UNIQUE [ KEYS ] | WITHOUT UNIQUE [ KEYS ] } ] [ RETURNING type [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ] ] ) Argument passing conventions# There are two conventions for passing keys and values: SELECT json_object('key1' : 1, 'key2' : true) --> '{"key1":1,"key2":true}' SELECT json_object(KEY 'key1' VALUE 1, KEY 'key2' VALUE true) --> '{"key1":1,"key2":true}' In the second convention, you can omit the KEY keyword: SELECT json_object('key1' VALUE 1, 'key2' VALUE true) --> '{"key1":1,"key2":true}' Argument types# The keys can be arbitrary expressions. They must be of character string type. Each key is converted into a JSON text item, and it becomes a key in the created JSON object. Keys must not be null. The values can be arbitrary expressions. Each passed value is converted into a JSON item according to its type, and optional FORMAT and ENCODING specification. You can pass SQL values of types boolean, numeric, and character string. They are converted to corresponding JSON literals: SELECT json_object('x' : true, 'y' : 12e-1, 'z' : 'text') --> '{"x":true,"y":1.2,"z":"text"}' Additionally to SQL values, you can pass JSON values. They are character or binary strings with a specified format and optional encoding: SELECT json_object( 'x' : '[ "text" ] ' FORMAT JSON, 'y' : X'5B0035005D00' FORMAT JSON ENCODING UTF16 ) --> '{"x":["text"],"y":[5]}' You can also nest other JSON-returning functions. In that case, the FORMAT option is implicit: SELECT json_object( 'x' : json_query('{"key" : [ "value" ]}', 'lax $.key') ) --> '{"x":["value"]}' Other passed values are cast to varchar, and they become JSON text literals: SELECT json_object( 'x' : DATE '2001-01-31', 'y' : UUID '12151fd2-7586-11e9-8f9e-2a86e4085a59' ) --> '{"x":"2001-01-31","y":"12151fd2-7586-11e9-8f9e-2a86e4085a59"}' You can omit the arguments altogether to get an empty object: SELECT json_object() --> '{}' Null handling# The values passed for JSON object keys must not be null. It is allowed to pass null for JSON object values. A null value is treated according to the specified null treatment option. If NULL ON NULL is specified, a JSON object entry with null value is added to the result. If ABSENT ON NULL is specified, the entry is omitted in the result. NULL ON NULL is the default configuration.: SELECT json_object('x' : null, 'y' : 1) --> '{"x":null,"y":1}' SELECT json_object('x' : null, 'y' : 1 NULL ON NULL) --> '{"x":null,"y":1}' SELECT json_object('x' : null, 'y' : 1 ABSENT ON NULL) --> '{"y":1}' Key uniqueness# If a duplicate key is encountered, it is handled according to the specified key uniqueness constraint. If WITH UNIQUE KEYS is specified, a duplicate key results in a query failure: SELECT json_object('x' : null, 'x' : 1 WITH UNIQUE KEYS) --> failure: "duplicate key passed to JSON_OBJECT function" Note that this option is not supported if any of the arguments has a FORMAT specification. If WITHOUT UNIQUE KEYS is specified, duplicate keys are not supported due to implementation limitation. WITHOUT UNIQUE KEYS is the default configuration. Returned type# The SQL standard imposes that there is no dedicated data type to represent JSON data in SQL. Instead, JSON data is represented as character or binary strings. By default, the json_object function returns varchar containing the textual representation of the JSON object. With the RETURNING clause, you can specify other character string type: SELECT json_object('x' : 1 RETURNING VARCHAR(100)) --> '{"x":1}' You can also specify to use varbinary and the required encoding as return type. The default encoding is UTF8: SELECT json_object('x' : 1 RETURNING VARBINARY) --> X'7b 22 78 22 3a 31 7d' SELECT json_object('x' : 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF8) --> X'7b 22 78 22 3a 31 7d' SELECT json_object('x' : 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF16) --> X'7b 00 22 00 78 00 22 00 3a 00 31 00 7d 00' SELECT json_object('x' : 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF32) --> X'7b 00 00 00 22 00 00 00 78 00 00 00 22 00 00 00 3a 00 00 00 31 00 00 00 7d 00 00 00' Warning The following functions and operators are not compliant with the SQL standard, and should be considered deprecated. According to the SQL standard, there shall be no JSON data type. Instead, JSON values should be represented as string values. The remaining functionality of the following functions is covered by the functions described previously. Cast to JSON# The following types can be cast to JSON: BOOLEAN TINYINT SMALLINT INTEGER BIGINT REAL DOUBLE VARCHAR Additionally, ARRAY, MAP, and ROW types can be cast to JSON when the following requirements are met: ARRAY types can be cast when the element type of the array is one of the supported types. MAP types can be cast when the key type of the map is VARCHAR and the value type of the map is a supported type, ROW types can be cast when every field type of the row is a supported type. Note Cast operations with supported character string types treat the input as a string, not validated as JSON. This means that a cast operation with a string-type input of invalid JSON results in a successful cast to invalid JSON. Instead, consider using the json_parse() function to create validated JSON from a string. The following examples show the behavior of casting to JSON with these types: SELECT CAST(NULL AS JSON); -- NULL SELECT CAST(1 AS JSON); -- JSON '1' SELECT CAST(9223372036854775807 AS JSON); -- JSON '9223372036854775807' SELECT CAST('abc' AS JSON); -- JSON '"abc"' SELECT CAST(true AS JSON); -- JSON 'true' SELECT CAST(1.234 AS JSON); -- JSON '1.234' SELECT CAST(ARRAY[1, 23, 456] AS JSON); -- JSON '[1,23,456]' SELECT CAST(ARRAY[1, NULL, 456] AS JSON); -- JSON '[1,null,456]' SELECT CAST(ARRAY[ARRAY[1, 23], ARRAY[456]] AS JSON); -- JSON '[[1,23],[456]]' SELECT CAST(MAP(ARRAY['k1', 'k2', 'k3'], ARRAY[1, 23, 456]) AS JSON); -- JSON '{"k1":1,"k2":23,"k3":456}' SELECT CAST(CAST(ROW(123, 'abc', true) AS ROW(v1 BIGINT, v2 VARCHAR, v3 BOOLEAN)) AS JSON); -- JSON '{"v1":123,"v2":"abc","v3":true}' Casting from NULL to JSON is not straightforward. Casting from a standalone NULL will produce SQL NULL instead of JSON 'null'. However, when casting from arrays or map containing NULLs, the produced JSON will have nulls in it. Cast from JSON# Casting to BOOLEAN, TINYINT, SMALLINT, INTEGER, BIGINT, REAL, DOUBLE or VARCHAR is supported. Casting to ARRAY and MAP is supported when the element type of the array is one of the supported types, or when the key type of the map is VARCHAR and value type of the map is one of the supported types. Behaviors of the casts are shown with the examples below: SELECT CAST(JSON 'null' AS VARCHAR); -- NULL SELECT CAST(JSON '1' AS INTEGER); -- 1 SELECT CAST(JSON '9223372036854775807' AS BIGINT); -- 9223372036854775807 SELECT CAST(JSON '"abc"' AS VARCHAR); -- abc SELECT CAST(JSON 'true' AS BOOLEAN); -- true SELECT CAST(JSON '1.234' AS DOUBLE); -- 1.234 SELECT CAST(JSON '[1,23,456]' AS ARRAY(INTEGER)); -- [1, 23, 456] SELECT CAST(JSON '[1,null,456]' AS ARRAY(INTEGER)); -- [1, NULL, 456] SELECT CAST(JSON '[[1,23],[456]]' AS ARRAY(ARRAY(INTEGER))); -- [[1, 23], [456]] SELECT CAST(JSON '{"k1":1,"k2":23,"k3":456}' AS MAP(VARCHAR, INTEGER)); -- {k1=1, k2=23, k3=456} SELECT CAST(JSON '{"v1":123,"v2":"abc","v3":true}' AS ROW(v1 BIGINT, v2 VARCHAR, v3 BOOLEAN)); -- {v1=123, v2=abc, v3=true} SELECT CAST(JSON '[123,"abc",true]' AS ROW(v1 BIGINT, v2 VARCHAR, v3 BOOLEAN)); -- {v1=123, v2=abc, v3=true} JSON arrays can have mixed element types and JSON maps can have mixed value types. This makes it impossible to cast them to SQL arrays and maps in some cases. To address this, Trino supports partial casting of arrays and maps: SELECT CAST(JSON '[[1, 23], 456]' AS ARRAY(JSON)); -- [JSON '[1,23]', JSON '456'] SELECT CAST(JSON '{"k1": [1, 23], "k2": 456}' AS MAP(VARCHAR, JSON)); -- {k1 = JSON '[1,23]', k2 = JSON '456'} SELECT CAST(JSON '[null]' AS ARRAY(JSON)); -- [JSON 'null'] When casting from JSON to ROW, both JSON array and JSON object are supported. Other JSON functions# In addition to the functions explained in more details in the preceding sections, the following functions are available: is_json_scalar(json) → boolean# Determine if json is a scalar (i.e. a JSON number, a JSON string, true, false or null): SELECT is_json_scalar('1'); -- true SELECT is_json_scalar('[1, 2, 3]'); -- false json_array_contains(json, value) → boolean# Determine if value exists in json (a string containing a JSON array): SELECT json_array_contains('[1, 2, 3]', 2); -- true json_array_get(json_array, index) → json# Warning The semantics of this function are broken. If the extracted element is a string, it will be converted into an invalid JSON value that is not properly quoted (the value will not be surrounded by quotes and any interior quotes will not be escaped). We recommend against using this function. It cannot be fixed without impacting existing usages and may be removed in a future release. Returns the element at the specified index into the json_array. The index is zero-based: SELECT json_array_get('["a", [3, 9], "c"]', 0); -- JSON 'a' (invalid JSON) SELECT json_array_get('["a", [3, 9], "c"]', 1); -- JSON '[3,9]' This function also supports negative indexes for fetching element indexed from the end of an array: SELECT json_array_get('["c", [3, 9], "a"]', -1); -- JSON 'a' (invalid JSON) SELECT json_array_get('["c", [3, 9], "a"]', -2); -- JSON '[3,9]' If the element at the specified index doesn’t exist, the function returns null: SELECT json_array_get('[]', 0); -- NULL SELECT json_array_get('["a", "b", "c"]', 10); -- NULL SELECT json_array_get('["c", "b", "a"]', -10); -- NULL json_array_length(json) → bigint# Returns the array length of json (a string containing a JSON array): SELECT json_array_length('[1, 2, 3]'); -- 3 json_extract(json, json_path) → json# Evaluates the JSONPath-like expression json_path on json (a string containing JSON) and returns the result as a JSON string: SELECT json_extract(json, '$.store.book'); SELECT json_extract(json, '$.store[book]'); SELECT json_extract(json, '$.store["book name"]'); The json_query function provides a more powerful and feature-rich alternative to parse and extract JSON data. json_extract_scalar(json, json_path) → varchar# Like json_extract(), but returns the result value as a string (as opposed to being encoded as JSON). The value referenced by json_path must be a scalar (boolean, number or string). SELECT json_extract_scalar('[1, 2, 3]', '$[2]'); SELECT json_extract_scalar(json, '$.store.book[0].author'); json_format(json) → varchar# Returns the JSON text serialized from the input JSON value. This is inverse function to json_parse(). SELECT json_format(JSON '[1, 2, 3]'); -- '[1,2,3]' SELECT json_format(JSON '"a"'); -- '"a"' Note json_format() and CAST(json AS VARCHAR) have completely different semantics. json_format() serializes the input JSON value to JSON text conforming to RFC 7159. The JSON value can be a JSON object, a JSON array, a JSON string, a JSON number, true, false or null. SELECT json_format(JSON '{"a": 1, "b": 2}'); -- '{"a":1,"b":2}' SELECT json_format(JSON '[1, 2, 3]'); -- '[1,2,3]' SELECT json_format(JSON '"abc"'); -- '"abc"' SELECT json_format(JSON '42'); -- '42' SELECT json_format(JSON 'true'); -- 'true' SELECT json_format(JSON 'null'); -- 'null' CAST(json AS VARCHAR) casts the JSON value to the corresponding SQL VARCHAR value. For JSON string, JSON number, true, false or null, the cast behavior is same as the corresponding SQL type. JSON object and JSON array cannot be cast to VARCHAR. SELECT CAST(JSON '{"a": 1, "b": 2}' AS VARCHAR); -- ERROR! SELECT CAST(JSON '[1, 2, 3]' AS VARCHAR); -- ERROR! SELECT CAST(JSON '"abc"' AS VARCHAR); -- 'abc' (the double quote is gone) SELECT CAST(JSON '42' AS VARCHAR); -- '42' SELECT CAST(JSON 'true' AS VARCHAR); -- 'true' SELECT CAST(JSON 'null' AS VARCHAR); -- NULL json_parse(string) → json# Returns the JSON value deserialized from the input JSON text. This is inverse function to json_format(): SELECT json_parse('[1, 2, 3]'); -- JSON '[1,2,3]' SELECT json_parse('"abc"'); -- JSON '"abc"' Note json_parse() and CAST(string AS JSON) have completely different semantics. json_parse() expects a JSON text conforming to RFC 7159, and returns the JSON value deserialized from the JSON text. The JSON value can be a JSON object, a JSON array, a JSON string, a JSON number, true, false or null. SELECT json_parse('not_json'); -- ERROR! SELECT json_parse('["a": 1, "b": 2]'); -- JSON '["a": 1, "b": 2]' SELECT json_parse('[1, 2, 3]'); -- JSON '[1,2,3]' SELECT json_parse('"abc"'); -- JSON '"abc"' SELECT json_parse('42'); -- JSON '42' SELECT json_parse('true'); -- JSON 'true' SELECT json_parse('null'); -- JSON 'null' CAST(string AS JSON) takes any VARCHAR value as input, and returns a JSON string with its value set to input string. SELECT CAST('not_json' AS JSON); -- JSON '"not_json"' SELECT CAST('["a": 1, "b": 2]' AS JSON); -- JSON '"[\"a\": 1, \"b\": 2]"' SELECT CAST('[1, 2, 3]' AS JSON); -- JSON '"[1, 2, 3]"' SELECT CAST('"abc"' AS JSON); -- JSON '"\"abc\""' SELECT CAST('42' AS JSON); -- JSON '"42"' SELECT CAST('true' AS JSON); -- JSON '"true"' SELECT CAST('null' AS JSON); -- JSON '"null"' json_size(json, json_path) → bigint# Like json_extract(), but returns the size of the value. For objects or arrays, the size is the number of members, and the size of a scalar value is zero. SELECT json_size('{"x": {"a": 1, "b": 2}}', '$.x'); -- 2 SELECT json_size('{"x": [1, 2, 3]}', '$.x'); -- 3 SELECT json_size('{"x": {"a": 1, "b": 2}}', '$.x.a'); -- 0

#### Code Examples

```
-1, 1.2e3, NaN
```
```
"Some text"
```
```
true, false
```
```
null
```
```
$
```
```
$param
```
```
@
```
```
last
```
```
<path1> + <path2>
<path1> - <path2>
<path1> * <path2>
<path1> / <path2>
<path1> % <path2>
```
```
+ <path>
- <path>
```
```
<path>.key
<path>."key"
```
```
{"customer" : 100, "region" : "AFRICA"},
{"region" : "ASIA"},
{"customer" : 300, "region" : "AFRICA", "comment" : null}
```
```
<path>.*
```
```
{"customer" : 100, "region" : "AFRICA"},
{"region" : "ASIA"},
{"customer" : 300, "region" : "AFRICA", "comment" : null}
```
```
100, "AFRICA", "ASIA", 300, "AFRICA", null
```
```
<path>..key
<path>.."key"
```
```
{
    "id" : 1,
    "notes" : [{"type" : 1, "comment" : "foo"}, {"type" : 2, "comment" : null}],
    "comment" : ["bar", "baz"]
}
```
```
<path>..comment --> ["bar", "baz"], "foo", null
```
```
<path>[ <subscripts> ]
```
```
<path>[<path1>, <path2> to <path3>, <path4>,...]
```
```
[0, 1, 2], ["a", "b", "c", "d"], [null, null]
```
```
<path>[last] --> 2, "d", null
```
```
<path>[2 to 3] --> 2, "c", "d"
```
```
<path>[1, 0, 0] --> 1, 0, 0, "b", "a", "a", null, null, null
```
```
<path>[*]
```
```
[0, 1, 2], ["a", "b", "c", "d"], [null, null]
<path>[*] --> 0, 1, 2, "a", "b", "c", "d", null, null
```
```
<path>?( <predicate> )
```
```
<predicate1> && <predicate2>
```
```
<predicate1> || <predicate2>
```
```
! <predicate>
```
```
exists( <path> )
```
```
<path> starts with "Some text"
<path> starts with $variable
```
```
( <predicate> ) is unknown
```
```
<path1> == <path2>
<path1> <> <path2>
<path1> != <path2>
<path1> < <path2>
<path1> > <path2>
<path1> <= <path2>
<path1> >= <path2>
```
```
{"customer" : 100, "region" : "AFRICA"},
{"region" : "ASIA"},
{"customer" : 300, "region" : "AFRICA", "comment" : null}
```
```
<path>?(@.region != "ASIA") --> {"customer" : 100, "region" : "AFRICA"},
                                {"customer" : 300, "region" : "AFRICA", "comment" : null}
<path>?(!exists(@.customer)) --> {"region" : "ASIA"}
```
```
<path>.double()
```
```
<path>.double() --> -1e0, 23e4, 5.6e0
```
```
<path>.ceiling() --> -1.0, -1, 2.0
<path>.floor() --> -2.0, -1, 1.0
<path>.abs() --> 1.5, 1, 1.3
```
```
<path>.keyvalue()
```
```
{"customer" : 100, "region" : "AFRICA"},
{"region" : "ASIA"},
{"customer" : 300, "region" : "AFRICA", "comment" : null}
```
```
<path>.keyvalue() --> {"name" : "customer", "value" : 100, "id" : 0},
                      {"name" : "region", "value" : "AFRICA", "id" : 0},
                      {"name" : "region", "value" : "ASIA", "id" : 1},
                      {"name" : "customer", "value" : 300, "id" : 2},
                      {"name" : "region", "value" : "AFRICA", "id" : 2},
                      {"name" : "comment", "value" : null, "id" : 2}
```
```
<path>.type()
```
```
<path>.size()
```
```
[0, 1, 2], ["a", "b", "c", "d"], [null, null]
<path>.size() --> 3, 4, 2
```
```
[1, "a", null], {"key1" : 1.0, "key2" : true}, -2e3
```
```
<path>[*] --> 1, "a", null, {"key1" : 1.0, "key2" : true}, -2e3
```
```
<path>.size() --> 3, 1, 1
```
```
<path>.floor() --> ERROR
```
```
JSON_EXISTS(
    json_input [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ],
    json_path
    [ PASSING json_argument [, ...] ]
    [ { TRUE | FALSE | UNKNOWN | ERROR } ON ERROR ]
    )
```
```
'strict ($.price + $.tax)?(@ > 99.9)'
'lax $[0 to 1].floor()?(@ > 10)'
```
```
PASSING orders.totalprice AS O_PRICE,
        orders.tax % 10 AS O_TAX
```
```
'lax $?(@.price > $O_PRICE || @.tax > $O_TAX)'
```
```
PASSING orders.json_desc FORMAT JSON AS o_desc,
        orders.binary_record FORMAT JSON ENCODING UTF16 AS o_rec
```
```
'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS KeyName --> ERROR; no passed value found
'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS "KeyName" --> correct
```
```
SELECT
      id,
      json_exists(
                  description,
                  'lax $.children[*]?(@ > 10)'
                 ) AS children_above_ten
FROM customers
```
```
SELECT
      id,
      json_exists(
                  description,
                  'strict $.children[2]?(@ > 10)'
                  UNKNOWN ON ERROR
                 ) AS child_3_above_ten
FROM customers
```
```
JSON_QUERY(
    json_input [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ],
    json_path
    [ PASSING json_argument [, ...] ]
    [ RETURNING type [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ] ]
    [ WITHOUT [ ARRAY ] WRAPPER |
      WITH [ { CONDITIONAL | UNCONDITIONAL } ] [ ARRAY ] WRAPPER ]
    [ { KEEP | OMIT } QUOTES [ ON SCALAR STRING ] ]
    [ { ERROR | NULL | EMPTY ARRAY | EMPTY OBJECT } ON EMPTY ]
    [ { ERROR | NULL | EMPTY ARRAY | EMPTY OBJECT } ON ERROR ]
    )
```
```
'strict $.keyvalue()?(@.name == $cust_id)'
'lax $[5 to last]'
```
```
PASSING orders.custkey AS CUST_ID
```
```
'strict $.keyvalue()?(@.value == $CUST_ID)'
```
```
PASSING orders.json_desc FORMAT JSON AS o_desc,
        orders.binary_record FORMAT JSON ENCODING UTF16 AS o_rec
```
```
'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS KeyName --> ERROR; no passed value found
'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS "KeyName" --> correct
```
```
SELECT
      id,
      json_query(
                 description,
                 'lax $.children'
                ) AS children
FROM customers
```
```
SELECT
      id,
      json_query(
                 description,
                 'lax $.children[*]'
                 WITHOUT ARRAY WRAPPER
                 NULL ON ERROR
                ) AS children
FROM customers
```
```
SELECT
      id,
      json_query(
                 description,
                 'lax $.children[last]'
                 WITH ARRAY WRAPPER
                ) AS last_child
FROM customers
```
```
SELECT
      id,
      json_query(
                 description,
                 'strict $.children[*]?(@ > 12)'
                 WITH ARRAY WRAPPER
                 EMPTY ARRAY ON EMPTY
                ) AS children
FROM customers
```
```
SELECT
      id,
      json_query(description, 'strict $.comment' KEEP QUOTES) AS quoted_comment,
      json_query(description, 'strict $.comment' OMIT QUOTES) AS unquoted_comment
FROM customers
```
```
JSON_VALUE(
    json_input [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ],
    json_path
    [ PASSING json_argument [, ...] ]
    [ RETURNING type ]
    [ { ERROR | NULL | DEFAULT expression } ON EMPTY ]
    [ { ERROR | NULL | DEFAULT expression } ON ERROR ]
    )
```
```
'strict $.price + $tax'
'lax $[last].abs().floor()'
```
```
PASSING orders.tax AS O_TAX
```
```
'strict $[last].price + $O_TAX'
```
```
PASSING orders.json_desc FORMAT JSON AS o_desc,
        orders.binary_record FORMAT JSON ENCODING UTF16 AS o_rec
```
```
'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS KeyName --> ERROR; no passed value found
'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS "KeyName" --> correct
```
```
DEFAULT -1 ON EMPTY
```
```
SELECT id, json_value(
                      description,
                      'lax $.comment'
                      RETURNING char(12)
                     ) AS comment
FROM customers
```
```
SELECT id, json_value(
                      description,
                      'lax $.children[0]'
                      RETURNING tinyint
                     ) AS child
FROM customers
```
```
SELECT id, json_value(
                      description,
                      'strict $.children[2]'
                      DEFAULT 'err' ON ERROR
                     ) AS child
FROM customers
```
```
SELECT id, json_value(
                      description,
                      'lax $.children[2]'
                      DEFAULT 'missing' ON EMPTY
                     ) AS child
FROM customers
```
```
JSON_TABLE(
    json_input,
    json_path [ AS path_name ]
    [ PASSING value AS parameter_name [, ...] ]
    COLUMNS (
        column_definition [, ...] )
    [ PLAN ( json_table_specific_plan )
      | PLAN DEFAULT ( json_table_default_plan ) ]
    [ { ERROR | EMPTY } ON ERROR ]
)
```
```
column_name FOR ORDINALITY
| column_name type
    [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ]
    [ PATH json_path ]
    [ { WITHOUT | WITH { CONDITIONAL | UNCONDITIONAL } } [ ARRAY ] WRAPPER ]
    [ { KEEP | OMIT } QUOTES [ ON SCALAR STRING ] ]
    [ { ERROR | NULL | EMPTY { [ARRAY] | OBJECT } | DEFAULT expression } ON EMPTY ]
    [ { ERROR | NULL | DEFAULT expression } ON ERROR ]
| NESTED [ PATH ] json_path [ AS path_name ] COLUMNS ( column_definition [, ...] )
```
```
'strict ($.price + $.tax)?(@ > 99.9)'
'lax $[0 to 1].floor()?(@ > 10)'
```
```
PASSING orders.totalprice AS o_price,
        orders.tax % 10 AS o_tax
```
```
'lax $?(@.price > $o_price || @.tax > $o_tax)'
```
```
PASSING orders.json_desc FORMAT JSON AS o_desc,
        orders.binary_record FORMAT JSON ENCODING UTF16 AS o_rec
```
```
'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS KeyName --> ERROR; no passed value found
'lax $.keyvalue()?(@.name == $KeyName).value' PASSING nation.name AS "KeyName" --> correct
```
```
row_num FOR ORDINALITY
```
```
SELECT
      *
FROM
      json_table(
                '[
                  {"id":1,"name":"Africa","wikiDataId":"Q15"},
                  {"id":2,"name":"Americas","wikiDataId":"Q828"},
                  {"id":3,"name":"Asia","wikiDataId":"Q48"},
                  {"id":4,"name":"Europe","wikiDataId":"Q51"}
                ]',
                'strict $' COLUMNS (
                  NESTED PATH 'strict $[*]' COLUMNS (
                    id integer PATH 'strict $.id',
                    name varchar PATH 'strict $.name',
                    wiki_data_id varchar PATH 'strict $."wikiDataId"'
                  )
                )
              );
```
```
SELECT
      *
FROM
      json_table(
                '[
                    {"continent": "Asia", "countries": [
                        {"name": "Japan", "population": 125.7},
                        {"name": "Thailand", "population": 71.6}
                    ]},
                    {"continent": "Europe", "countries": [
                        {"name": "France", "population": 67.4},
                        {"name": "Germany", "population": 83.2}
                    ]}
                ]',
                'lax $' COLUMNS (
                    NESTED PATH 'lax $[*]' COLUMNS (
                        continent varchar PATH 'lax $.continent',
                        NESTED PATH 'lax $.countries[*]' COLUMNS (
                            country varchar PATH 'lax $.name',
                            population double PATH 'lax $.population'
                        )
                    )
                ));
```
```
SELECT
      *
FROM
      JSON_TABLE(
                '[]',
                'lax $' AS "root_path"
                COLUMNS(
                    a varchar(1) PATH 'lax "A"',
                    NESTED PATH 'lax $[*]' AS "nested_path"
                            COLUMNS (b varchar(1) PATH 'lax "B"'))
                PLAN ("root_path" OUTER "nested_path")
                );
```
```
SELECT
      *
FROM
      JSON_TABLE(
                '[]',
                'lax $' AS "root_path"
                COLUMNS(
                    a varchar(1) PATH 'lax "A"',
                    NESTED PATH 'lax $[*]' AS "nested_path"
                            COLUMNS (b varchar(1) PATH 'lax "B"'))
                PLAN ("root_path" INNER "nested_path")
                );
```
```
JSON_ARRAY(
    [ array_element [, ...]
      [ { NULL ON NULL | ABSENT ON NULL } ] ],
    [ RETURNING type [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ] ]
    )
```
```
SELECT json_array(true, 12e-1, 'text')
--> '[true,1.2,"text"]'
```
```
SELECT json_array(
                  '[  "text"  ] ' FORMAT JSON,
                  X'5B0035005D00' FORMAT JSON ENCODING UTF16
                 )
--> '[["text"],[5]]'
```
```
SELECT json_array(
                  json_query('{"key" : [  "value"  ]}', 'lax $.key')
                 )
--> '[["value"]]'
```
```
SELECT json_array(
                  DATE '2001-01-31',
                  UUID '12151fd2-7586-11e9-8f9e-2a86e4085a59'
                 )
--> '["2001-01-31","12151fd2-7586-11e9-8f9e-2a86e4085a59"]'
```
```
SELECT json_array() --> '[]'
```
```
SELECT json_array(true, null, 1)
--> '[true,1]'

SELECT json_array(true, null, 1 ABSENT ON NULL)
--> '[true,1]'

SELECT json_array(true, null, 1 NULL ON NULL)
--> '[true,null,1]'
```
```
SELECT json_array(true, 1 RETURNING VARCHAR(100))
--> '[true,1]'
```
```
SELECT json_array(true, 1 RETURNING VARBINARY)
--> X'5b 74 72 75 65 2c 31 5d'

SELECT json_array(true, 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF8)
--> X'5b 74 72 75 65 2c 31 5d'

SELECT json_array(true, 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF16)
--> X'5b 00 74 00 72 00 75 00 65 00 2c 00 31 00 5d 00'

SELECT json_array(true, 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF32)
--> X'5b 00 00 00 74 00 00 00 72 00 00 00 75 00 00 00 65 00 00 00 2c 00 00 00 31 00 00 00 5d 00 00 00'
```
```
JSON_OBJECT(
    [ key_value [, ...]
      [ { NULL ON NULL | ABSENT ON NULL } ] ],
      [ { WITH UNIQUE [ KEYS ] | WITHOUT UNIQUE [ KEYS ] } ]
    [ RETURNING type [ FORMAT JSON [ ENCODING { UTF8 | UTF16 | UTF32 } ] ] ]
    )
```
```
SELECT json_object('key1' : 1, 'key2' : true)
--> '{"key1":1,"key2":true}'

SELECT json_object(KEY 'key1' VALUE 1, KEY 'key2' VALUE true)
--> '{"key1":1,"key2":true}'
```
```
SELECT json_object('key1' VALUE 1, 'key2' VALUE true)
--> '{"key1":1,"key2":true}'
```
```
SELECT json_object('x' : true, 'y' : 12e-1, 'z' : 'text')
--> '{"x":true,"y":1.2,"z":"text"}'
```
```
SELECT json_object(
                   'x' : '[  "text"  ] ' FORMAT JSON,
                   'y' : X'5B0035005D00' FORMAT JSON ENCODING UTF16
                  )
--> '{"x":["text"],"y":[5]}'
```
```
SELECT json_object(
                   'x' : json_query('{"key" : [  "value"  ]}', 'lax $.key')
                  )
--> '{"x":["value"]}'
```
```
SELECT json_object(
                   'x' : DATE '2001-01-31',
                   'y' : UUID '12151fd2-7586-11e9-8f9e-2a86e4085a59'
                  )
--> '{"x":"2001-01-31","y":"12151fd2-7586-11e9-8f9e-2a86e4085a59"}'
```
```
SELECT json_object() --> '{}'
```
```
SELECT json_object('x' : null, 'y' : 1)
--> '{"x":null,"y":1}'

SELECT json_object('x' : null, 'y' : 1 NULL ON NULL)
--> '{"x":null,"y":1}'

SELECT json_object('x' : null, 'y' : 1 ABSENT ON NULL)
--> '{"y":1}'
```
```
SELECT json_object('x' : null, 'x' : 1 WITH UNIQUE KEYS)
--> failure: "duplicate key passed to JSON_OBJECT function"
```
```
SELECT json_object('x' : 1 RETURNING VARCHAR(100))
--> '{"x":1}'
```
```
SELECT json_object('x' : 1 RETURNING VARBINARY)
--> X'7b 22 78 22 3a 31 7d'

SELECT json_object('x' : 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF8)
--> X'7b 22 78 22 3a 31 7d'

SELECT json_object('x' : 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF16)
--> X'7b 00 22 00 78 00 22 00 3a 00 31 00 7d 00'

SELECT json_object('x' : 1 RETURNING VARBINARY FORMAT JSON ENCODING UTF32)
--> X'7b 00 00 00 22 00 00 00 78 00 00 00 22 00 00 00 3a 00 00 00 31 00 00 00 7d 00 00 00'
```
```
SELECT CAST(NULL AS JSON);
-- NULL

SELECT CAST(1 AS JSON);
-- JSON '1'

SELECT CAST(9223372036854775807 AS JSON);
-- JSON '9223372036854775807'

SELECT CAST('abc' AS JSON);
-- JSON '"abc"'

SELECT CAST(true AS JSON);
-- JSON 'true'

SELECT CAST(1.234 AS JSON);
-- JSON '1.234'

SELECT CAST(ARRAY[1, 23, 456] AS JSON);
-- JSON '[1,23,456]'

SELECT CAST(ARRAY[1, NULL, 456] AS JSON);
-- JSON '[1,null,456]'

SELECT CAST(ARRAY[ARRAY[1, 23], ARRAY[456]] AS JSON);
-- JSON '[[1,23],[456]]'

SELECT CAST(MAP(ARRAY['k1', 'k2', 'k3'], ARRAY[1, 23, 456]) AS JSON);
-- JSON '{"k1":1,"k2":23,"k3":456}'

SELECT CAST(CAST(ROW(123, 'abc', true) AS
            ROW(v1 BIGINT, v2 VARCHAR, v3 BOOLEAN)) AS JSON);
-- JSON '{"v1":123,"v2":"abc","v3":true}'
```
```
SELECT CAST(JSON 'null' AS VARCHAR);
-- NULL

SELECT CAST(JSON '1' AS INTEGER);
-- 1

SELECT CAST(JSON '9223372036854775807' AS BIGINT);
-- 9223372036854775807

SELECT CAST(JSON '"abc"' AS VARCHAR);
-- abc

SELECT CAST(JSON 'true' AS BOOLEAN);
-- true

SELECT CAST(JSON '1.234' AS DOUBLE);
-- 1.234

SELECT CAST(JSON '[1,23,456]' AS ARRAY(INTEGER));
-- [1, 23, 456]

SELECT CAST(JSON '[1,null,456]' AS ARRAY(INTEGER));
-- [1, NULL, 456]

SELECT CAST(JSON '[[1,23],[456]]' AS ARRAY(ARRAY(INTEGER)));
-- [[1, 23], [456]]

SELECT CAST(JSON '{"k1":1,"k2":23,"k3":456}' AS MAP(VARCHAR, INTEGER));
-- {k1=1, k2=23, k3=456}

SELECT CAST(JSON '{"v1":123,"v2":"abc","v3":true}' AS
            ROW(v1 BIGINT, v2 VARCHAR, v3 BOOLEAN));
-- {v1=123, v2=abc, v3=true}

SELECT CAST(JSON '[123,"abc",true]' AS
            ROW(v1 BIGINT, v2 VARCHAR, v3 BOOLEAN));
-- {v1=123, v2=abc, v3=true}
```
```
SELECT CAST(JSON '[[1, 23], 456]' AS ARRAY(JSON));
-- [JSON '[1,23]', JSON '456']

SELECT CAST(JSON '{"k1": [1, 23], "k2": 456}' AS MAP(VARCHAR, JSON));
-- {k1 = JSON '[1,23]', k2 = JSON '456'}

SELECT CAST(JSON '[null]' AS ARRAY(JSON));
-- [JSON 'null']
```
```
SELECT is_json_scalar('1');         -- true
SELECT is_json_scalar('[1, 2, 3]'); -- false
```
```
SELECT json_array_contains('[1, 2, 3]', 2); -- true
```
```
SELECT json_array_get('["a", [3, 9], "c"]', 0); -- JSON 'a' (invalid JSON)
SELECT json_array_get('["a", [3, 9], "c"]', 1); -- JSON '[3,9]'
```
```
SELECT json_array_get('["c", [3, 9], "a"]', -1); -- JSON 'a' (invalid JSON)
SELECT json_array_get('["c", [3, 9], "a"]', -2); -- JSON '[3,9]'
```
```
SELECT json_array_get('[]', 0);                -- NULL
SELECT json_array_get('["a", "b", "c"]', 10);  -- NULL
SELECT json_array_get('["c", "b", "a"]', -10); -- NULL
```
```
SELECT json_array_length('[1, 2, 3]'); -- 3
```
```
SELECT json_extract(json, '$.store.book');
SELECT json_extract(json, '$.store[book]');
SELECT json_extract(json, '$.store["book name"]');
```
```
SELECT json_extract_scalar('[1, 2, 3]', '$[2]');
SELECT json_extract_scalar(json, '$.store.book[0].author');
```
```
SELECT json_format(JSON '[1, 2, 3]'); -- '[1,2,3]'
SELECT json_format(JSON '"a"');       -- '"a"'
```
```
SELECT json_format(JSON '{"a": 1, "b": 2}'); -- '{"a":1,"b":2}'
SELECT json_format(JSON '[1, 2, 3]');        -- '[1,2,3]'
SELECT json_format(JSON '"abc"');            -- '"abc"'
SELECT json_format(JSON '42');               -- '42'
SELECT json_format(JSON 'true');             -- 'true'
SELECT json_format(JSON 'null');             -- 'null'
```
```
SELECT CAST(JSON '{"a": 1, "b": 2}' AS VARCHAR); -- ERROR!
SELECT CAST(JSON '[1, 2, 3]' AS VARCHAR);        -- ERROR!
SELECT CAST(JSON '"abc"' AS VARCHAR);            -- 'abc' (the double quote is gone)
SELECT CAST(JSON '42' AS VARCHAR);               -- '42'
SELECT CAST(JSON 'true' AS VARCHAR);             -- 'true'
SELECT CAST(JSON 'null' AS VARCHAR);             -- NULL
```
```
SELECT json_parse('[1, 2, 3]');   -- JSON '[1,2,3]'
SELECT json_parse('"abc"');       -- JSON '"abc"'
```
```
SELECT json_parse('not_json');         -- ERROR!
SELECT json_parse('["a": 1, "b": 2]'); -- JSON '["a": 1, "b": 2]'
SELECT json_parse('[1, 2, 3]');        -- JSON '[1,2,3]'
SELECT json_parse('"abc"');            -- JSON '"abc"'
SELECT json_parse('42');               -- JSON '42'
SELECT json_parse('true');             -- JSON 'true'
SELECT json_parse('null');             -- JSON 'null'
```
```
SELECT CAST('not_json' AS JSON);         -- JSON '"not_json"'
SELECT CAST('["a": 1, "b": 2]' AS JSON); -- JSON '"[\"a\": 1, \"b\": 2]"'
SELECT CAST('[1, 2, 3]' AS JSON);        -- JSON '"[1, 2, 3]"'
SELECT CAST('"abc"' AS JSON);            -- JSON '"\"abc\""'
SELECT CAST('42' AS JSON);               -- JSON '"42"'
SELECT CAST('true' AS JSON);             -- JSON '"true"'
SELECT CAST('null' AS JSON);             -- JSON '"null"'
```
```
SELECT json_size('{"x": {"a": 1, "b": 2}}', '$.x');   -- 2
SELECT json_size('{"x": [1, 2, 3]}', '$.x');          -- 3
SELECT json_size('{"x": {"a": 1, "b": 2}}', '$.x.a'); -- 0
```


---

### Lambda Functions Documentation
Source: https://trino.io/docs/current/functions/lambda.html

Lambda expressions# Lambda expressions are anonymous functions which are passed as arguments to higher-order SQL functions. Lambda expressions are written with ->: x -> x + 1 (x, y) -> x + y x -> regexp_like(x, 'a+') x -> x[1] / x[2] x -> IF(x > 0, x, -x) x -> COALESCE(x, 0) x -> CAST(x AS JSON) x -> x + TRY(1 / 0) Limitations# Most SQL expressions can be used in a lambda body, with a few exceptions: Subqueries are not supported: x -> 2 + (SELECT 3) Aggregations are not supported: x -> max(y) Examples# Obtain the squared elements of an array column with transform(): SELECT numbers, transform(numbers, n -> n * n) as squared_numbers FROM ( VALUES (ARRAY[1, 2]), (ARRAY[3, 4]), (ARRAY[5, 6, 7]) ) AS t(numbers); numbers | squared_numbers -----------+----------------- [1, 2] | [1, 4] [3, 4] | [9, 16] [5, 6, 7] | [25, 36, 49] (3 rows) The function transform() can be also employed to safely cast the elements of an array to strings: SELECT transform(prices, n -> TRY_CAST(n AS VARCHAR) || '$') as price_tags FROM ( VALUES (ARRAY[100, 200]), (ARRAY[30, 4]) ) AS t(prices); price_tags -------------- [100$, 200$] [30$, 4$] (2 rows) Besides the array column being manipulated, other columns can be captured as well within the lambda expression. The following statement provides a showcase of this feature for calculating the value of the linear function f(x) = ax + b with transform(): SELECT xvalues, a, b, transform(xvalues, x -> a * x + b) as linear_function_values FROM ( VALUES (ARRAY[1, 2], 10, 5), (ARRAY[3, 4], 4, 2) ) AS t(xvalues, a, b); xvalues | a | b | linear_function_values ---------+----+---+------------------------ [1, 2] | 10 | 5 | [15, 25] [3, 4] | 4 | 2 | [14, 18] (2 rows) Find the array elements containing at least one value greater than 100 with any_match(): SELECT numbers FROM ( VALUES (ARRAY[1,NULL,3]), (ARRAY[10,20,30]), (ARRAY[100,200,300]) ) AS t(numbers) WHERE any_match(numbers, n -> COALESCE(n, 0) > 100); -- [100, 200, 300] Capitalize the first word in a string via regexp_replace(): SELECT regexp_replace('once upon a time ...', '^(\w)(\w*)(\s+.*)$',x -> upper(x[1]) || x[2] || x[3]); -- Once upon a time ... Lambda expressions can be also applied in aggregation functions. Following statement is a sample the overly complex calculation of the sum of all elements of a column by making use of reduce_agg(): SELECT reduce_agg(value, 0, (a, b) -> a + b, (a, b) -> a + b) sum_values FROM ( VALUES (1), (2), (3), (4), (5) ) AS t(value); -- 15

#### Code Examples

```
x -> x + 1
(x, y) -> x + y
x -> regexp_like(x, 'a+')
x -> x[1] / x[2]
x -> IF(x > 0, x, -x)
x -> COALESCE(x, 0)
x -> CAST(x AS JSON)
x -> x + TRY(1 / 0)
```
```
SELECT numbers,
       transform(numbers, n -> n * n) as squared_numbers
FROM (
    VALUES
        (ARRAY[1, 2]),
        (ARRAY[3, 4]),
        (ARRAY[5, 6, 7])
) AS t(numbers);
```
```
numbers  | squared_numbers
-----------+-----------------
 [1, 2]    | [1, 4]
 [3, 4]    | [9, 16]
 [5, 6, 7] | [25, 36, 49]
(3 rows)
```
```
SELECT transform(prices, n -> TRY_CAST(n AS VARCHAR) || '$') as price_tags
FROM (
    VALUES
        (ARRAY[100, 200]),
        (ARRAY[30, 4])
) AS t(prices);
```
```
price_tags
--------------
 [100$, 200$]
 [30$, 4$]
(2 rows)
```
```
SELECT xvalues,
       a,
       b,
       transform(xvalues, x -> a * x + b) as linear_function_values
FROM (
    VALUES
        (ARRAY[1, 2], 10, 5),
        (ARRAY[3, 4], 4, 2)
) AS t(xvalues, a, b);
```
```
xvalues | a  | b | linear_function_values
---------+----+---+------------------------
 [1, 2]  | 10 | 5 | [15, 25]
 [3, 4]  |  4 | 2 | [14, 18]
(2 rows)
```
```
SELECT numbers
FROM (
    VALUES
        (ARRAY[1,NULL,3]),
        (ARRAY[10,20,30]),
        (ARRAY[100,200,300])
) AS t(numbers)
WHERE any_match(numbers, n ->  COALESCE(n, 0) > 100);
-- [100, 200, 300]
```
```
SELECT regexp_replace('once upon a time ...', '^(\w)(\w*)(\s+.*)$',x -> upper(x[1]) || x[2] || x[3]);
-- Once upon a time ...
```
```
SELECT reduce_agg(value, 0, (a, b) -> a + b, (a, b) -> a + b) sum_values
FROM (
    VALUES (1), (2), (3), (4), (5)
) AS t(value);
-- 15
```


---

### Logical Functions Documentation
Source: https://trino.io/docs/current/functions/logical.html

Logical operators# Logical operators# Operator Description Example AND True if both values are true a AND b OR True if either value is true a OR b NOT True if the value is false NOT a Effect of NULL on logical operators# The result of an AND comparison may be NULL if one or both sides of the expression are NULL. If at least one side of an AND operator is FALSE the expression evaluates to FALSE: SELECT CAST(null AS boolean) AND true; -- null SELECT CAST(null AS boolean) AND false; -- false SELECT CAST(null AS boolean) AND CAST(null AS boolean); -- null The result of an OR comparison may be NULL if one or both sides of the expression are NULL. If at least one side of an OR operator is TRUE the expression evaluates to TRUE: SELECT CAST(null AS boolean) OR CAST(null AS boolean); -- null SELECT CAST(null AS boolean) OR false; -- null SELECT CAST(null AS boolean) OR true; -- true The following truth table demonstrates the handling of NULL in AND and OR: a b a AND b a OR b TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE NULL NULL TRUE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE NULL FALSE NULL NULL TRUE NULL TRUE NULL FALSE FALSE NULL NULL NULL NULL NULL The logical complement of NULL is NULL as shown in the following example: SELECT NOT CAST(null AS boolean); -- null The following truth table demonstrates the handling of NULL in NOT: a NOT a TRUE FALSE FALSE TRUE NULL NULL

#### Code Examples

```
SELECT CAST(null AS boolean) AND true; -- null

SELECT CAST(null AS boolean) AND false; -- false

SELECT CAST(null AS boolean) AND CAST(null AS boolean); -- null
```
```
SELECT CAST(null AS boolean) OR CAST(null AS boolean); -- null

SELECT CAST(null AS boolean) OR false; -- null

SELECT CAST(null AS boolean) OR true; -- true
```
```
SELECT NOT CAST(null AS boolean); -- null
```


---

### Machine Learning Functions Documentation
Source: https://trino.io/docs/current/functions/ml.html

Machine learning functions# The machine learning plugin provides machine learning functionality as an aggregation function. It enables you to train Support Vector Machine (SVM) based classifiers and regressors for the supervised learning problems. Note The machine learning functions are not optimized for distributed processing. The capability to train large data sets is limited by this execution of the final training on a single instance. Feature vector# To solve a problem with the machine learning technique, especially as a supervised learning problem, it is necessary to represent the data set with the sequence of pairs of labels and feature vector. A label is a target value you want to predict from the unseen feature and a feature is a A N-dimensional vector whose elements are numerical values. In Trino, a feature vector is represented as a map-type value, whose key is an index of each feature, so that it can express a sparse vector. Since classifiers and regressors can recognize the map-type feature vector, there is a function to construct the feature from the existing numerical values, features(): SELECT features(1.0, 2.0, 3.0) AS features; features ----------------------- {0=1.0, 1=2.0, 2=3.0} The output from features() can be directly passed to ML functions. Classification# Classification is a type of supervised learning problem to predict the distinct label from the given feature vector. The interface looks similar to the construction of the SVM model from the sequence of pairs of labels and features implemented in Teradata Aster or BigQuery ML. The function to train a classification model looks like as follows: SELECT learn_classifier( species, features(sepal_length, sepal_width, petal_length, petal_width) ) AS model FROM iris It returns the trained model in a serialized format. model ------------------------------------------------- 3c 43 6c 61 73 73 69 66 69 65 72 28 76 61 72 63 68 61 72 29 3e classify() returns the predicted label by using the trained model. The trained model can not be saved natively, and needs to be passed in the format of a nested query: SELECT classify(features(5.9, 3, 5.1, 1.8), model) AS predicted_label FROM ( SELECT learn_classifier(species, features(sepal_length, sepal_width, petal_length, petal_width)) AS model FROM iris ) t predicted_label ----------------- Iris-virginica As a result you need to run the training process at the same time when predicting values. Internally, the model is trained by libsvm. You can use learn_libsvm_classifier() to control the internal parameters of the model. Regression# Regression is another type of supervised learning problem, predicting continuous value, unlike the classification problem. The target must be numerical values that can be described as double. The following code shows the creation of the model predicting sepal_length from the other 3 features: SELECT learn_regressor(sepal_length, features(sepal_width, petal_length, petal_width)) AS model FROM iris The way to use the model is similar to the classification case: SELECT regress(features(3, 5.1, 1.8), model) AS predicted_target FROM ( SELECT learn_regressor(sepal_length, features(sepal_width, petal_length, petal_width)) AS model FROM iris ) t; predicted_target ------------------- 6.407376822560477 Internally, the model is trained by libsvm. learn_libsvm_regressor() provides you a way to control the training process. Machine learning functions# features(double, ...) -> map(bigint, double)# Returns the map representing the feature vector. learn_classifier(label, features) → Classifier# Returns an SVM-based classifier model, trained with the given label and feature data sets. learn_libsvm_classifier(label, features, params) → Classifier# Returns an SVM-based classifier model, trained with the given label and feature data sets. You can control the training process by libsvm parameters. classify(features, model) → label# Returns a label predicted by the given classifier SVM model. learn_regressor(target, features) → Regressor# Returns an SVM-based regressor model, trained with the given target and feature data sets. learn_libsvm_regressor(target, features, params) → Regressor# Returns an SVM-based regressor model, trained with the given target and feature data sets. You can control the training process by libsvm parameters. regress(features, model) → target# Returns a predicted target value by the given regressor SVM model.

#### Code Examples

```
SELECT features(1.0, 2.0, 3.0) AS features;
```
```
features
-----------------------
 {0=1.0, 1=2.0, 2=3.0}
```
```
SELECT
  learn_classifier(
    species,
    features(sepal_length, sepal_width, petal_length, petal_width)
  ) AS model
FROM
  iris
```
```
model
-------------------------------------------------
 3c 43 6c 61 73 73 69 66 69 65 72 28 76 61 72 63
 68 61 72 29 3e
```
```
SELECT
  classify(features(5.9, 3, 5.1, 1.8), model) AS predicted_label
FROM (
  SELECT
    learn_classifier(species, features(sepal_length, sepal_width, petal_length, petal_width)) AS model
  FROM
    iris
) t
```
```
predicted_label
-----------------
 Iris-virginica
```
```
SELECT
  learn_regressor(sepal_length, features(sepal_width, petal_length, petal_width)) AS model
FROM
  iris
```
```
SELECT
  regress(features(3, 5.1, 1.8), model) AS predicted_target
FROM (
  SELECT
    learn_regressor(sepal_length, features(sepal_width, petal_length, petal_width)) AS model
  FROM iris
) t;
```
```
predicted_target
-------------------
 6.407376822560477
```


---

### Map Functions Documentation
Source: https://trino.io/docs/current/functions/map.html

Map functions and operators# Map functions and operators use the MAP type. Create a map with the data type constructor using an array of keys and another array of values in the same order. Keys must be character-based and can not be null. Create an array with integer values SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY[2373, 3463, 45837]); -- {key1=2373, key2=3463, key3=45837} Create an array of character values: SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY['v1', 'v2', 'v3']); -- {key1=v1, key2=v2, key3=v3} Values must use the same type or it must be possible to coerce values to a common type. The following example uses integer and decimal values and the resulting array contains decimals: SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY[23, 34.63, 45.837]); -- {key1=23.000, key2=34.630, key3=45.837} Null values are allowed: SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY['v1', NULL, 'v3']); -- {key1=v1, key2=NULL, key3=v3} Subscript operator: []# The [] operator is used to retrieve the value corresponding to a given key from a map. This operator throws an error if the key is not contained in the map. See also element_at function that returns NULL in such case. SELECT name_to_age_map['Bob'] AS bob_age; The following example constructs a map and then accesses the element with the the key key2: SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY['v1', 'v2', 'v3'])['key2']; -- v2 Map functions# cardinality(x) → bigint Returns the cardinality (size) of the map x. element_at(map(K, V), key) → V Returns value for given key, or NULL if the key is not contained in the map. map() → map<unknown, unknown># Returns an empty map. SELECT map(); -- {} map(array(K), array(V)) -> map(K, V) Returns a map created using the given key/value arrays. SELECT map(ARRAY[1,3], ARRAY[2,4]); -- {1 -> 2, 3 -> 4} See also map_agg() and multimap_agg() for creating a map as an aggregation. map_from_entries(array(row(K, V))) -> map(K, V)# Returns a map created from the given array of entries. SELECT map_from_entries(ARRAY[(1, 'x'), (2, 'y')]); -- {1 -> 'x', 2 -> 'y'} multimap_from_entries(array(row(K, V))) -> map(K, array(V))# Returns a multimap created from the given array of entries. Each key can be associated with multiple values. SELECT multimap_from_entries(ARRAY[(1, 'x'), (2, 'y'), (1, 'z')]); -- {1 -> ['x', 'z'], 2 -> ['y']} map_entries(map(K, V)) -> array(row(K, V))# Returns an array of all entries in the given map. SELECT map_entries(MAP(ARRAY[1, 2], ARRAY['x', 'y'])); -- [ROW(1, 'x'), ROW(2, 'y')] map_concat(map1(K, V), map2(K, V), ..., mapN(K, V)) -> map(K, V)# Returns the union of all the given maps. If a key is found in multiple given maps, that key’s value in the resulting map comes from the last one of those maps. map_filter(map(K, V), function(K, V, boolean)) -> map(K, V)# Constructs a map from those entries of map for which function returns true: SELECT map_filter(MAP(ARRAY[], ARRAY[]), (k, v) -> true); -- {} SELECT map_filter(MAP(ARRAY[10, 20, 30], ARRAY['a', NULL, 'c']), (k, v) -> v IS NOT NULL); -- {10 -> a, 30 -> c} SELECT map_filter(MAP(ARRAY['k1', 'k2', 'k3'], ARRAY[20, 3, 15]), (k, v) -> v > 10); -- {k1 -> 20, k3 -> 15} map_keys(x(K, V)) -> array(K)# Returns all the keys in the map x. map_values(x(K, V)) -> array(V)# Returns all the values in the map x. map_zip_with(map(K, V1), map(K, V2), function(K, V1, V2, V3)) -> map(K, V3)# Merges the two given maps into a single map by applying function to the pair of values with the same key. For keys only presented in one map, NULL will be passed as the value for the missing key. SELECT map_zip_with(MAP(ARRAY[1, 2, 3], ARRAY['a', 'b', 'c']), MAP(ARRAY[1, 2, 3], ARRAY['d', 'e', 'f']), (k, v1, v2) -> concat(v1, v2)); -- {1 -> ad, 2 -> be, 3 -> cf} SELECT map_zip_with(MAP(ARRAY['k1', 'k2'], ARRAY[1, 2]), MAP(ARRAY['k2', 'k3'], ARRAY[4, 9]), (k, v1, v2) -> (v1, v2)); -- {k1 -> ROW(1, null), k2 -> ROW(2, 4), k3 -> ROW(null, 9)} SELECT map_zip_with(MAP(ARRAY['a', 'b', 'c'], ARRAY[1, 8, 27]), MAP(ARRAY['a', 'b', 'c'], ARRAY[1, 2, 3]), (k, v1, v2) -> k || CAST(v1 / v2 AS VARCHAR)); -- {a -> a1, b -> b4, c -> c9} transform_keys(map(K1, V), function(K1, V, K2)) -> map(K2, V)# Returns a map that applies function to each entry of map and transforms the keys: SELECT transform_keys(MAP(ARRAY[], ARRAY[]), (k, v) -> k + 1); -- {} SELECT transform_keys(MAP(ARRAY [1, 2, 3], ARRAY ['a', 'b', 'c']), (k, v) -> k + 1); -- {2 -> a, 3 -> b, 4 -> c} SELECT transform_keys(MAP(ARRAY ['a', 'b', 'c'], ARRAY [1, 2, 3]), (k, v) -> v * v); -- {1 -> 1, 4 -> 2, 9 -> 3} SELECT transform_keys(MAP(ARRAY ['a', 'b'], ARRAY [1, 2]), (k, v) -> k || CAST(v as VARCHAR)); -- {a1 -> 1, b2 -> 2} SELECT transform_keys(MAP(ARRAY [1, 2], ARRAY [1.0, 1.4]), (k, v) -> MAP(ARRAY[1, 2], ARRAY['one', 'two'])[k]); -- {one -> 1.0, two -> 1.4} transform_values(map(K, V1), function(K, V1, V2)) -> map(K, V2)# Returns a map that applies function to each entry of map and transforms the values: SELECT transform_values(MAP(ARRAY[], ARRAY[]), (k, v) -> v + 1); -- {} SELECT transform_values(MAP(ARRAY [1, 2, 3], ARRAY [10, 20, 30]), (k, v) -> v + k); -- {1 -> 11, 2 -> 22, 3 -> 33} SELECT transform_values(MAP(ARRAY [1, 2, 3], ARRAY ['a', 'b', 'c']), (k, v) -> k * k); -- {1 -> 1, 2 -> 4, 3 -> 9} SELECT transform_values(MAP(ARRAY ['a', 'b'], ARRAY [1, 2]), (k, v) -> k || CAST(v as VARCHAR)); -- {a -> a1, b -> b2} SELECT transform_values(MAP(ARRAY [1, 2], ARRAY [1.0, 1.4]), (k, v) -> MAP(ARRAY[1, 2], ARRAY['one', 'two'])[k] || '_' || CAST(v AS VARCHAR)); -- {1 -> one_1.0, 2 -> two_1.4}

#### Code Examples

```
SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY[2373, 3463, 45837]);
-- {key1=2373, key2=3463, key3=45837}
```
```
SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY['v1', 'v2', 'v3']);
-- {key1=v1, key2=v2, key3=v3}
```
```
SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY[23, 34.63, 45.837]);
-- {key1=23.000, key2=34.630, key3=45.837}
```
```
SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY['v1', NULL, 'v3']);
-- {key1=v1, key2=NULL, key3=v3}
```
```
SELECT name_to_age_map['Bob'] AS bob_age;
```
```
SELECT MAP(ARRAY['key1', 'key2', 'key3' ], ARRAY['v1', 'v2', 'v3'])['key2'];
-- v2
```
```
SELECT map();
-- {}
```
```
SELECT map(ARRAY[1,3], ARRAY[2,4]);
-- {1 -> 2, 3 -> 4}
```
```
SELECT map_from_entries(ARRAY[(1, 'x'), (2, 'y')]);
-- {1 -> 'x', 2 -> 'y'}
```
```
SELECT multimap_from_entries(ARRAY[(1, 'x'), (2, 'y'), (1, 'z')]);
-- {1 -> ['x', 'z'], 2 -> ['y']}
```
```
SELECT map_entries(MAP(ARRAY[1, 2], ARRAY['x', 'y']));
-- [ROW(1, 'x'), ROW(2, 'y')]
```
```
SELECT map_filter(MAP(ARRAY[], ARRAY[]), (k, v) -> true);
-- {}

SELECT map_filter(MAP(ARRAY[10, 20, 30], ARRAY['a', NULL, 'c']),
                  (k, v) -> v IS NOT NULL);
-- {10 -> a, 30 -> c}

SELECT map_filter(MAP(ARRAY['k1', 'k2', 'k3'], ARRAY[20, 3, 15]),
                  (k, v) -> v > 10);
-- {k1 -> 20, k3 -> 15}
```
```
SELECT map_zip_with(MAP(ARRAY[1, 2, 3], ARRAY['a', 'b', 'c']),
                    MAP(ARRAY[1, 2, 3], ARRAY['d', 'e', 'f']),
                    (k, v1, v2) -> concat(v1, v2));
-- {1 -> ad, 2 -> be, 3 -> cf}

SELECT map_zip_with(MAP(ARRAY['k1', 'k2'], ARRAY[1, 2]),
                    MAP(ARRAY['k2', 'k3'], ARRAY[4, 9]),
                    (k, v1, v2) -> (v1, v2));
-- {k1 -> ROW(1, null), k2 -> ROW(2, 4), k3 -> ROW(null, 9)}

SELECT map_zip_with(MAP(ARRAY['a', 'b', 'c'], ARRAY[1, 8, 27]),
                    MAP(ARRAY['a', 'b', 'c'], ARRAY[1, 2, 3]),
                    (k, v1, v2) -> k || CAST(v1 / v2 AS VARCHAR));
-- {a -> a1, b -> b4, c -> c9}
```
```
SELECT transform_keys(MAP(ARRAY[], ARRAY[]), (k, v) -> k + 1);
-- {}

SELECT transform_keys(MAP(ARRAY [1, 2, 3], ARRAY ['a', 'b', 'c']),
                      (k, v) -> k + 1);
-- {2 -> a, 3 -> b, 4 -> c}

SELECT transform_keys(MAP(ARRAY ['a', 'b', 'c'], ARRAY [1, 2, 3]),
                      (k, v) -> v * v);
-- {1 -> 1, 4 -> 2, 9 -> 3}

SELECT transform_keys(MAP(ARRAY ['a', 'b'], ARRAY [1, 2]),
                      (k, v) -> k || CAST(v as VARCHAR));
-- {a1 -> 1, b2 -> 2}

SELECT transform_keys(MAP(ARRAY [1, 2], ARRAY [1.0, 1.4]),
                      (k, v) -> MAP(ARRAY[1, 2], ARRAY['one', 'two'])[k]);
-- {one -> 1.0, two -> 1.4}
```
```
SELECT transform_values(MAP(ARRAY[], ARRAY[]), (k, v) -> v + 1);
-- {}

SELECT transform_values(MAP(ARRAY [1, 2, 3], ARRAY [10, 20, 30]),
                        (k, v) -> v + k);
-- {1 -> 11, 2 -> 22, 3 -> 33}

SELECT transform_values(MAP(ARRAY [1, 2, 3], ARRAY ['a', 'b', 'c']),
                        (k, v) -> k * k);
-- {1 -> 1, 2 -> 4, 3 -> 9}

SELECT transform_values(MAP(ARRAY ['a', 'b'], ARRAY [1, 2]),
                        (k, v) -> k || CAST(v as VARCHAR));
-- {a -> a1, b -> b2}

SELECT transform_values(MAP(ARRAY [1, 2], ARRAY [1.0, 1.4]),
                        (k, v) -> MAP(ARRAY[1, 2], ARRAY['one', 'two'])[k]
                          || '_' || CAST(v AS VARCHAR));
-- {1 -> one_1.0, 2 -> two_1.4}
```


---

### Math Functions Documentation
Source: https://trino.io/docs/current/functions/math.html

Mathematical functions and operators# Mathematical operators# Operator Description + Addition - Subtraction * Multiplication / Division (integer division performs truncation) % Modulus (remainder) Mathematical functions# abs(x) → [same as input]# Returns the absolute value of x. cbrt(x) → double# Returns the cube root of x. ceil(x) → [same as input]# This is an alias for ceiling(). ceiling(x) → [same as input]# Returns x rounded up to the nearest integer. degrees(x) → double# Converts angle x in radians to degrees. e() → double# Returns the constant Euler’s number. exp(x) → double# Returns Euler’s number raised to the power of x. floor(x) → [same as input]# Returns x rounded down to the nearest integer. ln(x) → double# Returns the natural logarithm of x. log(b, x) → double# Returns the base b logarithm of x. log2(x) → double# Returns the base 2 logarithm of x. log10(x) → double# Returns the base 10 logarithm of x. mod(n, m) → [same as input]# Returns the modulus (remainder) of n divided by m. pi() → double# Returns the constant Pi. pow(x, p) → double# This is an alias for power(). power(x, p) → double# Returns x raised to the power of p. radians(x) → double# Converts angle x in degrees to radians. round(x) → [same as input]# Returns x rounded to the nearest integer. round(x, d) → [same as input] Returns x rounded to d decimal places. sign(x) → [same as input]# Returns the signum function of x, that is: 0 if the argument is 0, 1 if the argument is greater than 0, -1 if the argument is less than 0. For floating point arguments, the function additionally returns: -0 if the argument is -0, NaN if the argument is NaN, 1 if the argument is +Infinity, -1 if the argument is -Infinity. sqrt(x) → double# Returns the square root of x. truncate(x) → double# Returns x rounded to integer by dropping digits after decimal point. width_bucket(x, bound1, bound2, n) → bigint# Returns the bin number of x in an equi-width histogram with the specified bound1 and bound2 bounds and n number of buckets. width_bucket(x, bins) → bigint Returns the bin number of x according to the bins specified by the array bins. The bins parameter must be an array of doubles and is assumed to be in sorted ascending order. Random functions# rand() → double# This is an alias for random(). random() → double# Returns a pseudo-random value in the range 0.0 <= x < 1.0. random(n) → [same as input] Returns a pseudo-random number between 0 and n (exclusive). random(m, n) → [same as input] Returns a pseudo-random number between m and n (exclusive). Trigonometric functions# All trigonometric function arguments are expressed in radians. See unit conversion functions degrees() and radians(). acos(x) → double# Returns the arc cosine of x. asin(x) → double# Returns the arc sine of x. atan(x) → double# Returns the arc tangent of x. atan2(y, x) → double# Returns the arc tangent of y / x. cos(x) → double# Returns the cosine of x. cosh(x) → double# Returns the hyperbolic cosine of x. sin(x) → double# Returns the sine of x. sinh(x) → double# Returns the hyperbolic sine of x. tan(x) → double# Returns the tangent of x. tanh(x) → double# Returns the hyperbolic tangent of x. Geometric functions# cosine_distance(array(double), array(double)) → double# Calculates the cosine distance between two dense vectors: SELECT cosine_distance(ARRAY[1.0, 2.0], ARRAY[3.0, 4.0]); -- 0.01613008990009257 cosine_similarity(array(double), array(double)) → double# Calculates the cosine similarity of two dense vectors: SELECT cosine_similarity(ARRAY[1.0, 2.0], ARRAY[3.0, 4.0]); -- 0.9838699100999074 cosine_similarity(x, y) → double Calculates the cosine similarity of two sparse vectors: SELECT cosine_similarity(MAP(ARRAY['a'], ARRAY[1.0]), MAP(ARRAY['a'], ARRAY[2.0])); -- 1.0 Floating point functions# infinity() → double# Returns the constant representing positive infinity. is_finite(x) → boolean# Determine if x is finite. is_infinite(x) → boolean# Determine if x is infinite. is_nan(x) → boolean# Determine if x is not-a-number. nan() → double# Returns the constant representing not-a-number. Base conversion functions# from_base(string, radix) → bigint# Returns the value of string interpreted as a base-radix number. to_base(x, radix) → varchar# Returns the base-radix representation of x. Statistical functions# t_pdf(x, df) → double# Computes the Student’s t-distribution probability density function for given x and degrees of freedom (df). The x must be a real value and degrees of freedom must be an integer and positive value. wilson_interval_lower(successes, trials, z) → double# Returns the lower bound of the Wilson score interval of a Bernoulli trial process at a confidence specified by the z-score z. wilson_interval_upper(successes, trials, z) → double# Returns the upper bound of the Wilson score interval of a Bernoulli trial process at a confidence specified by the z-score z. Cumulative distribution functions# beta_cdf(a, b, v) → double# Compute the Beta cdf with given a, b parameters: P(N < v; a, b). The a, b parameters must be positive real numbers and value v must be a real value. The value v must lie on the interval [0, 1]. inverse_beta_cdf(a, b, p) → double# Compute the inverse of the Beta cdf with given a, b parameters for the cumulative probability (p): P(N < n). The a, b parameters must be positive real values. The probability p must lie on the interval [0, 1]. inverse_normal_cdf(mean, sd, p) → double# Compute the inverse of the Normal cdf with given mean and standard deviation (sd) for the cumulative probability (p): P(N < n). The mean must be a real value and the standard deviation must be a real and positive value. The probability p must lie on the interval (0, 1). normal_cdf(mean, sd, v) → double# Compute the Normal cdf with given mean and standard deviation (sd): P(N < v; mean, sd). The mean and value v must be real values and the standard deviation must be a real and positive value. t_cdf(x, df) → double# Compute the Student’s t-distribution cumulative density function for given x and degrees of freedom (df). The x must be a real value and degrees of freedom must be an integer and positive value.

#### Code Examples

```
SELECT cosine_distance(ARRAY[1.0, 2.0], ARRAY[3.0, 4.0]);
-- 0.01613008990009257
```
```
SELECT cosine_similarity(ARRAY[1.0, 2.0], ARRAY[3.0, 4.0]);
-- 0.9838699100999074
```
```
SELECT cosine_similarity(MAP(ARRAY['a'], ARRAY[1.0]), MAP(ARRAY['a'], ARRAY[2.0]));
-- 1.0
```


---

### Quantile Digest Functions Documentation
Source: https://trino.io/docs/current/functions/qdigest.html

Quantile digest functions# Data structures# A quantile digest is a data sketch which stores approximate percentile information. The Trino type for this data structure is called qdigest, and it takes a parameter which must be one of bigint, double or real which represent the set of numbers that may be ingested by the qdigest. They may be merged without losing precision, and for storage and retrieval they may be cast to/from VARBINARY. Functions# merge(qdigest) → qdigest Merges all input qdigests into a single qdigest. value_at_quantile(qdigest(T), quantile) → T# Returns the approximate percentile value from the quantile digest given the number quantile between 0 and 1. quantile_at_value(qdigest(T), T) → quantile# Returns the approximate quantile number between 0 and 1 from the quantile digest given an input value. Null is returned if the quantile digest is empty or the input value is outside of the range of the quantile digest. values_at_quantiles(qdigest(T), quantiles) -> array(T)# Returns the approximate percentile values as an array given the input quantile digest and array of values between 0 and 1 which represent the quantiles to return. qdigest_agg(x) -> qdigest([same as x])# Returns the qdigest which is composed of all input values of x. qdigest_agg(x, w) -> qdigest([same as x]) Returns the qdigest which is composed of all input values of x using the per-item weight w. qdigest_agg(x, w, accuracy) -> qdigest([same as x]) Returns the qdigest which is composed of all input values of x using the per-item weight w and maximum error of accuracy. accuracy must be a value greater than zero and less than one, and it must be constant for all input rows.

---

### Regular Expression Functions Documentation
Source: https://trino.io/docs/current/functions/regexp.html

Regular expression functions# All of the regular expression functions use the Java pattern syntax, with a few notable exceptions: When using multi-line mode (enabled via the (?m) flag), only \n is recognized as a line terminator. Additionally, the (?d) flag is not supported and must not be used. Case-insensitive matching (enabled via the (?i) flag) is always performed in a Unicode-aware manner. However, context-sensitive and local-sensitive matching is not supported. Additionally, the (?u) flag is not supported and must not be used. Surrogate pairs are not supported. For example, \uD800\uDC00 is not treated as U+10000 and must be specified as \x{10000}. Boundaries (\b) are incorrectly handled for a non-spacing mark without a base character. \Q and \E are not supported in character classes (such as [A-Z123]) and are instead treated as literals. Unicode character classes (\p{prop}) are supported with the following differences: All underscores in names must be removed. For example, use OldItalic instead of Old_Italic. Scripts must be specified directly, without the Is, script= or sc= prefixes. Example: \p{Hiragana} Blocks must be specified with the In prefix. The block= and blk= prefixes are not supported. Example: \p{Mongolian} Categories must be specified directly, without the Is, general_category= or gc= prefixes. Example: \p{L} Binary properties must be specified directly, without the Is. Example: \p{NoncharacterCodePoint} regexp_count(string, pattern) → bigint# Returns the number of occurrence of pattern in string: SELECT regexp_count('1a 2b 14m', '\s*[a-z]+\s*'); -- 3 regexp_extract_all(string, pattern)# Returns the substring(s) matched by the regular expression pattern in string: SELECT regexp_extract_all('1a 2b 14m', '\d+'); -- [1, 2, 14] regexp_extract_all(string, pattern, group) Finds all occurrences of the regular expression pattern in string and returns the capturing group number group: SELECT regexp_extract_all('1a 2b 14m', '(\d+)([a-z]+)', 2); -- ['a', 'b', 'm'] regexp_extract(string, pattern) → varchar# Returns the first substring matched by the regular expression pattern in string: SELECT regexp_extract('1a 2b 14m', '\d+'); -- 1 regexp_extract(string, pattern, group) → varchar Finds the first occurrence of the regular expression pattern in string and returns the capturing group number group: SELECT regexp_extract('1a 2b 14m', '(\d+)([a-z]+)', 2); -- 'a' regexp_like(string, pattern) → boolean# Evaluates the regular expression pattern and determines if it is contained within string. The pattern only needs to be contained within string, rather than needing to match all of string. In other words, this performs a contains operation rather than a match operation. You can match the entire string by anchoring the pattern using ^ and $: SELECT regexp_like('1a 2b 14m', '\d+b'); -- true regexp_position(string, pattern) → integer# Returns the index of the first occurrence (counting from 1) of pattern in string. Returns -1 if not found: SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b'); -- 8 regexp_position(string, pattern, start) → integer Returns the index of the first occurrence of pattern in string, starting from start (include start). Returns -1 if not found: SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 5); -- 8 SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 12); -- 19 regexp_position(string, pattern, start, occurrence) → integer Returns the index of the nth occurrence of pattern in string, starting from start (include start). Returns -1 if not found: SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 12, 1); -- 19 SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 12, 2); -- 31 SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 12, 3); -- -1 regexp_replace(string, pattern) → varchar# Removes every instance of the substring matched by the regular expression pattern from string: SELECT regexp_replace('1a 2b 14m', '\d+[ab] '); -- '14m' regexp_replace(string, pattern, replacement) → varchar Replaces every instance of the substring matched by the regular expression pattern in string with replacement. Capturing groups can be referenced in replacement using $g for a numbered group or ${name} for a named group. A dollar sign ($) may be included in the replacement by escaping it with a backslash (\$): SELECT regexp_replace('1a 2b 14m', '(\d+)([ab]) ', '3c$2 '); -- '3ca 3cb 14m' regexp_replace(string, pattern, function) → varchar Replaces every instance of the substring matched by the regular expression pattern in string using function. The lambda expression function is invoked for each match with the capturing groups passed as an array. Capturing group numbers start at one; there is no group for the entire match (if you need this, surround the entire expression with parenthesis). SELECT regexp_replace('new york', '(\w)(\w*)', x -> upper(x[1]) || lower(x[2])); --'New York' regexp_split(string, pattern)# Splits string using the regular expression pattern and returns an array. Trailing empty strings are preserved: SELECT regexp_split('1a 2b 14m', '\s*[a-z]+\s*'); -- [1, 2, 14, ]

#### Code Examples

```
SELECT regexp_count('1a 2b 14m', '\s*[a-z]+\s*'); -- 3
```
```
SELECT regexp_extract_all('1a 2b 14m', '\d+'); -- [1, 2, 14]
```
```
SELECT regexp_extract_all('1a 2b 14m', '(\d+)([a-z]+)', 2); -- ['a', 'b', 'm']
```
```
SELECT regexp_extract('1a 2b 14m', '\d+'); -- 1
```
```
SELECT regexp_extract('1a 2b 14m', '(\d+)([a-z]+)', 2); -- 'a'
```
```
SELECT regexp_like('1a 2b 14m', '\d+b'); -- true
```
```
SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b'); -- 8
```
```
SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 5); -- 8
SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 12); -- 19
```
```
SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 12, 1); -- 19
SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 12, 2); -- 31
SELECT regexp_position('I have 23 apples, 5 pears and 13 oranges', '\b\d+\b', 12, 3); -- -1
```
```
SELECT regexp_replace('1a 2b 14m', '\d+[ab] '); -- '14m'
```
```
SELECT regexp_replace('1a 2b 14m', '(\d+)([ab]) ', '3c$2 '); -- '3ca 3cb 14m'
```
```
SELECT regexp_replace('new york', '(\w)(\w*)', x -> upper(x[1]) || lower(x[2])); --'New York'
```
```
SELECT regexp_split('1a 2b 14m', '\s*[a-z]+\s*'); -- [1, 2, 14, ]
```


---

### Session Functions Documentation
Source: https://trino.io/docs/current/functions/session.html

Session information# Functions providing information about the query execution environment. current_user# Returns the current user running the query. current_groups()# Returns the list of groups for the current user running the query. current_catalog# Returns a character string that represents the current catalog name. current_schema# Returns a character string that represents the current unqualified schema name. Note This is part of the SQL standard and does not use parenthesis.

---

### Set Digest Functions Documentation
Source: https://trino.io/docs/current/functions/setdigest.html

Set Digest functions# Trino offers several functions that deal with the MinHash technique. MinHash is used to quickly estimate the Jaccard similarity coefficient between two sets. It is commonly used in data mining to detect near-duplicate web pages at scale. By using this information, the search engines efficiently avoid showing within the search results two pages that are nearly identical. The following example showcases how the Set Digest functions can be used to naively estimate the similarity between texts. The input texts are split by using the function ngrams() to 4-shingles which are used as input for creating a set digest of each initial text. The set digests are compared to each other to get an approximation of the similarity of their corresponding initial texts: WITH text_input(id, text) AS ( VALUES (1, 'The quick brown fox jumps over the lazy dog'), (2, 'The quick and the lazy'), (3, 'The quick brown fox jumps over the dog') ), text_ngrams(id, ngrams) AS ( SELECT id, transform( ngrams( split(text, ' '), 4 ), token -> array_join(token, ' ') ) FROM text_input ), minhash_digest(id, digest) AS ( SELECT id, (SELECT make_set_digest(v) FROM unnest(ngrams) u(v)) FROM text_ngrams ), setdigest_side_by_side(id1, digest1, id2, digest2) AS ( SELECT m1.id as id1, m1.digest as digest1, m2.id as id2, m2.digest as digest2 FROM (SELECT id, digest FROM minhash_digest) m1 JOIN (SELECT id, digest FROM minhash_digest) m2 ON m1.id != m2.id AND m1.id < m2.id ) SELECT id1, id2, intersection_cardinality(digest1, digest2) AS intersection_cardinality, jaccard_index(digest1, digest2) AS jaccard_index FROM setdigest_side_by_side ORDER BY id1, id2; id1 | id2 | intersection_cardinality | jaccard_index -----+-----+--------------------------+--------------- 1 | 2 | 0 | 0.0 1 | 3 | 4 | 0.6 2 | 3 | 0 | 0.0 The above result listing points out, as expected, that the texts with the id 1 and 3 are quite similar. One may argue that the text with the id 2 is somewhat similar to the texts with the id 1 and 3. Due to the fact in the example above 4-shingles are taken into account for measuring the similarity of the texts, there are no intersections found for the text pairs 1 and 2, respectively 3 and 2 and therefore there the similarity index for these text pairs is 0. Data structures# Trino implements Set Digest data sketches by encapsulating the following components: HyperLogLog MinHash with a single hash function The HyperLogLog structure is used for the approximation of the distinct elements in the original set. The MinHash structure is used to store a low memory footprint signature of the original set. The similarity of any two sets is estimated by comparing their signatures. The Trino type for this data structure is called setdigest. Trino offers the ability to merge multiple Set Digest data sketches. Serialization# Data sketches can be serialized to and deserialized from varbinary. This allows them to be stored for later use. Functions# make_set_digest(x) → setdigest# Composes all input values of x into a setdigest. Create a setdigest corresponding to a bigint array: SELECT make_set_digest(value) FROM (VALUES 1, 2, 3) T(value); Create a setdigest corresponding to a varchar array: SELECT make_set_digest(value) FROM (VALUES 'Trino', 'SQL', 'on', 'everything') T(value); merge_set_digest(setdigest) → setdigest# Returns the setdigest of the aggregate union of the individual setdigest Set Digest structures. cardinality(setdigest) → long Returns the cardinality of the set digest from its internal HyperLogLog component. Examples: SELECT cardinality(make_set_digest(value)) FROM (VALUES 1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5) T(value); -- 5 intersection_cardinality(x, y) → long# Returns the estimation for the cardinality of the intersection of the two set digests. x and y must be of type setdigest Examples: SELECT intersection_cardinality(make_set_digest(v1), make_set_digest(v2)) FROM (VALUES (1, 1), (NULL, 2), (2, 3), (3, 4)) T(v1, v2); -- 3 jaccard_index(x, y) → double# Returns the estimation of Jaccard index for the two set digests. x and y must be of type setdigest. Examples: SELECT jaccard_index(make_set_digest(v1), make_set_digest(v2)) FROM (VALUES (1, 1), (NULL,2), (2, 3), (NULL, 4)) T(v1, v2); -- 0.5 hash_counts(x)# Returns a map containing the Murmur3Hash128 hashed values and the count of their occurences within the internal MinHash structure belonging to x. x must be of type setdigest. Examples: SELECT hash_counts(make_set_digest(value)) FROM (VALUES 1, 1, 1, 2, 2) T(value); -- {19144387141682250=3, -2447670524089286488=2}

#### Code Examples

```
WITH text_input(id, text) AS (
         VALUES
             (1, 'The quick brown fox jumps over the lazy dog'),
             (2, 'The quick and the lazy'),
             (3, 'The quick brown fox jumps over the dog')
     ),
     text_ngrams(id, ngrams) AS (
         SELECT id,
                transform(
                  ngrams(
                    split(text, ' '),
                    4
                  ),
                  token -> array_join(token, ' ')
                )
         FROM text_input
     ),
     minhash_digest(id, digest) AS (
         SELECT id,
                (SELECT make_set_digest(v) FROM unnest(ngrams) u(v))
         FROM text_ngrams
     ),
     setdigest_side_by_side(id1, digest1, id2, digest2) AS (
         SELECT m1.id as id1,
                m1.digest as digest1,
                m2.id as id2,
                m2.digest as digest2
         FROM (SELECT id, digest FROM minhash_digest) m1
         JOIN (SELECT id, digest FROM minhash_digest) m2
           ON m1.id != m2.id AND m1.id < m2.id
     )
SELECT id1,
       id2,
       intersection_cardinality(digest1, digest2) AS intersection_cardinality,
       jaccard_index(digest1, digest2)            AS jaccard_index
FROM setdigest_side_by_side
ORDER BY id1, id2;
```
```
id1 | id2 | intersection_cardinality | jaccard_index
-----+-----+--------------------------+---------------
   1 |   2 |                        0 |           0.0
   1 |   3 |                        4 |           0.6
   2 |   3 |                        0 |           0.0
```
```
SELECT make_set_digest(value)
FROM (VALUES 1, 2, 3) T(value);
```
```
SELECT make_set_digest(value)
FROM (VALUES 'Trino', 'SQL', 'on', 'everything') T(value);
```
```
SELECT cardinality(make_set_digest(value))
FROM (VALUES 1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5) T(value);
-- 5
```
```
SELECT intersection_cardinality(make_set_digest(v1), make_set_digest(v2))
FROM (VALUES (1, 1), (NULL, 2), (2, 3), (3, 4)) T(v1, v2);
-- 3
```
```
SELECT jaccard_index(make_set_digest(v1), make_set_digest(v2))
FROM (VALUES (1, 1), (NULL,2), (2, 3), (NULL, 4)) T(v1, v2);
-- 0.5
```
```
SELECT hash_counts(make_set_digest(value))
FROM (VALUES 1, 1, 1, 2, 2) T(value);
-- {19144387141682250=3, -2447670524089286488=2}
```


---

### String Functions Documentation
Source: https://trino.io/docs/current/functions/string.html

String functions and operators# String operators# The || operator performs concatenation. The LIKE statement can be used for pattern matching and is documented in Pattern comparison: LIKE. String functions# Note These functions assume that the input strings contain valid UTF-8 encoded Unicode code points. There are no explicit checks for valid UTF-8 and the functions may return incorrect results on invalid UTF-8. Invalid UTF-8 data can be corrected with from_utf8(). Additionally, the functions operate on Unicode code points and not user visible characters (or grapheme clusters). Some languages combine multiple code points into a single user-perceived character, the basic unit of a writing system for a language, but the functions will treat each code point as a separate unit. The lower() and upper() functions do not perform locale-sensitive, context-sensitive, or one-to-many mappings required for some languages. Specifically, this will return incorrect results for Lithuanian, Turkish and Azeri. chr(n) → varchar# Returns the Unicode code point n as a single character string. codepoint(string) → integer# Returns the Unicode code point of the only character of string. concat(string1, ..., stringN) → varchar# Returns the concatenation of string1, string2, ..., stringN. This function provides the same functionality as the SQL-standard concatenation operator (||). concat_ws(string0, string1, ..., stringN) → varchar# Returns the concatenation of string1, string2, ..., stringN using string0 as a separator. If string0 is null, then the return value is null. Any null values provided in the arguments after the separator are skipped. concat_ws(string0, array(varchar)) → varchar Returns the concatenation of elements in the array using string0 as a separator. If string0 is null, then the return value is null. Any null values in the array are skipped. format(format, args...) → varchar See format(). hamming_distance(string1, string2) → bigint# Returns the Hamming distance of string1 and string2, i.e. the number of positions at which the corresponding characters are different. Note that the two strings must have the same length. length(string) → bigint# Returns the length of string in characters. levenshtein_distance(string1, string2) → bigint# Returns the Levenshtein edit distance of string1 and string2, i.e. the minimum number of single-character edits (insertions, deletions or substitutions) needed to change string1 into string2. lower(string) → varchar# Converts string to lowercase. lpad(string, size, padstring) → varchar# Left pads string to size characters with padstring. If size is less than the length of string, the result is truncated to size characters. size must not be negative and padstring must be non-empty. ltrim(string) → varchar# Removes leading whitespace from string. luhn_check(string) → boolean# Tests whether a string of digits is valid according to the Luhn algorithm. This checksum function, also known as modulo 10 or mod 10, is widely applied on credit card numbers and government identification numbers to distinguish valid numbers from mistyped, incorrect numbers. Valid identification number: select luhn_check('79927398713'); -- true Invalid identification number: select luhn_check('79927398714'); -- false position(substring IN string) → bigint# Returns the starting position of the first instance of substring in string. Positions start with 1. If not found, 0 is returned. Note This SQL-standard function has special syntax and uses the IN keyword for the arguments. See also strpos(). replace(string, search) → varchar# Removes all instances of search from string. replace(string, search, replace) → varchar Replaces all instances of search with replace in string. reverse(string) → varchar# Returns string with the characters in reverse order. rpad(string, size, padstring) → varchar# Right pads string to size characters with padstring. If size is less than the length of string, the result is truncated to size characters. size must not be negative and padstring must be non-empty. rtrim(string) → varchar# Removes trailing whitespace from string. soundex(char) → string# soundex returns a character string containing the phonetic representation of char.It is typically used to evaluate the similarity of two expressions phonetically, that is how the string sounds when spoken: SELECT name FROM nation WHERE SOUNDEX(name) = SOUNDEX('CHYNA'); name | -------+---- CHINA | (1 row) split(string, delimiter)# Splits string on delimiter and returns an array. split(string, delimiter, limit) Splits string on delimiter and returns an array of size at most limit. The last element in the array always contain everything left in the string. limit must be a positive number. split_part(string, delimiter, index) → varchar# Splits string on delimiter and returns the field index. Field indexes start with 1. If the index is larger than the number of fields, then null is returned. split_to_map(string, entryDelimiter, keyValueDelimiter) → map<varchar, varchar># Splits string by entryDelimiter and keyValueDelimiter and returns a map. entryDelimiter splits string into key-value pairs. keyValueDelimiter splits each pair into key and value. split_to_multimap(string, entryDelimiter, keyValueDelimiter)# Splits string by entryDelimiter and keyValueDelimiter and returns a map containing an array of values for each unique key. entryDelimiter splits string into key-value pairs. keyValueDelimiter splits each pair into key and value. The values for each key will be in the same order as they appeared in string. strpos(string, substring) → bigint# Returns the starting position of the first instance of substring in string. Positions start with 1. If not found, 0 is returned. strpos(string, substring, instance) → bigint Returns the position of the N-th instance of substring in string. When instance is a negative number the search will start from the end of string. Positions start with 1. If not found, 0 is returned. starts_with(string, substring) → boolean# Tests whether substring is a prefix of string. substr(string, start) → varchar# This is an alias for substring(). substring(string, start) → varchar# Returns the rest of string from the starting position start. Positions start with 1. A negative starting position is interpreted as being relative to the end of the string. substr(string, start, length) → varchar This is an alias for substring(). substring(string, start, length) → varchar Returns a substring from string of length length from the starting position start. Positions start with 1. A negative starting position is interpreted as being relative to the end of the string. translate(source, from, to) → varchar# Returns the source string translated by replacing characters found in the from string with the corresponding characters in the to string. If the from string contains duplicates, only the first is used. If the source character does not exist in the from string, the source character will be copied without translation. If the index of the matching character in the from string is beyond the length of the to string, the source character will be omitted from the resulting string. Here are some examples illustrating the translate function: SELECT translate('abcd', '', ''); -- 'abcd' SELECT translate('abcd', 'a', 'z'); -- 'zbcd' SELECT translate('abcda', 'a', 'z'); -- 'zbcdz' SELECT translate('Palhoça', 'ç','c'); -- 'Palhoca' SELECT translate('abcd', 'b', U&'\+01F600'); -- a😀cd SELECT translate('abcd', 'a', ''); -- 'bcd' SELECT translate('abcd', 'a', 'zy'); -- 'zbcd' SELECT translate('abcd', 'ac', 'z'); -- 'zbd' SELECT translate('abcd', 'aac', 'zq'); -- 'zbd' trim(string) → varchar Removes leading and trailing whitespace from string. trim([ [ specification ] [ string ] FROM ] source ) → varchar# Removes any leading and/or trailing characters as specified up to and including string from source: SELECT trim('!' FROM '!foo!'); -- 'foo' SELECT trim(LEADING FROM ' abcd'); -- 'abcd' SELECT trim(BOTH '$' FROM '$var$'); -- 'var' SELECT trim(TRAILING 'ER' FROM upper('worker')); -- 'WORK' upper(string) → varchar# Converts string to uppercase. word_stem(word) → varchar# Returns the stem of word in the English language. word_stem(word, lang) → varchar Returns the stem of word in the lang language. Unicode functions# normalize(string) → varchar# Transforms string with NFC normalization form. normalize(string, form) → varchar Transforms string with the specified normalization form. form must be one of the following keywords: Form Description NFD Canonical Decomposition NFC Canonical Decomposition, followed by Canonical Composition NFKD Compatibility Decomposition NFKC Compatibility Decomposition, followed by Canonical Composition Note This SQL-standard function has special syntax and requires specifying form as a keyword, not as a string. to_utf8(string) → varbinary# Encodes string into a UTF-8 varbinary representation. from_utf8(binary) → varchar# Decodes a UTF-8 encoded string from binary. Invalid UTF-8 sequences are replaced with the Unicode replacement character U+FFFD. from_utf8(binary, replace) → varchar Decodes a UTF-8 encoded string from binary. Invalid UTF-8 sequences are replaced with replace. The replacement string replace must either be a single character or empty (in which case invalid characters are removed).

#### Code Examples

```
select luhn_check('79927398713');
-- true
```
```
select luhn_check('79927398714');
-- false
```
```
SELECT name
FROM nation
WHERE SOUNDEX(name)  = SOUNDEX('CHYNA');

 name  |
-------+----
 CHINA |
(1 row)
```
```
SELECT translate('abcd', '', ''); -- 'abcd'
SELECT translate('abcd', 'a', 'z'); -- 'zbcd'
SELECT translate('abcda', 'a', 'z'); -- 'zbcdz'
SELECT translate('Palhoça', 'ç','c'); -- 'Palhoca'
SELECT translate('abcd', 'b', U&'\+01F600'); -- a😀cd
SELECT translate('abcd', 'a', ''); -- 'bcd'
SELECT translate('abcd', 'a', 'zy'); -- 'zbcd'
SELECT translate('abcd', 'ac', 'z'); -- 'zbd'
SELECT translate('abcd', 'aac', 'zq'); -- 'zbd'
```
```
SELECT trim('!' FROM '!foo!'); -- 'foo'
SELECT trim(LEADING FROM '  abcd');  -- 'abcd'
SELECT trim(BOTH '$' FROM '$var$'); -- 'var'
SELECT trim(TRAILING 'ER' FROM upper('worker')); -- 'WORK'
```


---

### System Functions Documentation
Source: https://trino.io/docs/current/functions/system.html

System information# Functions providing information about the Trino cluster system environment. More information is available by querying the various schemas and tables exposed by the System connector. version() → varchar# Returns the Trino version used on the cluster. Equivalent to the value of the node_version column in the system.runtime.nodes table.

---

### Table Functions Documentation
Source: https://trino.io/docs/current/functions/table.html

Table functions# A table function is a function returning a table. It can be invoked inside the FROM clause of a query: SELECT * FROM TABLE(my_function(1, 100)) The row type of the returned table can depend on the arguments passed with invocation of the function. If different row types can be returned, the function is a polymorphic table function. Polymorphic table functions allow you to dynamically invoke custom logic from within the SQL query. They can be used for working with external systems as well as for enhancing Trino with capabilities going beyond the SQL standard. For the list of built-in table functions available in Trino, see built in table functions. Trino supports adding custom table functions. They are declared by connectors through implementing dedicated interfaces. For guidance on adding new table functions, see the developer guide. Connectors offer support for different functions on a per-connector basis. For more information about supported table functions, refer to the connector documentation. Built-in table functions# exclude_columns table function# Use the exclude_columns table function to return a new table based on an input table table, with the exclusion of all columns specified in descriptor: exclude_columns(input => table, columns => descriptor) → table The argument input is a table or a query. The argument columns is a descriptor without types. Example query using the orders table from the TPC-H dataset, provided by the TPC-H connector: SELECT * FROM TABLE(exclude_columns( input => TABLE(orders), columns => DESCRIPTOR(clerk, comment))); The table function is useful for queries where you want to return nearly all columns from tables with many columns. You can avoid enumerating all columns, and only need to specify the columns to exclude. sequence table function# Use the sequence table function to return a table with a single column sequential_number containing a sequence of bigint: sequence(start => bigint, stop => bigint, step => bigint) -> table(sequential_number bigint) start is the first element in the sequence. The default value is 0. stop is the end of the range, inclusive. The last element in the sequence is equal to stop, or it is the last value within range, reachable by steps. step is the difference between subsequent values. The default value is 1. Example query: SELECT * FROM TABLE(sequence( start => 1000000, stop => -2000000, step => -3)); The result of the sequence table function might not be ordered. If required, enforce ordering in the enclosing query: SELECT * FROM TABLE(sequence( start => 0, stop => 100, step => 5)) ORDER BY sequential_number; Table function invocation# You invoke a table function in the FROM clause of a query. Table function invocation syntax is similar to a scalar function call. Function resolution# Every table function is provided by a catalog, and it belongs to a schema in the catalog. You can qualify the function name with a schema name, or with catalog and schema names: SELECT * FROM TABLE(schema_name.my_function(1, 100)) SELECT * FROM TABLE(catalog_name.schema_name.my_function(1, 100)) Otherwise, the standard Trino name resolution is applied. The connection between the function and the catalog must be identified, because the function is executed by the corresponding connector. If the function is not registered by the specified catalog, the query fails. The table function name is resolved case-insensitive, analogically to scalar function and table resolution in Trino. Arguments# There are three types of arguments. Scalar arguments They must be constant expressions, and they can be of any SQL type, which is compatible with the declared argument type: factor => 42 Descriptor arguments Descriptors consist of fields with names and optional data types: schema => DESCRIPTOR(id BIGINT, name VARCHAR) columns => DESCRIPTOR(date, status, comment) To pass null for a descriptor, use: schema => CAST(null AS DESCRIPTOR) Table arguments You can pass a table name, or a query. Use the keyword TABLE: input => TABLE(orders) data => TABLE(SELECT * FROM region, nation WHERE region.regionkey = nation.regionkey) If the table argument is declared as set semantics, you can specify partitioning and ordering. Each partition is processed independently by the table function. If you do not specify partitioning, the argument is processed as a single partition. You can also specify PRUNE WHEN EMPTY or KEEP WHEN EMPTY. With PRUNE WHEN EMPTY you declare that you are not interested in the function result if the argument is empty. This information is used by the Trino engine to optimize the query. The KEEP WHEN EMPTY option indicates that the function should be executed even if the table argument is empty. By specifying KEEP WHEN EMPTY or PRUNE WHEN EMPTY, you override the property set for the argument by the function author. The following example shows how the table argument properties should be ordered: input => TABLE(orders) PARTITION BY orderstatus KEEP WHEN EMPTY ORDER BY orderdate Argument passing conventions# There are two conventions of passing arguments to a table function: Arguments passed by name: SELECT * FROM TABLE(my_function(row_count => 100, column_count => 1)) In this convention, you can pass the arguments in arbitrary order. Arguments declared with default values can be skipped. Argument names are resolved case-sensitive, and with automatic uppercasing of unquoted names. Arguments passed positionally: SELECT * FROM TABLE(my_function(1, 100)) In this convention, you must follow the order in which the arguments are declared. You can skip a suffix of the argument list, provided that all the skipped arguments are declared with default values. You cannot mix the argument conventions in one invocation. You can also use parameters in arguments: PREPARE stmt FROM SELECT * FROM TABLE(my_function(row_count => ? + 1, column_count => ?)); EXECUTE stmt USING 100, 1;

#### Code Examples

```
SELECT * FROM TABLE(my_function(1, 100))
```
```
SELECT *
FROM TABLE(exclude_columns(
                        input => TABLE(orders),
                        columns => DESCRIPTOR(clerk, comment)));
```
```
SELECT *
FROM TABLE(sequence(
                start => 1000000,
                stop => -2000000,
                step => -3));
```
```
SELECT *
FROM TABLE(sequence(
                start => 0,
                stop => 100,
                step => 5))
ORDER BY sequential_number;
```
```
SELECT * FROM TABLE(schema_name.my_function(1, 100))
SELECT * FROM TABLE(catalog_name.schema_name.my_function(1, 100))
```
```
factor => 42
```
```
schema => DESCRIPTOR(id BIGINT, name VARCHAR)
columns => DESCRIPTOR(date, status, comment)
```
```
schema => CAST(null AS DESCRIPTOR)
```
```
input => TABLE(orders)
data => TABLE(SELECT * FROM region, nation WHERE region.regionkey = nation.regionkey)
```
```
input => TABLE(orders)
                    PARTITION BY orderstatus
                    KEEP WHEN EMPTY
                    ORDER BY orderdate
```
```
SELECT * FROM TABLE(my_function(row_count => 100, column_count => 1))
```
```
SELECT * FROM TABLE(my_function(1, 100))
```
```
PREPARE stmt FROM
SELECT * FROM TABLE(my_function(row_count => ? + 1, column_count => ?));

EXECUTE stmt USING 100, 1;
```


---

### Teradata Functions Documentation
Source: https://trino.io/docs/current/functions/teradata.html

Teradata functions# These functions provide compatibility with Teradata SQL. String functions# char2hexint(string) → varchar# Returns the hexadecimal representation of the UTF-16BE encoding of the string. index(string, substring) → bigint# Alias for strpos() function. Date functions# The functions in this section use a format string that is compatible with the Teradata datetime functions. The following table, based on the Teradata reference manual, describes the supported format specifiers: Specifier Description - / , . ; : Punctuation characters are ignored dd Day of month (1-31) hh Hour of day (1-12) hh24 Hour of the day (0-23) mi Minute (0-59) mm Month (01-12) ss Second (0-59) yyyy 4-digit year yy 2-digit year Warning Case insensitivity is not currently supported. All specifiers must be lowercase. to_char(timestamp, format) → varchar# Formats timestamp as a string using format. to_timestamp(string, format) → timestamp# Parses string into a TIMESTAMP using format. to_date(string, format) → date# Parses string into a DATE using format.

---

### T-Digest Functions Documentation
Source: https://trino.io/docs/current/functions/tdigest.html

T-Digest functions# Data structures# A T-digest is a data sketch which stores approximate percentile information. The Trino type for this data structure is called tdigest. T-digests can be merged, and for storage and retrieval they can be cast to and from VARBINARY. Functions# merge(tdigest) → tdigest Aggregates all inputs into a single tdigest. value_at_quantile(tdigest, quantile) → double Returns the approximate percentile value from the T-digest, given the number quantile between 0 and 1. values_at_quantiles(tdigest, quantiles) Returns the approximate percentile values as an array, given the input T-digest and an array of values between 0 and 1, which represent the quantiles to return. tdigest_agg(x) → tdigest# Composes all input values of x into a tdigest. x can be of any numeric type. tdigest_agg(x, w) → tdigest Composes all input values of x into a tdigest using the per-item weight w. w must be greater or equal than 1. x and w can be of any numeric type.

---

### URL Functions Documentation
Source: https://trino.io/docs/current/functions/url.html

URL functions# Extraction functions# The URL extraction functions extract components from HTTP URLs (or any valid URIs conforming to RFC 2396). The following syntax is supported: [protocol:][//host[:port]][path][?query][#fragment] The extracted components do not contain URI syntax separators such as : or ?. url_extract_fragment(url) → varchar# Returns the fragment identifier from url. url_extract_host(url) → varchar# Returns the host from url. url_extract_parameter(url, name) → varchar# Returns the value of the first query string parameter named name from url. Parameter extraction is handled in the typical manner as specified by RFC 1866#section-8.2.1. url_extract_path(url) → varchar# Returns the path from url. url_extract_port(url) → bigint# Returns the port number from url. url_extract_protocol(url) → varchar# Returns the protocol from url: SELECT url_extract_protocol('http://localhost:8080/req_path'); -- http SELECT url_extract_protocol('https://127.0.0.1:8080/req_path'); -- https SELECT url_extract_protocol('ftp://path/file'); -- ftp url_extract_query(url) → varchar# Returns the query string from url. Encoding functions# url_encode(value) → varchar# Escapes value by encoding it so that it can be safely included in URL query parameter names and values: Alphanumeric characters are not encoded. The characters ., -, * and _ are not encoded. The ASCII space character is encoded as +. All other characters are converted to UTF-8 and the bytes are encoded as the string %XX where XX is the uppercase hexadecimal value of the UTF-8 byte. url_decode(value) → varchar# Unescapes the URL encoded value. This function is the inverse of url_encode().

#### Code Examples

```
[protocol:][//host[:port]][path][?query][#fragment]
```
```
SELECT url_extract_protocol('http://localhost:8080/req_path');
-- http

SELECT url_extract_protocol('https://127.0.0.1:8080/req_path');
-- https

SELECT url_extract_protocol('ftp://path/file');
-- ftp
```


---

### UUID Functions Documentation
Source: https://trino.io/docs/current/functions/uuid.html

UUID functions# uuid() → uuid# Returns a pseudo randomly generated UUID (type 4).

---

### Window Functions Documentation
Source: https://trino.io/docs/current/functions/window.html

Window functions# Window functions perform calculations across rows of the query result. They run after the HAVING clause but before the ORDER BY clause. Invoking a window function requires special syntax using the OVER clause to specify the window. For example, the following query ranks orders for each clerk by price: SELECT orderkey, clerk, totalprice, rank() OVER (PARTITION BY clerk ORDER BY totalprice DESC) AS rnk FROM orders ORDER BY clerk, rnk The window can be specified in two ways (see WINDOW clause): By a reference to a named window specification defined in the WINDOW clause, By an in-line window specification which allows to define window components as well as refer to the window components pre-defined in the WINDOW clause. Aggregate functions# All Aggregate functions can be used as window functions by adding the OVER clause. The aggregate function is computed for each row over the rows within the current row’s window frame. Note that ordering during aggregation is not supported. For example, the following query produces a rolling sum of order prices by day for each clerk: SELECT clerk, orderdate, orderkey, totalprice, sum(totalprice) OVER (PARTITION BY clerk ORDER BY orderdate) AS rolling_sum FROM orders ORDER BY clerk, orderdate, orderkey Ranking functions# cume_dist() → bigint# Returns the cumulative distribution of a value in a group of values. The result is the number of rows preceding or peer with the row in the window ordering of the window partition divided by the total number of rows in the window partition. Thus, any tie values in the ordering will evaluate to the same distribution value. The window frame must not be specified. dense_rank() → bigint# Returns the rank of a value in a group of values. This is similar to rank(), except that tie values do not produce gaps in the sequence. The window frame must not be specified. ntile(n) → bigint# Divides the rows for each window partition into n buckets ranging from 1 to at most n. Bucket values will differ by at most 1. If the number of rows in the partition does not divide evenly into the number of buckets, then the remainder values are distributed one per bucket, starting with the first bucket. For example, with 6 rows and 4 buckets, the bucket values would be as follows: 1 1 2 2 3 4 For the ntile() function, the window frame must not be specified. percent_rank() → double# Returns the percentage ranking of a value in group of values. The result is (r - 1) / (n - 1) where r is the rank() of the row and n is the total number of rows in the window partition. The window frame must not be specified. rank() → bigint# Returns the rank of a value in a group of values. The rank is one plus the number of rows preceding the row that are not peer with the row. Thus, tie values in the ordering will produce gaps in the sequence. The ranking is performed for each window partition. The window frame must not be specified. row_number() → bigint# Returns a unique, sequential number for each row, starting with one, according to the ordering of rows within the window partition. The window frame must not be specified. Value functions# By default, null values are respected. If IGNORE NULLS is specified, all rows where x is null are excluded from the calculation. If IGNORE NULLS is specified and x is null for all rows, the default_value is returned, or if it is not specified, null is returned. first_value(x) → [same as input]# Returns the first value of the window. last_value(x) → [same as input]# Returns the last value of the window. nth_value(x, offset) → [same as input]# Returns the value at the specified offset from the beginning of the window. Offsets start at 1. The offset can be any scalar expression. If the offset is null or greater than the number of values in the window, null is returned. It is an error for the offset to be zero or negative. lead(x[, offset[, default_value]]) → [same as input]# Returns the value at offset rows after the current row in the window partition. Offsets start at 0, which is the current row. The offset can be any scalar expression. The default offset is 1. If the offset is null, an error is raised. If the offset refers to a row that is not within the partition, the default_value is returned, or if it is not specified null is returned. The lead() function requires that the window ordering be specified. Window frame must not be specified. lag(x[, offset[, default_value]]) → [same as input]# Returns the value at offset rows before the current row in the window partition. Offsets start at 0, which is the current row. The offset can be any scalar expression. The default offset is 1. If the offset is null, an error is raised. If the offset refers to a row that is not within the partition, the default_value is returned, or if it is not specified null is returned. The lag() function requires that the window ordering be specified. Window frame must not be specified.

#### Code Examples

```
SELECT orderkey, clerk, totalprice,
       rank() OVER (PARTITION BY clerk
                    ORDER BY totalprice DESC) AS rnk
FROM orders
ORDER BY clerk, rnk
```
```
SELECT clerk, orderdate, orderkey, totalprice,
       sum(totalprice) OVER (PARTITION BY clerk
                             ORDER BY orderdate) AS rolling_sum
FROM orders
ORDER BY clerk, orderdate, orderkey
```


---


## Query Optimizer

### Table Statistics Documentation
Source: https://trino.io/docs/current/optimizer/statistics.html

Table statistics# Trino supports statistics based optimizations for queries. For a query to take advantage of these optimizations, Trino must have statistical information for the tables in that query. Table statistics are estimates about the stored data. They are provided to the query planner by connectors and enable performance improvements for query processing. Available statistics# The following statistics are available in Trino: For a table: row count: the total number of rows in the table For each column in a table: data size: the size of the data that needs to be read nulls fraction: the fraction of null values distinct value count: the number of distinct values low value: the smallest value in the column high value: the largest value in the column The set of statistics available for a particular query depends on the connector being used and can also vary by table. For example, the Hive connector does not currently provide statistics on data size. Table statistics can be displayed via the Trino SQL interface using the SHOW STATS command. Depending on the connector support, table statistics are updated by Trino when executing data management statements like INSERT, UPDATE, or DELETE. For example, the Delta Lake connector, the Hive connector, and the Iceberg connector all support table statistics management from Trino. You can also initialize statistics collection with the ANALYZE command. This is needed when other systems manipulate the data without Trino, and therefore statistics tracked by Trino are out of date. Other connectors rely on the underlying data source to manage table statistics or do not support table statistics use at all.

---

### Cost in EXPLAIN Documentation
Source: https://trino.io/docs/current/optimizer/cost-in-explain.html

Cost in EXPLAIN# During planning, the cost associated with each node of the plan is computed based on the table statistics for the tables in the query. This calculated cost is printed as part of the output of an EXPLAIN statement. Cost information is displayed in the plan tree using the format {rows: XX (XX), cpu: XX, memory: XX, network: XX}. rows refers to the expected number of rows output by each plan node during execution. The value in the parentheses following the number of rows refers to the expected size of the data output by each plan node in bytes. Other parameters indicate the estimated amount of CPU, memory, and network utilized by the execution of a plan node. These values do not represent any actual unit, but are numbers that are used to compare the relative costs between plan nodes, allowing the optimizer to choose the best plan for executing a query. If any of the values is not known, a ? is printed. For example: EXPLAIN SELECT comment FROM tpch.sf1.nation WHERE nationkey > 3; - Output[comment] => [[comment]] Estimates: {rows: 22 (1.69kB), cpu: 6148.25, memory: 0.00, network: 1734.25} - RemoteExchange[GATHER] => [[comment]] Estimates: {rows: 22 (1.69kB), cpu: 6148.25, memory: 0.00, network: 1734.25} - ScanFilterProject[table = tpch:nation:sf1.0, filterPredicate = ("nationkey" > BIGINT '3')] => [[comment]] Estimates: {rows: 25 (1.94kB), cpu: 2207.00, memory: 0.00, network: 0.00}/{rows: 22 (1.69kB), cpu: 4414.00, memory: 0.00, network: 0.00}/{rows: 22 (1.69kB), cpu: 6148.25, memory: 0.00, network: 0.00} nationkey := tpch:nationkey comment := tpch:comment Generally, there is only one cost printed for each plan node. However, when a Scan operator is combined with a Filter and/or Project operator, then multiple cost structures are printed, each corresponding to an individual logical part of the combined operator. For example, three cost structures are printed for a ScanFilterProject operator, corresponding to the Scan, Filter, and Project parts of the operator, in that order. Estimated cost is also printed in EXPLAIN ANALYZE in addition to actual runtime statistics.

#### Code Examples

```
EXPLAIN SELECT comment FROM tpch.sf1.nation WHERE nationkey > 3;
```
```
- Output[comment] => [[comment]]
        Estimates: {rows: 22 (1.69kB), cpu: 6148.25, memory: 0.00, network: 1734.25}
    - RemoteExchange[GATHER] => [[comment]]
            Estimates: {rows: 22 (1.69kB), cpu: 6148.25, memory: 0.00, network: 1734.25}
        - ScanFilterProject[table = tpch:nation:sf1.0, filterPredicate = ("nationkey" > BIGINT '3')] => [[comment]]
                Estimates: {rows: 25 (1.94kB), cpu: 2207.00, memory: 0.00, network: 0.00}/{rows: 22 (1.69kB), cpu: 4414.00, memory: 0.00, network: 0.00}/{rows: 22 (1.69kB), cpu: 6148.25, memory: 0.00, network: 0.00}
                nationkey := tpch:nationkey
                comment := tpch:comment
```


---

### Cost-based Optimizations Documentation
Source: https://trino.io/docs/current/optimizer/cost-based-optimizations.html

Cost-based optimizations# Trino supports several cost based optimizations, described below. Join enumeration# The order in which joins are executed in a query can have a significant impact on the query’s performance. The aspect of join ordering that has the largest impact on performance is the size of the data being processed and transferred over the network. If a join which produces a lot of data is performed early in the query’s execution, then subsequent stages need to process large amounts of data for longer than necessary, increasing the time and resources needed for processing the query. With cost-based join enumeration, Trino uses Table statistics provided by connectors to estimate the costs for different join orders and automatically picks the join order with the lowest computed costs. The join enumeration strategy is governed by the join_reordering_strategy session property, with the optimizer.join-reordering-strategy configuration property providing the default value. The possible values are: AUTOMATIC (default) - enable full automatic join enumeration ELIMINATE_CROSS_JOINS - eliminate unnecessary cross joins NONE - purely syntactic join order If you are using AUTOMATIC join enumeration and statistics are not available or a cost can not be computed for any other reason, the ELIMINATE_CROSS_JOINS strategy is used instead. Join distribution selection# Trino uses a hash-based join algorithm. For each join operator, a hash table must be created from one join input, referred to as the build side. The other input, called the probe side, is then iterated on. For each row, the hash table is queried to find matching rows. There are two types of join distributions: Partitioned: each node participating in the query builds a hash table from only a fraction of the data Broadcast: each node participating in the query builds a hash table from all of the data. The data is replicated to each node. Each type has advantages and disadvantages. Partitioned joins require redistributing both tables using a hash of the join key. These joins can be much slower than broadcast joins, but they allow much larger joins overall. Broadcast joins are faster if the build side is much smaller than the probe side. However, broadcast joins require that the tables on the build side of the join after filtering fit in memory on each node, whereas distributed joins only need to fit in distributed memory across all nodes. With cost-based join distribution selection, Trino automatically chooses whether to use a partitioned or broadcast join. With cost-based join enumeration, Trino automatically chooses which sides are probe and build. The join distribution strategy is governed by the join_distribution_type session property, with the join-distribution-type configuration property providing the default value. The valid values are: AUTOMATIC (default) - join distribution type is determined automatically for each join BROADCAST - broadcast join distribution is used for all joins PARTITIONED - partitioned join distribution is used for all join Capping replicated table size# The join distribution type is automatically chosen when the join reordering strategy is set to AUTOMATIC or when the join distribution type is set to AUTOMATIC. In both cases, it is possible to cap the maximum size of the replicated table with the join-max-broadcast-table-size configuration property or with the join_max_broadcast_table_size session property. This allows you to improve cluster concurrency and prevent bad plans when the cost-based optimizer misestimates the size of the joined tables. By default, the replicated table size is capped to 100MB. Syntactic join order# If not using cost-based optimization, Trino defaults to syntactic join ordering. While there is no formal way to optimize queries for this case, it is possible to take advantage of how Trino implements joins to make them more performant. Trino uses in-memory hash joins. When processing a join statement, Trino loads the right-most table of the join into memory as the build side, then streams the next right-most table as the probe side to execute the join. If a query has multiple joins, the result of this first join stays in memory as the build side, and the third right-most table is then used as the probe side, and so on for additional joins. In the case where join order is made more complex, such as when using parentheses to specify specific parents for joins, Trino may execute multiple lower-level joins at once, but each step of that process follows the same logic, and the same applies when the results are ultimately joined together. Because of this behavior, it is optimal to syntactically order joins in your SQL queries from the largest tables to the smallest, as this minimizes memory usage. As an example, if you have a small, medium, and large table and are using left joins: SELECT * FROM large_table l LEFT JOIN medium_table m ON l.user_id = m.user_id LEFT JOIN small_table s ON s.user_id = l.user_id Warning This means of optimization is not a feature of Trino. It is an artifact of how joins are implemented, and therefore this behavior may change without notice. Connector implementations# In order for the Trino optimizer to use the cost based strategies, the connector implementation must provide Table statistics.

#### Code Examples

```
SELECT
  *
FROM
  large_table l
  LEFT JOIN medium_table m ON l.user_id = m.user_id
  LEFT JOIN small_table s ON s.user_id = l.user_id
```


---

### Pushdown Documentation
Source: https://trino.io/docs/current/optimizer/pushdown.html

Pushdown# Trino can push down the processing of queries, or parts of queries, into the connected data source. This means that a specific predicate, aggregation function, or other operation, is passed through to the underlying database or storage system for processing. The results of this pushdown can include the following benefits: Improved overall query performance Reduced network traffic between Trino and the data source Reduced load on the remote data source These benefits often result in significant cost reduction. Support for pushdown is specific to each connector and the relevant underlying database or storage system. Predicate pushdown# Predicate pushdown optimizes row-based filtering. It uses the inferred filter, typically resulting from a condition in a WHERE clause to omit unnecessary rows. The processing is pushed down to the data source by the connector and then processed by the data source. If predicate pushdown for a specific clause is successful, the EXPLAIN plan for the query does not include a ScanFilterProject operation for that clause. Projection pushdown# Projection pushdown optimizes column-based filtering. It uses the columns specified in the SELECT clause and other parts of the query to limit access to these columns. The processing is pushed down to the data source by the connector and then the data source only reads and returns the necessary columns. If projection pushdown is successful, the EXPLAIN plan for the query only accesses the relevant columns in the Layout of the TableScan operation. Dereference pushdown# Projection pushdown and dereference pushdown limit access to relevant columns, except dereference pushdown is more selective. It limits access to only read the specified fields within a top level or nested ROW data type. For example, consider a table in the Hive connector that has a ROW type column with several fields. If a query only accesses one field, dereference pushdown allows the file reader to read only that single field within the row. The same applies to fields of a row nested within the top level row. This can result in significant savings in the amount of data read from the storage system. Aggregation pushdown# Aggregation pushdown can take place provided the following conditions are satisfied: If aggregation pushdown is generally supported by the connector. If pushdown of the specific function or functions is supported by the connector. If the query structure allows pushdown to take place. You can check if pushdown for a specific query is performed by looking at the EXPLAIN plan of the query. If an aggregate function is successfully pushed down to the connector, the explain plan does not show that Aggregate operator. The explain plan only shows the operations that are performed by Trino. As an example, we loaded the TPC-H data set into a PostgreSQL database and then queried it using the PostgreSQL connector: SELECT regionkey, count(*) FROM nation GROUP BY regionkey; You can get the explain plan by prepending the above query with EXPLAIN: EXPLAIN SELECT regionkey, count(*) FROM nation GROUP BY regionkey; The explain plan for this query does not show any Aggregate operator with the count function, as this operation is now performed by the connector. You can see the count(*) function as part of the PostgreSQL TableScan operator. This shows you that the pushdown was successful. Fragment 0 [SINGLE] Output layout: [regionkey_0, _generated_1] Output partitioning: SINGLE [] Output[regionkey, _col1] │ Layout: [regionkey_0:bigint, _generated_1:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: ?} │ regionkey := regionkey_0 │ _col1 := _generated_1 └─ RemoteSource[1] Layout: [regionkey_0:bigint, _generated_1:bigint] Fragment 1 [SOURCE] Output layout: [regionkey_0, _generated_1] Output partitioning: SINGLE [] TableScan[postgresql:tpch.nation tpch.nation columns=[regionkey:bigint:int8, count(*):_generated_1:bigint:bigint] groupingSets=[[regionkey:bigint:int8]], gro Layout: [regionkey_0:bigint, _generated_1:bigint] Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B} _generated_1 := count(*):_generated_1:bigint:bigint regionkey_0 := regionkey:bigint:int8 A number of factors can prevent a push down: adding a condition to the query using a different aggregate function that cannot be pushed down into the connector using a connector without pushdown support for the specific function As a result, the explain plan shows the Aggregate operation being performed by Trino. This is a clear sign that now pushdown to the remote data source is not performed, and instead Trino performs the aggregate processing. Fragment 0 [SINGLE] Output layout: [regionkey, count] Output partitioning: SINGLE [] Output[regionkey, _col1] │ Layout: [regionkey:bigint, count:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} │ _col1 := count └─ RemoteSource[1] Layout: [regionkey:bigint, count:bigint] Fragment 1 [HASH] Output layout: [regionkey, count] Output partitioning: SINGLE [] Aggregate(FINAL)[regionkey] │ Layout: [regionkey:bigint, count:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} │ count := count("count_0") └─ LocalExchange[HASH][$hashvalue] ("regionkey") │ Layout: [regionkey:bigint, count_0:bigint, $hashvalue:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} └─ RemoteSource[2] Layout: [regionkey:bigint, count_0:bigint, $hashvalue_1:bigint] Fragment 2 [SOURCE] Output layout: [regionkey, count_0, $hashvalue_2] Output partitioning: HASH [regionkey][$hashvalue_2] Project[] │ Layout: [regionkey:bigint, count_0:bigint, $hashvalue_2:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} │ $hashvalue_2 := combine_hash(bigint '0', COALESCE("$operator$hash_code"("regionkey"), 0)) └─ Aggregate(PARTIAL)[regionkey] │ Layout: [regionkey:bigint, count_0:bigint] │ count_0 := count(*) └─ TableScan[tpch:nation:sf0.01, grouped = false] Layout: [regionkey:bigint] Estimates: {rows: 25 (225B), cpu: 225, memory: 0B, network: 0B} regionkey := tpch:regionkey Limitations# Aggregation pushdown does not support a number of more complex statements: complex grouping operations such as ROLLUP, CUBE, or GROUPING SETS expressions inside the aggregation function call: sum(a * b) coercions: sum(integer_column) aggregations with ordering aggregations with filter Join pushdown# Join pushdown allows the connector to delegate the table join operation to the underlying data source. This can result in performance gains, and allows Trino to perform the remaining query processing on a smaller amount of data. The specifics for the supported pushdown of table joins varies for each data source, and therefore for each connector. However, there are some generic conditions that must be met in order for a join to be pushed down: all predicates that are part of the join must be possible to be pushed down the tables in the join must be from the same catalog You can verify if pushdown for a specific join is performed by looking at the EXPLAIN plan of the query. The explain plan does not show a Join operator, if the join is pushed down to the data source by the connector: EXPLAIN SELECT c.custkey, o.orderkey FROM orders o JOIN customer c ON c.custkey = o.custkey; The following plan results from the PostgreSQL connector querying TPC-H data in a PostgreSQL database. It does not show any Join operator as a result of the successful join push down. Fragment 0 [SINGLE] Output layout: [custkey, orderkey] Output partitioning: SINGLE [] Output[custkey, orderkey] │ Layout: [custkey:bigint, orderkey:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: ?} └─ RemoteSource[1] Layout: [orderkey:bigint, custkey:bigint] Fragment 1 [SOURCE] Output layout: [orderkey, custkey] Output partitioning: SINGLE [] TableScan[postgres:Query[SELECT l."orderkey" AS "orderkey_0", l."custkey" AS "custkey_1", r."custkey" AS "custkey_2" FROM (SELECT "orderkey", "custkey" FROM "tpch"."orders") l INNER JOIN (SELECT "custkey" FROM "tpch"."customer") r O Layout: [orderkey:bigint, custkey:bigint] Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B} orderkey := orderkey_0:bigint:int8 custkey := custkey_1:bigint:int8 It is typically beneficial to push down a join. Pushing down a join can also increase the row count compared to the size of the input to the join. This may impact performance. Limit pushdown# A LIMIT or FETCH FIRST clause reduces the number of returned records for a statement. Limit pushdown enables a connector to push processing of such queries of unsorted record to the underlying data source. A pushdown of this clause can improve the performance of the query and significantly reduce the amount of data transferred from the data source to Trino. Queries include sections such as LIMIT N or FETCH FIRST N ROWS. Implementation and support is connector-specific since different data sources have varying capabilities. Top-N pushdown# The combination of a LIMIT or FETCH FIRST clause with an ORDER BY clause creates a small set of records to return out of a large sorted dataset. It relies on the order to determine which records need to be returned, and is therefore quite different to optimize compared to a Limit pushdown. The pushdown for such a query is called a Top-N pushdown, since the operation is returning the top N rows. It enables a connector to push processing of such queries to the underlying data source, and therefore significantly reduces the amount of data transferred to and processed by Trino. Queries include sections such as ORDER BY ... LIMIT N or ORDER BY ... FETCH FIRST N ROWS. Implementation and support is connector-specific since different data sources support different SQL syntax and processing. For example, you can find two queries to learn how to identify Top-N pushdown behavior in the following section. First, a concrete example of a Top-N pushdown query on top of a PostgreSQL database: SELECT id, name FROM postgresql.public.company ORDER BY id LIMIT 5; You can get the explain plan by prepending the above query with EXPLAIN: EXPLAIN SELECT id, name FROM postgresql.public.company ORDER BY id LIMIT 5; Fragment 0 [SINGLE] Output layout: [id, name] Output partitioning: SINGLE [] Stage Execution Strategy: UNGROUPED_EXECUTION Output[id, name] │ Layout: [id:integer, name:varchar] │ Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: ?} └─ RemoteSource[1] Layout: [id:integer, name:varchar] Fragment 1 [SOURCE] Output layout: [id, name] Output partitioning: SINGLE [] Stage Execution Strategy: UNGROUPED_EXECUTION TableScan[postgresql:public.company public.company sortOrder=[id:integer:int4 ASC NULLS LAST] limit=5, grouped = false] Layout: [id:integer, name:varchar] Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B} name := name:varchar:text id := id:integer:int4 Second, an example of a Top-N query on the tpch connector which does not support Top-N pushdown functionality: SELECT custkey, name FROM tpch.sf1.customer ORDER BY custkey LIMIT 5; The related query plan: Fragment 0 [SINGLE] Output layout: [custkey, name] Output partitioning: SINGLE [] Stage Execution Strategy: UNGROUPED_EXECUTION Output[custkey, name] │ Layout: [custkey:bigint, name:varchar(25)] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} └─ TopN[5 by (custkey ASC NULLS LAST)] │ Layout: [custkey:bigint, name:varchar(25)] └─ LocalExchange[SINGLE] () │ Layout: [custkey:bigint, name:varchar(25)] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} └─ RemoteSource[1] Layout: [custkey:bigint, name:varchar(25)] Fragment 1 [SOURCE] Output layout: [custkey, name] Output partitioning: SINGLE [] Stage Execution Strategy: UNGROUPED_EXECUTION TopNPartial[5 by (custkey ASC NULLS LAST)] │ Layout: [custkey:bigint, name:varchar(25)] └─ TableScan[tpch:customer:sf1.0, grouped = false] Layout: [custkey:bigint, name:varchar(25)] Estimates: {rows: 150000 (4.58MB), cpu: 4.58M, memory: 0B, network: 0B} custkey := tpch:custkey name := tpch:name In the preceding query plan, the Top-N operation TopN[5 by (custkey ASC NULLS LAST)] is being applied in the Fragment 0 by Trino and not by the source database. Note that, compared to the query executed on top of the tpch connector, the explain plan of the query applied on top of the postgresql connector is missing the reference to the operation TopN[5 by (id ASC NULLS LAST)] in the Fragment 0. The absence of the TopN Trino operator in the Fragment 0 from the query plan demonstrates that the query benefits of the Top-N pushdown optimization.

#### Code Examples

```
SELECT regionkey, count(*)
FROM nation
GROUP BY regionkey;
```
```
EXPLAIN
SELECT regionkey, count(*)
FROM nation
GROUP BY regionkey;
```
```
Fragment 0 [SINGLE]
    Output layout: [regionkey_0, _generated_1]
    Output partitioning: SINGLE []
    Output[regionkey, _col1]
    │   Layout: [regionkey_0:bigint, _generated_1:bigint]
    │   Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: ?}
    │   regionkey := regionkey_0
    │   _col1 := _generated_1
    └─ RemoteSource[1]
            Layout: [regionkey_0:bigint, _generated_1:bigint]

Fragment 1 [SOURCE]
    Output layout: [regionkey_0, _generated_1]
    Output partitioning: SINGLE []
    TableScan[postgresql:tpch.nation tpch.nation columns=[regionkey:bigint:int8, count(*):_generated_1:bigint:bigint] groupingSets=[[regionkey:bigint:int8]], gro
        Layout: [regionkey_0:bigint, _generated_1:bigint]
        Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B}
        _generated_1 := count(*):_generated_1:bigint:bigint
        regionkey_0 := regionkey:bigint:int8
```
```
Fragment 0 [SINGLE]
    Output layout: [regionkey, count]
    Output partitioning: SINGLE []
    Output[regionkey, _col1]
    │   Layout: [regionkey:bigint, count:bigint]
    │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
    │   _col1 := count
    └─ RemoteSource[1]
           Layout: [regionkey:bigint, count:bigint]

Fragment 1 [HASH]
    Output layout: [regionkey, count]
    Output partitioning: SINGLE []
    Aggregate(FINAL)[regionkey]
    │   Layout: [regionkey:bigint, count:bigint]
    │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
    │   count := count("count_0")
    └─ LocalExchange[HASH][$hashvalue] ("regionkey")
       │   Layout: [regionkey:bigint, count_0:bigint, $hashvalue:bigint]
       │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
       └─ RemoteSource[2]
              Layout: [regionkey:bigint, count_0:bigint, $hashvalue_1:bigint]

Fragment 2 [SOURCE]
    Output layout: [regionkey, count_0, $hashvalue_2]
    Output partitioning: HASH [regionkey][$hashvalue_2]
    Project[]
    │   Layout: [regionkey:bigint, count_0:bigint, $hashvalue_2:bigint]
    │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
    │   $hashvalue_2 := combine_hash(bigint '0', COALESCE("$operator$hash_code"("regionkey"), 0))
    └─ Aggregate(PARTIAL)[regionkey]
       │   Layout: [regionkey:bigint, count_0:bigint]
       │   count_0 := count(*)
       └─ TableScan[tpch:nation:sf0.01, grouped = false]
              Layout: [regionkey:bigint]
              Estimates: {rows: 25 (225B), cpu: 225, memory: 0B, network: 0B}
              regionkey := tpch:regionkey
```
```
EXPLAIN SELECT c.custkey, o.orderkey
FROM orders o JOIN customer c ON c.custkey = o.custkey;
```
```
Fragment 0 [SINGLE]
    Output layout: [custkey, orderkey]
    Output partitioning: SINGLE []
    Output[custkey, orderkey]
    │   Layout: [custkey:bigint, orderkey:bigint]
    │   Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: ?}
    └─ RemoteSource[1]
           Layout: [orderkey:bigint, custkey:bigint]

Fragment 1 [SOURCE]
    Output layout: [orderkey, custkey]
    Output partitioning: SINGLE []
    TableScan[postgres:Query[SELECT l."orderkey" AS "orderkey_0", l."custkey" AS "custkey_1", r."custkey" AS "custkey_2" FROM (SELECT "orderkey", "custkey" FROM "tpch"."orders") l INNER JOIN (SELECT "custkey" FROM "tpch"."customer") r O
        Layout: [orderkey:bigint, custkey:bigint]
        Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B}
        orderkey := orderkey_0:bigint:int8
        custkey := custkey_1:bigint:int8
```
```
SELECT id, name
FROM postgresql.public.company
ORDER BY id
LIMIT 5;
```
```
EXPLAIN SELECT id, name
FROM postgresql.public.company
ORDER BY id
LIMIT 5;
```
```
Fragment 0 [SINGLE]
    Output layout: [id, name]
    Output partitioning: SINGLE []
    Stage Execution Strategy: UNGROUPED_EXECUTION
    Output[id, name]
    │   Layout: [id:integer, name:varchar]
    │   Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: ?}
    └─ RemoteSource[1]
           Layout: [id:integer, name:varchar]

Fragment 1 [SOURCE]
    Output layout: [id, name]
    Output partitioning: SINGLE []
    Stage Execution Strategy: UNGROUPED_EXECUTION
    TableScan[postgresql:public.company public.company sortOrder=[id:integer:int4 ASC NULLS LAST] limit=5, grouped = false]
        Layout: [id:integer, name:varchar]
        Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B}
        name := name:varchar:text
        id := id:integer:int4
```
```
SELECT custkey, name
FROM tpch.sf1.customer
ORDER BY custkey
LIMIT 5;
```
```
Fragment 0 [SINGLE]
    Output layout: [custkey, name]
    Output partitioning: SINGLE []
    Stage Execution Strategy: UNGROUPED_EXECUTION
    Output[custkey, name]
    │   Layout: [custkey:bigint, name:varchar(25)]
    │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
    └─ TopN[5 by (custkey ASC NULLS LAST)]
       │   Layout: [custkey:bigint, name:varchar(25)]
       └─ LocalExchange[SINGLE] ()
          │   Layout: [custkey:bigint, name:varchar(25)]
          │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
          └─ RemoteSource[1]
                 Layout: [custkey:bigint, name:varchar(25)]

Fragment 1 [SOURCE]
    Output layout: [custkey, name]
    Output partitioning: SINGLE []
    Stage Execution Strategy: UNGROUPED_EXECUTION
    TopNPartial[5 by (custkey ASC NULLS LAST)]
    │   Layout: [custkey:bigint, name:varchar(25)]
    └─ TableScan[tpch:customer:sf1.0, grouped = false]
           Layout: [custkey:bigint, name:varchar(25)]
           Estimates: {rows: 150000 (4.58MB), cpu: 4.58M, memory: 0B, network: 0B}
           custkey := tpch:custkey
           name := tpch:name
```


---

### Adaptive Plan Optimizations Documentation
Source: https://trino.io/docs/current/optimizer/adaptive-plan-optimizations.html

Adaptive plan optimizations# Trino offers several adaptive plan optimizations that adjust query execution plans dynamically based on runtime statistics. These optimizations are only available when Fault-tolerant execution is enabled. To deactivate all adaptive plan optimizations, set the fault-tolerant-execution-adaptive-query-planning-enabled configuration property to false. The equivalent session property is fault_tolerant_execution_adaptive_query_planning_enabled. Adaptive reordering of partitioned joins# By default, Trino enables adaptive reordering of partitioned joins. This optimization allows Trino to dynamically reorder the join inputs, based on the actual size of the build and probe sides during query execution. This is particularly useful when table statistics are not available beforehand, as it can improve query performance by making more efficient join order decisions based on runtime information. To deactivate this optimization, set the fault-tolerant-execution-adaptive-join-reordering-enabled configuration property to false. The equivalent session property is fault_tolerant_execution_adaptive_join_reordering_enabled.

---


## User Defined Functions

### Introduction to UDFs Documentation
Source: https://trino.io/docs/current/udf/introduction.html

Introduction to UDFs# A user-defined function (UDF) is a custom function authored by a user of Trino in a client application. UDFs are scalar functions that return a single output value, similar to built-in functions. Note Custom functions can alternatively be written in Java and deployed as a plugin. Details are available in the developer guide. UDF declaration# Declare the UDF with the SQL FUNCTION keyword and the supported statements for SQL user-defined functions or Python user-defined functions. A UDF can be declared as an inline UDF to be used in the current query, or declared as a catalog UDF to be used in any future query. Inline user-defined functions# An inline user-defined function (inline UDF) declares and uses the UDF within a query processing context. The UDF is declared in a WITH block before the query: WITH FUNCTION doubleup(x integer) RETURNS integer RETURN x * 2 SELECT doubleup(21); -- 42 Inline UDF names must follow SQL identifier naming conventions, and cannot contain . characters. The UDF declaration is only valid within the context of the query. A separate later invocation of the UDF is not possible. If this is desired, use a catalog UDF. Multiple inline UDF declarations are comma-separated, and can include UDFs calling each other, as long as a called UDF is declared before the first invocation. WITH FUNCTION doubleup(x integer) RETURNS integer RETURN x * 2, FUNCTION doubleupplusone(x integer) RETURNS integer RETURN doubleup(x) + 1 SELECT doubleupplusone(21); -- 43 Note that inline UDFs can mask and override the meaning of a built-in function: WITH FUNCTION abs(x integer) RETURNS integer RETURN x * 2 SELECT abs(-10); -- -20, not 10! Catalog user-defined functions# You can store a UDF in the context of a catalog, if the connector used in the catalog supports UDF storage. The following connectors support catalog UDF storage: Hive connector Memory connector In this scenario, the following commands can be used: CREATE FUNCTION to create and store a UDF. DROP FUNCTION to remove a UDF. SHOW FUNCTIONS to display a list of UDFs in a catalog. Catalog UDFs must use a name that combines the catalog name and schema name with the UDF name, such as example.default.power for the power UDF in the default schema of the example catalog. Invocation must use the fully qualified name, such as example.default.power. SQL environment configuration for UDFs# Configuration of the sql.default-function-catalog and sql.default-function-schema SQL environment properties allows you to set the default storage for UDFs. The catalog and schema must be added to the sql.path as well. This enables users to call UDFs and perform all User-defined function management without specifying the full path to the UDF. Note Use the Memory connector in a catalog for simple storing and testing of your UDFs. Recommendations# Processing UDFs can potentially be resource intensive on the cluster in terms of memory and processing. Take the following considerations into account when writing and running UDFs: Some checks for the runtime behavior of queries, and therefore UDF processing, are in place. For example, if a query takes longer to process than a hardcoded threshold, processing is automatically terminated. Avoid creation of arrays in a looping construct. Each iteration creates a separate new array with all items and copies the data for each modification, leaving the prior array in memory for automated clean up later. Use a lambda expression instead of the loop. Avoid concatenating strings in a looping construct. Each iteration creates a separate new string and copying the old string for each modification, leaving the prior string in memory for automated clean up later. Use a lambda expression instead of the loop. Most UDFs should declare the RETURNS NULL ON NULL INPUT characteristics unless the code has some special handling for null values. You must declare this explicitly since CALLED ON NULL INPUT is the default characteristic.

#### Code Examples

```
WITH
  FUNCTION doubleup(x integer)
    RETURNS integer
    RETURN x * 2
SELECT doubleup(21);
-- 42
```
```
WITH
  FUNCTION doubleup(x integer)
    RETURNS integer
    RETURN x * 2,
  FUNCTION doubleupplusone(x integer)
    RETURNS integer
    RETURN doubleup(x) + 1
SELECT doubleupplusone(21);
-- 43
```
```
WITH
  FUNCTION abs(x integer)
    RETURNS integer
    RETURN x * 2
SELECT abs(-10); -- -20, not 10!
```


---

### FUNCTION Documentation
Source: https://trino.io/docs/current/udf/function.html

FUNCTION# Synopsis# FUNCTION name ( [ parameter_name data_type [, ...] ] ) RETURNS type [ LANGUAGE language] [ NOT? DETERMINISTIC ] [ RETURNS NULL ON NULL INPUT ] [ CALLED ON NULL INPUT ] [ SECURITY { DEFINER | INVOKER } ] [ COMMENT description] [ WITH ( property_name = expression [, ...] ) ] { statements | AS definition } Description# Declare a user-defined function. The name of the UDF. Inline user-defined functions can use a simple string. Catalog user-defined functions must qualify the name of the catalog and schema, delimited by ., to store the UDF or rely on the default catalog and schema for UDF storage. The list of parameters is a comma-separated list of names parameter_name and data types data_type, see data type. An empty list, specified as () is also valid. The type value after the RETURNS keyword identifies the data type of the UDF output. The optional LANGUAGE characteristic identifies the language used for the UDF definition with language. The SQL and PYTHON languages are supported by default. Additional languages may be supported via a language engine plugin. If not specified, the default language is SQL. The optional DETERMINISTIC or NOT DETERMINISTIC characteristic declares that the UDF is deterministic. This means that repeated UDF calls with identical input parameters yield the same result. A UDF is non-deterministic if it calls any non-deterministic UDFs and functions. By default, UDFs are assumed to have a deterministic behavior. The optional RETURNS NULL ON NULL INPUT characteristic declares that the UDF returns a NULL value when any of the input parameters are NULL. The UDF is not invoked with a NULL input value. The CALLED ON NULL INPUT characteristic declares that the UDF is invoked with NULL input parameter values. The RETURNS NULL ON NULL INPUT and CALLED ON NULL INPUT characteristics are mutually exclusive, with CALLED ON NULL INPUT as the default. The security declaration of SECURITY INVOKER or SECURITY DEFINER is only valid for catalog UDFs. It sets the mode for processing the UDF with the permissions of the user who calls the UDF (INVOKER) or the user who created the UDF (DEFINER). The COMMENT characteristic can be used to provide information about the function to other users as description. The information is accessible with SHOW FUNCTIONS. The optional WITH clause can be used to specify properties for the function. The available properties vary based on the function language. For Python user-defined functions, the handler property specifies the name of the Python function to invoke. For SQL UDFs the body of the UDF can either be a simple single RETURN statement with an expression, or compound list of statements in a BEGIN block. UDF must contain a RETURN statement at the end of the top-level block, even if it’s unreachable. For UDFs in other languages, the definition is enclosed in a $$-quoted string. Examples# A simple catalog function: CREATE FUNCTION example.default.meaning_of_life() RETURNS BIGINT RETURN 42; And used: SELECT example.default.meaning_of_life(); -- returns 42 Equivalent usage with an inline function: WITH FUNCTION meaning_of_life() RETURNS BIGINT RETURN 42 SELECT meaning_of_life(); Further examples of varying complexity that cover usage of the FUNCTION statement in combination with other statements are available in the SQL UDF documentation and the Python UDF documentation. See also# User-defined functions SQL user-defined functions Python user-defined functions CREATE FUNCTION

#### Code Examples

```
FUNCTION name ( [ parameter_name data_type [, ...] ] )
  RETURNS type
  [ LANGUAGE language]
  [ NOT? DETERMINISTIC ]
  [ RETURNS NULL ON NULL INPUT ]
  [ CALLED ON NULL INPUT ]
  [ SECURITY { DEFINER | INVOKER } ]
  [ COMMENT description]
  [ WITH ( property_name = expression [, ...] ) ]
  { statements | AS definition }
```
```
CREATE FUNCTION example.default.meaning_of_life()
  RETURNS BIGINT
  RETURN 42;
```
```
SELECT example.default.meaning_of_life(); -- returns 42
```
```
WITH FUNCTION meaning_of_life()
  RETURNS BIGINT
  RETURN 42
SELECT meaning_of_life();
```


---

### SQL User-defined Functions Documentation
Source: https://trino.io/docs/current/udf/sql.html

SQL user-defined functions# A SQL user-defined function, also known as SQL routine, is a user-defined function that uses the SQL routine language and statements for the definition of the function. SQL UDF declaration# Declare a SQL UDF using the FUNCTION keyword and the following statements can be used in addition to built-in functions and operators and other UDFs: BEGIN CASE DECLARE IF ITERATE LEAVE LOOP REPEAT RETURN SET WHILE A minimal example declares the UDF doubleup that returns the input integer value x multiplied by two. The example shows declaration as Inline user-defined functions and invocation with the value 21 to yield the result 42: WITH FUNCTION doubleup(x integer) RETURNS integer RETURN x * 2 SELECT doubleup(21); -- 42 The same UDF can also be declared as Catalog user-defined functions. Find simple examples in each statement documentation, and refer to the Example SQL UDFs for more complex use cases that combine multiple statements. Labels# SQL UDFs can contain labels as markers for a specific block in the declaration before the following keywords: CASE IF LOOP REPEAT WHILE The label is used to name the block to continue processing with the ITERATE statement or exit the block with the LEAVE statement. This flow control is supported for nested blocks, allowing to continue or exit an outer block, not just the innermost block. For example, the following snippet uses the label top to name the complete block from REPEAT to END REPEAT: top: REPEAT SET a = a + 1; IF a <= 3 THEN ITERATE top; END IF; SET b = b + 1; UNTIL a >= 10 END REPEAT; Labels can be used with the ITERATE and LEAVE statements to continue processing the block or leave the block. This flow control is also supported for nested blocks and labels. Limitations# The following limitations apply to SQL UDFs. UDFs must be declared before they are referenced. Recursion cannot be declared or processed. Mutual recursion can not be declared or processed. Queries cannot be processed in a UDF. Specifically this means that UDFs can not use SELECT queries to retrieve data or any other queries to process data within the UDF. Instead queries can use UDFs to process data. UDFs only work on data provided as input values and only provide output data from the RETURN statement.

#### Code Examples

```
WITH
  FUNCTION doubleup(x integer)
    RETURNS integer
    RETURN x * 2
SELECT doubleup(21);
-- 42
```
```
top: REPEAT
  SET a = a + 1;
  IF a <= 3 THEN
    ITERATE top;
  END IF;
  SET b = b + 1;
  UNTIL a >= 10
END REPEAT;
```


---

### Python User-defined Functions Documentation
Source: https://trino.io/docs/current/udf/python.html

Python user-defined functions# A Python user-defined function is a user-defined function that uses the Python programming language and statements for the definition of the function. Python UDF declaration# Declare a Python UDF as inline or catalog UDF with the following steps: Use the FUNCTION keyword to declare the UDF name and parameters. Add the RETURNS declaration to specify the data type of the result. Set the LANGUAGE to PYTHON. Declare the name of the Python function to call with the handler property in the WITH block. Use $$ to enclose the Python code after the AS keyword. Add the function from the handler property and ensure it returns the declared data type. Expand your Python code section to implement the function using the available Python language. The following snippet shows pseudo-code: FUNCTION python_udf_name(input_parameter data_type) RETURNS result_data_type LANGUAGE PYTHON WITH (handler = 'python_function') AS $$ ... def python_function(input): return ... ... $$ A minimal example declares the UDF doubleup that returns the input integer value x multiplied by two. The example shows declaration as Inline user-defined functions and invocation with the value 21 to yield the result 42. Set the language to PYTHON to override the default SQL for SQL user-defined functions. The Python code is enclosed with $$ and must use valid formatting. WITH FUNCTION doubleup(x integer) RETURNS integer LANGUAGE PYTHON WITH (handler = 'twice') AS $$ def twice(a): return a * 2 $$ SELECT doubleup(21); -- 42 The same UDF can also be declared as Catalog user-defined functions. Refer to the Example Python UDFs for more complex use cases and examples. Python language details# The Trino Python UDF integrations uses Python 3.13.0 in a sandboxed environment. Python code runs within a WebAssembly (WASM) runtime within the Java virtual machine running Trino. Python language rules including indents must be observed. Python UDFs therefore only have access to the Python language and core libraries included in the sandboxed runtime. Access to external resources with network or file system operations is not supported. Usage of other Python libraries as well as command line tools or package managers is not supported. The following libraries are explicitly removed from the runtime and therefore not available within a Python UDF: bdb concurrent curses ensurepip doctest idlelib multiprocessing pdb pydoc socketserver sqlite3 ssl subprocess tkinter turtle unittest venv webbrowser wsgiref xmlrpc The following libraries are explicitly added to the runtime and therefore available within a Python UDF: attrs bleach charset-normalizer defusedxml idna jmespath jsonschema pyasn1 pyparsing python-dateutil rsa tomli ua-parser Type mapping# The following table shows supported Trino types and their corresponding Python types for input and output values of a Python UDF: Trino type Python type ROW tuple ARRAY list MAP dict BOOLEAN bool TINYINT int SMALLINT int INTEGER int BIGINT int REAL float DOUBLE float DECIMAL decimal.Decimal VARCHAR str VARBINARY bytes DATE datetime.date TIME datetime.time TIME WITH TIME ZONE datetime.time with datetime.tzinfo TIMESTAMP datetime.datetime TIMESTAMP WITH TIME ZONE datetime.datetime with datetime.tzinfo INTERVAL YEAR TO MONTH int as the number of months INTERVAL DAY TO SECOND datetime.timedelta JSON str UUID uuid.UUID IPADDRESS ipaddress.IPv4Address or ipaddress.IPv6Address Time and timestamp# Python datetime and time objects only support microsecond precision. Trino argument values with greater precision are rounded when converted to Python values, and Python return values are rounded if the Trino return type has less than microsecond precision. Timestamp with time zone# Only fixed offset time zones are supported. Timestamps with political time zones have the zone converted to the zone’s offset for the timestamp’s instant.

#### Code Examples

```
FUNCTION python_udf_name(input_parameter data_type)
  RETURNS result_data_type
  LANGUAGE PYTHON
  WITH (handler = 'python_function')
  AS $$
  ...
  def python_function(input):
      return ...
  ...
  $$
```
```
WITH
  FUNCTION doubleup(x integer)
    RETURNS integer
    LANGUAGE PYTHON
    WITH (handler = 'twice')
    AS $$
    def twice(a):
        return a * 2
    $$
SELECT doubleup(21);
-- 42
```


---


## SQL Language

### SQL Statement Support Documentation
Source: https://trino.io/docs/current/language/sql-support.html

SQL statement support# The SQL statement support in Trino can be categorized into several topics. Many statements are part of the core engine and therefore available in all use cases. For example, you can always set session properties or inspect an explain plan and perform other actions with the globally available statements. However, the details and architecture of the connected data sources can limit some SQL functionality. For example, if the data source does not support any write operations, then a DELETE statement cannot be executed against the data source. Similarly, if the underlying system does not have any security concepts, SQL statements like CREATE ROLE cannot be supported by Trino and the connector. The categories of these different topics are related to read operations, write operations, security operations and transactions. Details of the support for specific statements is available with the documentation for each connector. Globally available statements# The following statements are implemented in the core engine and available with any connector: CALL DEALLOCATE PREPARE DESCRIBE INPUT DESCRIBE OUTPUT EXECUTE EXECUTE IMMEDIATE EXPLAIN EXPLAIN ANALYZE PREPARE RESET SESSION SET SESSION SET TIME ZONE SHOW FUNCTIONS SHOW SESSION USE VALUES Catalog management# The following statements are used to manage dynamic catalogs: CREATE CATALOG DROP CATALOG Read operations# The following statements provide read access to data and meta data exposed by a connector accessing a data source. They are supported by all connectors: SELECT including MATCH_RECOGNIZE DESCRIBE SHOW CATALOGS SHOW COLUMNS SHOW CREATE MATERIALIZED VIEW SHOW CREATE SCHEMA SHOW CREATE TABLE SHOW CREATE VIEW SHOW GRANTS SHOW ROLES SHOW SCHEMAS SHOW TABLES SHOW STATS Write operations# The following statements provide write access to data and meta data exposed by a connector accessing a data source. Availability varies widely from connector to connector: Data management# INSERT UPDATE DELETE TRUNCATE MERGE Schema and table management# CREATE TABLE CREATE TABLE AS DROP TABLE ALTER TABLE CREATE SCHEMA DROP SCHEMA ALTER SCHEMA COMMENT View management# CREATE VIEW DROP VIEW ALTER VIEW Materialized view management# CREATE MATERIALIZED VIEW ALTER MATERIALIZED VIEW DROP MATERIALIZED VIEW REFRESH MATERIALIZED VIEW User-defined function management# The following statements are used to manage Catalog user-defined functions: CREATE FUNCTION DROP FUNCTION SHOW FUNCTIONS Security operations# The following statements provide security-related operations to security configuration, data, and meta data exposed by a connector accessing a data source. Most connectors do not support these operations: Connector roles: CREATE ROLE DROP ROLE GRANT role REVOKE role SET ROLE SHOW ROLE GRANTS Grants management: DENY GRANT privilege REVOKE privilege Transactions# The following statements manage transactions. Most connectors do not support transactions: START TRANSACTION COMMIT ROLLBACK

---

### Data Types Documentation
Source: https://trino.io/docs/current/language/types.html

Data types# Trino has a set of built-in data types, described below. Additional types can be provided by plugins. Trino type support and mapping# Connectors to data sources are not required to support all Trino data types described on this page. If there are data types similar to Trino’s that are used on the data source, the connector may map the Trino and remote data types to each other as needed. Depending on the connector and the data source, type mapping may apply in either direction as follows: Data source to Trino mapping applies to any operation where columns in the data source are read by Trino, such as a SELECT statement, and the underlying source data type needs to be represented by a Trino data type. Trino to data source mapping applies to any operation where the columns or expressions in Trino need to be translated into data types or expressions compatible with the underlying data source. For example, CREATE TABLE AS statements specify Trino types that are then mapped to types on the remote data source. Predicates like WHERE also use these mappings in order to ensure that the predicate is translated to valid syntax on the remote data source. Data type support and mappings vary depending on the connector. Refer to the connector documentation for more information. Boolean# BOOLEAN# This type captures boolean values true and false. Integer# Integer numbers can be expressed as numeric literals in the following formats: Decimal integer. Examples are -7, 0, or 3. Hexadecimal integer composed of 0X or 0x and the value. Examples are 0x0A for decimal 10 or 0x11 for decimal 17. Octal integer composed of 0O or 0o and the value. Examples are 0o40 for decimal 32 or 0o11 for decimal 9. Binary integer composed of 0B or 0b and the value. Examples are 0b1001 for decimal 9 or 0b101010 for decimal `42``. Underscore characters are ignored within literal values, and can be used to increase readability. For example, decimal integer 123_456 is equivalent to 123456. Preceding underscores, trailing underscores, and consecutive underscores are not permitted. Integers are supported by the following data types. TINYINT# A 8-bit signed two’s complement integer with a minimum value of -2^7 or -0x80 and a maximum value of 2^7 - 1 or 0x7F. SMALLINT# A 16-bit signed two’s complement integer with a minimum value of -2^15 or -0x8000 and a maximum value of 2^15 - 1 or 0x7FFF. INTEGER or INT# A 32-bit signed two’s complement integer with a minimum value of -2^31 or -0x80000000 and a maximum value of 2^31 - 1 or 0x7FFFFFFF. The names INTEGER and INT can both be used for this type. BIGINT# A 64-bit signed two’s complement integer with a minimum value of -2^63 or -0x8000000000000000 and a maximum value of 2^63 - 1 or 0x7FFFFFFFFFFFFFFF. Floating-point# Floating-point, fixed-precision numbers can be expressed as numeric literal using scientific notation such as 1.03e1 and are cast as DOUBLE data type. Underscore characters are ignored within literal values, and can be used to increase readability. For example, value 123_456.789e4 is equivalent to 123456.789e4. Preceding underscores, trailing underscores, consecutive underscores, and underscores beside the comma (.) are not permitted. REAL# A real is a 32-bit inexact, variable-precision implementing the IEEE Standard 754 for Binary Floating-Point Arithmetic. Example literals: REAL '10.3', REAL '10.3e0', REAL '1.03e1' DOUBLE# A double is a 64-bit inexact, variable-precision implementing the IEEE Standard 754 for Binary Floating-Point Arithmetic. Example literals: DOUBLE '10.3', DOUBLE '1.03e1', 10.3e0, 1.03e1 Exact numeric# Exact numeric values can be expressed as numeric literals such as 1.1, and are supported by the DECIMAL data type. Underscore characters are ignored within literal values, and can be used to increase readability. For example, decimal 123_456.789_123 is equivalent to 123456.789123. Preceding underscores, trailing underscores, consecutive underscores, and underscores beside the comma (.) are not permitted. Leading zeros in literal values are permitted and ignored. For example, 000123.456 is equivalent to 123.456. DECIMAL# A exact decimal number. Precision up to 38 digits is supported but performance is best up to 18 digits. The decimal type takes two literal parameters: precision - total number of digits scale - number of digits in fractional part. Scale is optional and defaults to 0. Example type definitions: DECIMAL(10,3), DECIMAL(20) Example literals: DECIMAL '10.3', DECIMAL '1234567890', 1.1 String# VARCHAR# Variable length character data with an optional maximum length. Example type definitions: varchar, varchar(20) SQL statements support simple literal, as well as Unicode usage: literal string : 'Hello winter !' Unicode string with default escape character: U&'Hello winter \2603 !' Unicode string with custom escape character: U&'Hello winter #2603 !' UESCAPE '#' A Unicode string is prefixed with U& and requires an escape character before any Unicode character usage with 4 digits. In the examples above \2603 and #2603 represent a snowman character. Long Unicode codes with 6 digits require usage of the plus symbol before the code. For example, you need to use \+01F600 for a grinning face emoji. Single quotes in string literals can be escaped by using another single quote: 'I am big, it''s the pictures that got small!' CHAR# Fixed length character data. A CHAR type without length specified has a default length of 1. A CHAR(x) value always has a fixed length of x characters. For example, casting dog to CHAR(7) adds four implicit trailing spaces. As with VARCHAR, a single quote in a CHAR literal can be escaped with another single quote: SELECT CHAR 'All right, Mr. DeMille, I''m ready for my close-up.' Example type definitions: char, char(20) VARBINARY# Variable length binary data. SQL statements support usage of binary literal data with the prefix X or x. The binary data has to use hexadecimal format. For example, the binary form of eh? is X'65683F' as you can confirm with the following statement: SELECT from_utf8(x'65683F'); Binary literals ignore any whitespace characters. For example, the literal X'FFFF 0FFF 3FFF FFFF' is equivalent to X'FFFF0FFF3FFFFFFF'. Note Binary strings with length are not yet supported: varbinary(n) JSON# JSON value type, which can be a JSON object, a JSON array, a JSON number, a JSON string, true, false or null. Date and time# See also Date and time functions and operators DATE# Calendar date (year, month, day). Example: DATE '2001-08-22' TIME# TIME is an alias for TIME(3) (millisecond precision). TIME(P)# Time of day (hour, minute, second) without a time zone with P digits of precision for the fraction of seconds. A precision of up to 12 (picoseconds) is supported. Example: TIME '01:02:03.456' TIME WITH TIME ZONE# Time of day (hour, minute, second, millisecond) with a time zone. Values of this type are rendered using the time zone from the value. Time zones are expressed as the numeric UTC offset value: SELECT TIME '01:02:03.456 -08:00'; -- 1:02:03.456-08:00 TIMESTAMP# TIMESTAMP is an alias for TIMESTAMP(3) (millisecond precision). TIMESTAMP(P)# Calendar date and time of day without a time zone with P digits of precision for the fraction of seconds. A precision of up to 12 (picoseconds) is supported. This type is effectively a combination of the DATE and TIME(P) types. TIMESTAMP(P) WITHOUT TIME ZONE is an equivalent name. Timestamp values can be constructed with the TIMESTAMP literal expression. Alternatively, language constructs such as localtimestamp(p), or a number of date and time functions and operators can return timestamp values. Casting to lower precision causes the value to be rounded, and not truncated. Casting to higher precision appends zeros for the additional digits. The following examples illustrate the behavior: SELECT TIMESTAMP '2020-06-10 15:55:23'; -- 2020-06-10 15:55:23 SELECT TIMESTAMP '2020-06-10 15:55:23.383345'; -- 2020-06-10 15:55:23.383345 SELECT typeof(TIMESTAMP '2020-06-10 15:55:23.383345'); -- timestamp(6) SELECT cast(TIMESTAMP '2020-06-10 15:55:23.383345' as TIMESTAMP(1)); -- 2020-06-10 15:55:23.4 SELECT cast(TIMESTAMP '2020-06-10 15:55:23.383345' as TIMESTAMP(12)); -- 2020-06-10 15:55:23.383345000000 TIMESTAMP WITH TIME ZONE# TIMESTAMP WITH TIME ZONE is an alias for TIMESTAMP(3) WITH TIME ZONE (millisecond precision). TIMESTAMP(P) WITH TIME ZONE# Instant in time that includes the date and time of day with P digits of precision for the fraction of seconds and with a time zone. Values of this type are rendered using the time zone from the value. Time zones can be expressed in the following ways: UTC, with GMT, Z, or UT usable as aliases for UTC. +hh:mm or -hh:mm with hh:mm as an hour and minute offset from UTC. Can be written with or without UTC, GMT, or UT as an alias for UTC. An IANA time zone name. The following examples demonstrate some of these syntax options: SELECT TIMESTAMP '2001-08-22 03:04:05.321 UTC'; -- 2001-08-22 03:04:05.321 UTC SELECT TIMESTAMP '2001-08-22 03:04:05.321 -08:30'; -- 2001-08-22 03:04:05.321 -08:30 SELECT TIMESTAMP '2001-08-22 03:04:05.321 GMT-08:30'; -- 2001-08-22 03:04:05.321 -08:30 SELECT TIMESTAMP '2001-08-22 03:04:05.321 America/New_York'; -- 2001-08-22 03:04:05.321 America/New_York INTERVAL YEAR TO MONTH# Span of years and months. Example: INTERVAL '3' MONTH INTERVAL DAY TO SECOND# Span of days, hours, minutes, seconds and milliseconds. Example: INTERVAL '2' DAY Structural# ARRAY# An array of the given component type. Example: ARRAY[1, 2, 3] More information in Array functions and operators. MAP# A map between the given component types. A map is a collection of key-value pairs, where each key is associated with a single value. Example: MAP(ARRAY['foo', 'bar'], ARRAY[1, 2]) More information in Map functions and operators. ROW# A structure made up of fields that allows mixed types. The fields may be of any SQL type. By default, row fields are not named, but names can be assigned. Example: CAST(ROW(1, 2e0) AS ROW(x BIGINT, y DOUBLE)) Named row fields are accessed with field reference operator (.). Example: CAST(ROW(1, 2.0) AS ROW(x BIGINT, y DOUBLE)).x Named or unnamed row fields are accessed by position with the subscript operator ([]). The position starts at 1 and must be a constant. Example: ROW(1, 2.0)[1] Network address# IPADDRESS# An IP address that can represent either an IPv4 or IPv6 address. Internally, the type is a pure IPv6 address. Support for IPv4 is handled using the IPv4-mapped IPv6 address range (RFC 4291#section-2.5.5.2). When creating an IPADDRESS, IPv4 addresses will be mapped into that range. When formatting an IPADDRESS, any address within the mapped range will be formatted as an IPv4 address. Other addresses will be formatted as IPv6 using the canonical format defined in RFC 5952. Examples: IPADDRESS '10.0.0.1', IPADDRESS '2001:db8::1' UUID# UUID# This type represents a UUID (Universally Unique IDentifier), also known as a GUID (Globally Unique IDentifier), using the format defined in RFC 4122. Example: UUID '12151fd2-7586-11e9-8f9e-2a86e4085a59' HyperLogLog# Calculating the approximate distinct count can be done much more cheaply than an exact count using the HyperLogLog data sketch. See HyperLogLog functions. HyperLogLog# A HyperLogLog sketch allows efficient computation of approx_distinct(). It starts as a sparse representation, switching to a dense representation when it becomes more efficient. P4HyperLogLog# A P4HyperLogLog sketch is similar to HyperLogLog, but it starts (and remains) in the dense representation. SetDigest# SetDigest# A SetDigest (setdigest) is a data sketch structure used in calculating Jaccard similarity coefficient between two sets. SetDigest encapsulates the following components: HyperLogLog MinHash with a single hash function The HyperLogLog structure is used for the approximation of the distinct elements in the original set. The MinHash structure is used to store a low memory footprint signature of the original set. The similarity of any two sets is estimated by comparing their signatures. SetDigests are additive, meaning they can be merged together. Quantile digest# QDigest# A quantile digest (qdigest) is a summary structure which captures the approximate distribution of data for a given input set, and can be queried to retrieve approximate quantile values from the distribution. The level of accuracy for a qdigest is tunable, allowing for more precise results at the expense of space. A qdigest can be used to give approximate answer to queries asking for what value belongs at a certain quantile. A useful property of qdigests is that they are additive, meaning they can be merged together without losing precision. A qdigest may be helpful whenever the partial results of approx_percentile can be reused. For example, one may be interested in a daily reading of the 99th percentile values that are read over the course of a week. Instead of calculating the past week of data with approx_percentile, qdigests could be stored daily, and quickly merged to retrieve the 99th percentile value. T-Digest# TDigest# A T-digest (tdigest) is a summary structure which, similarly to qdigest, captures the approximate distribution of data for a given input set. It can be queried to retrieve approximate quantile values from the distribution. TDigest has the following advantages compared to QDigest: higher performance lower memory usage higher accuracy at high and low percentiles T-digests are additive, meaning they can be merged together.

#### Code Examples

```
SELECT CHAR 'All right, Mr. DeMille, I''m ready for my close-up.'
```
```
SELECT from_utf8(x'65683F');
```
```
SELECT TIME '01:02:03.456 -08:00';
-- 1:02:03.456-08:00
```
```
SELECT TIMESTAMP '2020-06-10 15:55:23';
-- 2020-06-10 15:55:23

SELECT TIMESTAMP '2020-06-10 15:55:23.383345';
-- 2020-06-10 15:55:23.383345

SELECT typeof(TIMESTAMP '2020-06-10 15:55:23.383345');
-- timestamp(6)

SELECT cast(TIMESTAMP '2020-06-10 15:55:23.383345' as TIMESTAMP(1));
 -- 2020-06-10 15:55:23.4

SELECT cast(TIMESTAMP '2020-06-10 15:55:23.383345' as TIMESTAMP(12));
-- 2020-06-10 15:55:23.383345000000
```
```
SELECT TIMESTAMP '2001-08-22 03:04:05.321 UTC';
-- 2001-08-22 03:04:05.321 UTC

SELECT TIMESTAMP '2001-08-22 03:04:05.321 -08:30';
-- 2001-08-22 03:04:05.321 -08:30

SELECT TIMESTAMP '2001-08-22 03:04:05.321 GMT-08:30';
-- 2001-08-22 03:04:05.321 -08:30

SELECT TIMESTAMP '2001-08-22 03:04:05.321 America/New_York';
-- 2001-08-22 03:04:05.321 America/New_York
```


---


## SQL Statement Syntax

### ALTER MATERIALIZED VIEW
Source: https://trino.io/docs/current/sql/alter-materialized-view.html

ALTER MATERIALIZED VIEW# Synopsis# ALTER MATERIALIZED VIEW [ IF EXISTS ] name RENAME TO new_name ALTER MATERIALIZED VIEW name SET PROPERTIES property_name = expression [, ...] Description# Change the name of an existing materialized view. The optional IF EXISTS clause causes the error to be suppressed if the materialized view does not exist. The error is not suppressed if the materialized view does not exist, but a table or view with the given name exists. SET PROPERTIES# The ALTER MATERIALIZED VIEW SET PROPERTIES statement followed by some number of property_name and expression pairs applies the specified properties and values to a materialized view. Ommitting an already-set property from this statement leaves that property unchanged in the materialized view. A property in a SET PROPERTIES statement can be set to DEFAULT, which reverts its value back to the default in that materialized view. Support for ALTER MATERIALIZED VIEW SET PROPERTIES varies between connectors. Refer to the connector documentation for more details. Examples# Rename materialized view people to users in the current schema: ALTER MATERIALIZED VIEW people RENAME TO users; Rename materialized view people to users, if materialized view people exists in the current catalog and schema: ALTER MATERIALIZED VIEW IF EXISTS people RENAME TO users; Set view properties (x = y) in materialized view people: ALTER MATERIALIZED VIEW people SET PROPERTIES x = 'y'; Set multiple view properties (foo = 123 and foo bar = 456) in materialized view people: ALTER MATERIALIZED VIEW people SET PROPERTIES foo = 123, "foo bar" = 456; Set view property x to its default value in materialized view people: ALTER MATERIALIZED VIEW people SET PROPERTIES x = DEFAULT; See also# CREATE MATERIALIZED VIEW REFRESH MATERIALIZED VIEW DROP MATERIALIZED VIEW

#### Code Examples

```
ALTER MATERIALIZED VIEW [ IF EXISTS ] name RENAME TO new_name
ALTER MATERIALIZED VIEW name SET PROPERTIES property_name = expression [, ...]
```
```
ALTER MATERIALIZED VIEW people RENAME TO users;
```
```
ALTER MATERIALIZED VIEW IF EXISTS people RENAME TO users;
```
```
ALTER MATERIALIZED VIEW people SET PROPERTIES x = 'y';
```
```
ALTER MATERIALIZED VIEW people SET PROPERTIES foo = 123, "foo bar" = 456;
```
```
ALTER MATERIALIZED VIEW people SET PROPERTIES x = DEFAULT;
```


---

### ALTER SCHEMA
Source: https://trino.io/docs/current/sql/alter-schema.html

ALTER SCHEMA# Synopsis# ALTER SCHEMA name RENAME TO new_name ALTER SCHEMA name SET AUTHORIZATION ( user | USER user | ROLE role ) Description# Change the definition of an existing schema. Examples# Rename schema web to traffic: ALTER SCHEMA web RENAME TO traffic Change owner of schema web to user alice: ALTER SCHEMA web SET AUTHORIZATION alice Allow everyone to drop schema and create tables in schema web: ALTER SCHEMA web SET AUTHORIZATION ROLE PUBLIC See Also# CREATE SCHEMA

#### Code Examples

```
ALTER SCHEMA name RENAME TO new_name
ALTER SCHEMA name SET AUTHORIZATION ( user | USER user | ROLE role )
```
```
ALTER SCHEMA web RENAME TO traffic
```
```
ALTER SCHEMA web SET AUTHORIZATION alice
```
```
ALTER SCHEMA web SET AUTHORIZATION ROLE PUBLIC
```


---

### ALTER TABLE
Source: https://trino.io/docs/current/sql/alter-table.html

ALTER TABLE# Synopsis# ALTER TABLE [ IF EXISTS ] name RENAME TO new_name ALTER TABLE [ IF EXISTS ] name ADD COLUMN [ IF NOT EXISTS ] column_name data_type [ NOT NULL ] [ COMMENT comment ] [ WITH ( property_name = expression [, ...] ) ] [ FIRST | LAST | AFTER after_column_name ] ALTER TABLE [ IF EXISTS ] name DROP COLUMN [ IF EXISTS ] column_name ALTER TABLE [ IF EXISTS ] name RENAME COLUMN [ IF EXISTS ] old_name TO new_name ALTER TABLE [ IF EXISTS ] name ALTER COLUMN column_name SET DATA TYPE new_type ALTER TABLE [ IF EXISTS ] name ALTER COLUMN column_name DROP NOT NULL ALTER TABLE name SET AUTHORIZATION ( user | USER user | ROLE role ) ALTER TABLE name SET PROPERTIES property_name = expression [, ...] ALTER TABLE name EXECUTE command [ ( parameter => expression [, ... ] ) ] [ WHERE expression ] Description# Change the definition of an existing table. The optional IF EXISTS clause, when used before the table name, causes the error to be suppressed if the table does not exist. The optional IF EXISTS clause, when used before the column name, causes the error to be suppressed if the column does not exist. The optional IF NOT EXISTS clause causes the error to be suppressed if the column already exists. SET PROPERTIES# The ALTER TABLE SET PROPERTIES statement followed by a number of property_name and expression pairs applies the specified properties and values to a table. Ommitting an already-set property from this statement leaves that property unchanged in the table. A property in a SET PROPERTIES statement can be set to DEFAULT, which reverts its value back to the default in that table. Support for ALTER TABLE SET PROPERTIES varies between connectors, as not all connectors support modifying table properties. EXECUTE# The ALTER TABLE EXECUTE statement followed by a command and parameters modifies the table according to the specified command and parameters. ALTER TABLE EXECUTE supports different commands on a per-connector basis. You can use the => operator for passing named parameter values. The left side is the name of the parameter, the right side is the value being passed. Executable commands are contributed by connectors, such as the optimize command provided by the Hive, Delta Lake, and Iceberg connectors. For example, a user observing many small files in the storage of a table called test_table in the test schema of the example catalog, can use the optimize command to merge all files below the file_size_threshold value. The result is fewer, but larger files, which typically results in higher query performance on the data in the files: ALTER TABLE example.test.test_table EXECUTE optimize(file_size_threshold => '16MB') Examples# Rename table users to people: ALTER TABLE users RENAME TO people; Rename table users to people if table users exists: ALTER TABLE IF EXISTS users RENAME TO people; Add column zip to the users table: ALTER TABLE users ADD COLUMN zip varchar; Add column zip to the users table if table users exists and column zip not already exists: ALTER TABLE IF EXISTS users ADD COLUMN IF NOT EXISTS zip varchar; Add column id as the first column to the users table: ALTER TABLE users ADD COLUMN id varchar FIRST; Add column zip after column country to the users table: ALTER TABLE users ADD COLUMN zip varchar AFTER country; Drop column zip from the users table: ALTER TABLE users DROP COLUMN zip; Drop column zip from the users table if table users and column zip exists: ALTER TABLE IF EXISTS users DROP COLUMN IF EXISTS zip; Rename column id to user_id in the users table: ALTER TABLE users RENAME COLUMN id TO user_id; Rename column id to user_id in the users table if table users and column id exists: ALTER TABLE IF EXISTS users RENAME column IF EXISTS id to user_id; Change type of column id to bigint in the users table: ALTER TABLE users ALTER COLUMN id SET DATA TYPE bigint; Drop a not null constraint on id column in the users table: ALTER TABLE users ALTER COLUMN id DROP NOT NULL; Change owner of table people to user alice: ALTER TABLE people SET AUTHORIZATION alice Allow everyone with role public to drop and alter table people: ALTER TABLE people SET AUTHORIZATION ROLE PUBLIC Set table properties (x = y) in table people: ALTER TABLE people SET PROPERTIES x = 'y'; Set multiple table properties (foo = 123 and foo bar = 456) in table people: ALTER TABLE people SET PROPERTIES foo = 123, "foo bar" = 456; Set table property x to its default value in table``people``: ALTER TABLE people SET PROPERTIES x = DEFAULT; See also# CREATE TABLE

#### Code Examples

```
ALTER TABLE [ IF EXISTS ] name RENAME TO new_name
ALTER TABLE [ IF EXISTS ] name ADD COLUMN [ IF NOT EXISTS ] column_name data_type
  [ NOT NULL ] [ COMMENT comment ]
  [ WITH ( property_name = expression [, ...] ) ]
  [ FIRST | LAST | AFTER after_column_name ]
ALTER TABLE [ IF EXISTS ] name DROP COLUMN [ IF EXISTS ] column_name
ALTER TABLE [ IF EXISTS ] name RENAME COLUMN [ IF EXISTS ] old_name TO new_name
ALTER TABLE [ IF EXISTS ] name ALTER COLUMN column_name SET DATA TYPE new_type
ALTER TABLE [ IF EXISTS ] name ALTER COLUMN column_name DROP NOT NULL
ALTER TABLE name SET AUTHORIZATION ( user | USER user | ROLE role )
ALTER TABLE name SET PROPERTIES property_name = expression [, ...]
ALTER TABLE name EXECUTE command [ ( parameter => expression [, ... ] ) ]
    [ WHERE expression ]
```
```
ALTER TABLE example.test.test_table EXECUTE optimize(file_size_threshold => '16MB')
```
```
ALTER TABLE users RENAME TO people;
```
```
ALTER TABLE IF EXISTS users RENAME TO people;
```
```
ALTER TABLE users ADD COLUMN zip varchar;
```
```
ALTER TABLE IF EXISTS users ADD COLUMN IF NOT EXISTS zip varchar;
```
```
ALTER TABLE users ADD COLUMN id varchar FIRST;
```
```
ALTER TABLE users ADD COLUMN zip varchar AFTER country;
```
```
ALTER TABLE users DROP COLUMN zip;
```
```
ALTER TABLE IF EXISTS users DROP COLUMN IF EXISTS zip;
```
```
ALTER TABLE users RENAME COLUMN id TO user_id;
```
```
ALTER TABLE IF EXISTS users RENAME column IF EXISTS id to user_id;
```
```
ALTER TABLE users ALTER COLUMN id SET DATA TYPE bigint;
```
```
ALTER TABLE users ALTER COLUMN id DROP NOT NULL;
```
```
ALTER TABLE people SET AUTHORIZATION alice
```
```
ALTER TABLE people SET AUTHORIZATION ROLE PUBLIC
```
```
ALTER TABLE people SET PROPERTIES x = 'y';
```
```
ALTER TABLE people SET PROPERTIES foo = 123, "foo bar" = 456;
```
```
ALTER TABLE people SET PROPERTIES x = DEFAULT;
```


---

### ALTER VIEW
Source: https://trino.io/docs/current/sql/alter-view.html

ALTER VIEW# Synopsis# ALTER VIEW name RENAME TO new_name ALTER VIEW name SET AUTHORIZATION ( user | USER user | ROLE role ) Description# Change the definition of an existing view. Examples# Rename view people to users: ALTER VIEW people RENAME TO users Change owner of VIEW people to user alice: ALTER VIEW people SET AUTHORIZATION alice See also# CREATE VIEW

#### Code Examples

```
ALTER VIEW name RENAME TO new_name
ALTER VIEW name SET AUTHORIZATION ( user | USER user | ROLE role )
```
```
ALTER VIEW people RENAME TO users
```
```
ALTER VIEW people SET AUTHORIZATION alice
```


---

### CREATE CATALOG
Source: https://trino.io/docs/current/sql/create-catalog.html

CREATE CATALOG# Synopsis# CREATE CATALOG catalog_name USING connector_name [ WITH ( property_name = expression [, ...] ) ] Description# Create a new catalog using the specified connector. The optional WITH clause is used to set properties on the newly created catalog. Property names can be double quoted, which is required if they contain special characters, like -. Refer to the connectors documentation to learn about all available properties. All property values must be varchars (single quoted), including numbers and boolean values. The query fails in the following circumstances: A required property is missing. An invalid property is set, for example there is a typo in the property name, or a property name from a different connector was used. The value of the property is invalid, for example a numeric value is out of range, or a string value doesn’t match the required pattern. The value references an environmental variable that is not set on the coordinator node. Warning The complete CREATE CATALOG query is logged, and visible in the Web UI. This includes any sensitive properties, like passwords and other credentials. See Secrets. Note This command requires the catalog management type to be set to dynamic. Examples# Create a new catalog called tpch using the TPC-H connector: CREATE CATALOG tpch USING tpch; Create a new catalog called brain using the Memory connector: CREATE CATALOG brain USING memory WITH ("memory.max-data-per-node" = '128MB'); Notice that the connector property contains dashes (-) and needs to quoted using a double quote ("). The value 128MB is quoted using single quotes, because it is a string literal. Create a new catalog called example using the PostgreSQL connector: CREATE CATALOG example USING postgresql WITH ( "connection-url" = 'jdbc:pg:localhost:5432', "connection-user" = '${ENV:POSTGRES_USER}', "connection-password" = '${ENV:POSTGRES_PASSWORD}', "case-insensitive-name-matching" = 'true' ); This example assumes that the POSTGRES_USER and POSTGRES_PASSWORD environmental variables are set as secrets on all nodes of the cluster. See also# DROP CATALOG Catalog management properties

#### Code Examples

```
CREATE CATALOG
catalog_name
USING connector_name
[ WITH ( property_name = expression [, ...] ) ]
```
```
CREATE CATALOG tpch USING tpch;
```
```
CREATE CATALOG brain USING memory
WITH ("memory.max-data-per-node" = '128MB');
```
```
CREATE CATALOG example USING postgresql
WITH (
  "connection-url" = 'jdbc:pg:localhost:5432',
  "connection-user" = '${ENV:POSTGRES_USER}',
  "connection-password" = '${ENV:POSTGRES_PASSWORD}',
  "case-insensitive-name-matching" = 'true'
);
```


---

### CREATE FUNCTION
Source: https://trino.io/docs/current/sql/create-function.html

CREATE FUNCTION# Synopsis# CREATE [OR REPLACE] FUNCTION udf_definition Description# Create or replace a Catalog user-defined functions. The udf_definition is composed of the usage of FUNCTION and nested statements. The name of the UDF must be fully qualified with catalog and schema location, unless the default UDF storage catalog and schema are configured. The connector used in the catalog must support UDF storage. The optional OR REPLACE clause causes the UDF to be replaced if it already exists rather than raising an error. Examples# The following example creates the meaning_of_life UDF in the default schema of the example catalog: CREATE FUNCTION example.default.meaning_of_life() RETURNS bigint BEGIN RETURN 42; END; If the default catalog and schema for UDF storage is configured, you can use the following more compact syntax: CREATE FUNCTION meaning_of_life() RETURNS bigint RETURN 42; Further examples of varying complexity that cover usage of the FUNCTION statement in combination with other statements are available in the SQL UDF examples documentation. See also# DROP FUNCTION SHOW CREATE FUNCTION SHOW FUNCTIONS User-defined functions SQL environment properties

#### Code Examples

```
CREATE [OR REPLACE] FUNCTION
  udf_definition
```
```
CREATE FUNCTION example.default.meaning_of_life()
  RETURNS bigint
  BEGIN
    RETURN 42;
  END;
```
```
CREATE FUNCTION meaning_of_life() RETURNS bigint RETURN 42;
```


---

### CREATE MATERIALIZED VIEW
Source: https://trino.io/docs/current/sql/create-materialized-view.html

CREATE MATERIALIZED VIEW# Synopsis# CREATE [ OR REPLACE ] MATERIALIZED VIEW [ IF NOT EXISTS ] view_name [ GRACE PERIOD interval ] [ COMMENT string ] [ WITH properties ] AS query Description# Create and validate the definition of a new materialized view view_name of a SELECT query. You need to run the REFRESH MATERIALIZED VIEW statement after the creation to populate the materialized view with data. This materialized view is a physical manifestation of the query results at time of refresh. The data is stored, and can be referenced by future queries. Queries accessing materialized views are typically faster than retrieving data from a view created with the same query. Any computation, aggregation, and other operation to create the data is performed once during refresh of the materialized views, as compared to each time of accessing the view. Multiple reads of view data over time, or by multiple users, all trigger repeated processing. This is avoided for materialized views. The optional OR REPLACE clause causes the materialized view to be replaced if it already exists rather than raising an error. The optional IF NOT EXISTS clause causes the materialized view only to be created if it does not exist yet. Note that OR REPLACE and IF NOT EXISTS are mutually exclusive clauses. The optional GRACE PERIOD clause specifies how long the query materialization is used for querying: Within the grace period since last refresh, data retrieval is highly performant because the query materialization is used. However, the data may not be up to date with the base tables. After the grace period has elapsed, the data of the materialized view is computed on-the-fly using the query. Retrieval is therefore slower, but the data is up to date with the base tables. If not specified, the grace period defaults to infinity, and therefore all queries are within the grace period. Every REFRESH MATERIALIZED VIEW operation resets the start time for the grace period. The optional COMMENT clause causes a string comment to be stored with the metadata about the materialized view. The comment is displayed with the SHOW CREATE MATERIALIZED VIEW statement and is available in the table system.metadata.materialized_view_properties. The optional WITH clause is used to define properties for the materialized view creation. Separate multiple property/value pairs by commas. The connector uses the properties as input parameters for the materialized view refresh operation. The supported properties are different for each connector and detailed in the SQL support section of the specific connector’s documentation. After successful creation, all metadata about the materialized view is available in a system table. Examples# Create a simple materialized view cancelled_orders over the orders table that only includes cancelled orders. Note that orderstatus is a numeric value that is potentially meaningless to a consumer, yet the name of the view clarifies the content: CREATE MATERIALIZED VIEW cancelled_orders AS SELECT orderkey, totalprice FROM orders WHERE orderstatus = 3; Create or replace a materialized view order_totals_by_date that summarizes orders across all orders from all customers: CREATE OR REPLACE MATERIALIZED VIEW order_totals_by_date AS SELECT orderdate, sum(totalprice) AS price FROM orders GROUP BY orderdate; Create a materialized view for a catalog using the Iceberg connector, with a comment and partitioning on two fields in the storage: CREATE MATERIALIZED VIEW orders_nation_mkgsegment COMMENT 'Orders with nation and market segment data' WITH ( partitioning = ARRAY['mktsegment', 'nationkey'] ) AS SELECT o.*, c.nationkey, c.mktsegment FROM orders AS o JOIN customer AS c ON o.custkey = c.custkey; Set multiple properties: WITH ( format = 'ORC', partitioning = ARRAY['_date'] ) Show defined materialized view properties for all catalogs: SELECT * FROM system.metadata.materialized_view_properties; Show metadata about the materialized views in all catalogs: SELECT * FROM system.metadata.materialized_views; See also# DROP MATERIALIZED VIEW SHOW CREATE MATERIALIZED VIEW REFRESH MATERIALIZED VIEW

#### Code Examples

```
CREATE [ OR REPLACE ] MATERIALIZED VIEW
[ IF NOT EXISTS ] view_name
[ GRACE PERIOD interval ]
[ COMMENT string ]
[ WITH properties ]
AS query
```
```
CREATE MATERIALIZED VIEW cancelled_orders
AS
    SELECT orderkey, totalprice
    FROM orders
    WHERE orderstatus = 3;
```
```
CREATE OR REPLACE MATERIALIZED VIEW order_totals_by_date
AS
    SELECT orderdate, sum(totalprice) AS price
    FROM orders
    GROUP BY orderdate;
```
```
CREATE MATERIALIZED VIEW orders_nation_mkgsegment
COMMENT 'Orders with nation and market segment data'
WITH ( partitioning = ARRAY['mktsegment', 'nationkey'] )
AS
    SELECT o.*, c.nationkey, c.mktsegment
    FROM orders AS o
    JOIN customer AS c
    ON o.custkey = c.custkey;
```
```
WITH ( format = 'ORC', partitioning = ARRAY['_date'] )
```
```
SELECT * FROM system.metadata.materialized_view_properties;
```
```
SELECT * FROM system.metadata.materialized_views;
```


---

### CREATE ROLE
Source: https://trino.io/docs/current/sql/create-role.html

CREATE ROLE# Synopsis# CREATE ROLE role_name [ WITH ADMIN ( user | USER user | ROLE role | CURRENT_USER | CURRENT_ROLE ) ] [ IN catalog ] Description# CREATE ROLE creates the specified role. The optional WITH ADMIN clause causes the role to be created with the specified user as a role admin. A role admin has permission to drop or grant a role. If the optional WITH ADMIN clause is not specified, the role is created with current user as admin. The optional IN catalog clause creates the role in a catalog as opposed to a system role. Examples# Create role admin CREATE ROLE admin; Create role moderator with admin bob: CREATE ROLE moderator WITH ADMIN USER bob; Limitations# Some connectors do not support role management. See connector documentation for more details. See also# DROP ROLE, SET ROLE, GRANT role, REVOKE role

#### Code Examples

```
CREATE ROLE role_name
[ WITH ADMIN ( user | USER user | ROLE role | CURRENT_USER | CURRENT_ROLE ) ]
[ IN catalog ]
```
```
CREATE ROLE admin;
```
```
CREATE ROLE moderator WITH ADMIN USER bob;
```


---

### CREATE SCHEMA
Source: https://trino.io/docs/current/sql/create-schema.html

CREATE SCHEMA# Synopsis# CREATE SCHEMA [ IF NOT EXISTS ] schema_name [ AUTHORIZATION ( user | USER user | ROLE role ) ] [ WITH ( property_name = expression [, ...] ) ] Description# Create a new, empty schema. A schema is a container that holds tables, views and other database objects. The optional IF NOT EXISTS clause causes the error to be suppressed if the schema already exists. The optional AUTHORIZATION clause can be used to set the owner of the newly created schema to a user or role. The optional WITH clause can be used to set properties on the newly created schema. To list all available schema properties, run the following query: SELECT * FROM system.metadata.schema_properties Examples# Create a new schema web in the current catalog: CREATE SCHEMA web Create a new schema sales in the hive catalog: CREATE SCHEMA hive.sales Create the schema traffic if it does not already exist: CREATE SCHEMA IF NOT EXISTS traffic Create a new schema web and set the owner to user alice: CREATE SCHEMA web AUTHORIZATION alice Create a new schema web, set the LOCATION property to /hive/data/web and set the owner to user alice: CREATE SCHEMA web AUTHORIZATION alice WITH ( LOCATION = '/hive/data/web' ) Create a new schema web and allow everyone to drop schema and create tables in schema web: CREATE SCHEMA web AUTHORIZATION ROLE PUBLIC Create a new schema web, set the LOCATION property to /hive/data/web and allow everyone to drop schema and create tables in schema web: CREATE SCHEMA web AUTHORIZATION ROLE PUBLIC WITH ( LOCATION = '/hive/data/web' ) See also# ALTER SCHEMA, DROP SCHEMA

#### Code Examples

```
CREATE SCHEMA [ IF NOT EXISTS ] schema_name
[ AUTHORIZATION ( user | USER user | ROLE role ) ]
[ WITH ( property_name = expression [, ...] ) ]
```
```
SELECT * FROM system.metadata.schema_properties
```
```
CREATE SCHEMA web
```
```
CREATE SCHEMA hive.sales
```
```
CREATE SCHEMA IF NOT EXISTS traffic
```
```
CREATE SCHEMA web AUTHORIZATION alice
```
```
CREATE SCHEMA web AUTHORIZATION alice WITH ( LOCATION = '/hive/data/web' )
```
```
CREATE SCHEMA web AUTHORIZATION ROLE PUBLIC
```
```
CREATE SCHEMA web AUTHORIZATION ROLE PUBLIC WITH ( LOCATION = '/hive/data/web' )
```


---

### CREATE TABLE
Source: https://trino.io/docs/current/sql/create-table.html

CREATE TABLE# Synopsis# CREATE [ OR REPLACE ] TABLE [ IF NOT EXISTS ] table_name ( { column_name data_type [ NOT NULL ] [ COMMENT comment ] [ WITH ( property_name = expression [, ...] ) ] | LIKE existing_table_name [ { INCLUDING | EXCLUDING } PROPERTIES ] } [, ...] ) [ COMMENT table_comment ] [ WITH ( property_name = expression [, ...] ) ] Description# Create a new, empty table with the specified columns. Use CREATE TABLE AS to create a table with data. The optional OR REPLACE clause causes an existing table with the specified name to be replaced with the new table definition. Support for table replacement varies across connectors. Refer to the connector documentation for details. The optional IF NOT EXISTS clause causes the error to be suppressed if the table already exists. OR REPLACE and IF NOT EXISTS cannot be used together. The optional WITH clause can be used to set properties on the newly created table or on single columns. To list all available table properties, run the following query: SELECT * FROM system.metadata.table_properties To list all available column properties, run the following query: SELECT * FROM system.metadata.column_properties The LIKE clause can be used to include all the column definitions from an existing table in the new table. Multiple LIKE clauses may be specified, which allows copying the columns from multiple tables. If INCLUDING PROPERTIES is specified, all of the table properties are copied to the new table. If the WITH clause specifies the same property name as one of the copied properties, the value from the WITH clause will be used. The default behavior is EXCLUDING PROPERTIES. The INCLUDING PROPERTIES option maybe specified for at most one table. Examples# Create a new table orders: CREATE TABLE orders ( orderkey bigint, orderstatus varchar, totalprice double, orderdate date ) WITH (format = 'ORC') Create the table orders if it does not already exist, adding a table comment and a column comment: CREATE TABLE IF NOT EXISTS orders ( orderkey bigint, orderstatus varchar, totalprice double COMMENT 'Price in cents.', orderdate date ) COMMENT 'A table to keep track of orders.' Create the table bigger_orders using the columns from orders plus additional columns at the start and end: CREATE TABLE bigger_orders ( another_orderkey bigint, LIKE orders, another_orderdate date ) See also# ALTER TABLE, DROP TABLE, CREATE TABLE AS, SHOW CREATE TABLE

#### Code Examples

```
CREATE [ OR REPLACE ] TABLE [ IF NOT EXISTS ]
table_name (
  { column_name data_type [ NOT NULL ]
      [ COMMENT comment ]
      [ WITH ( property_name = expression [, ...] ) ]
  | LIKE existing_table_name
      [ { INCLUDING | EXCLUDING } PROPERTIES ]
  }
  [, ...]
)
[ COMMENT table_comment ]
[ WITH ( property_name = expression [, ...] ) ]
```
```
SELECT * FROM system.metadata.table_properties
```
```
SELECT * FROM system.metadata.column_properties
```
```
CREATE TABLE orders (
  orderkey bigint,
  orderstatus varchar,
  totalprice double,
  orderdate date
)
WITH (format = 'ORC')
```
```
CREATE TABLE IF NOT EXISTS orders (
  orderkey bigint,
  orderstatus varchar,
  totalprice double COMMENT 'Price in cents.',
  orderdate date
)
COMMENT 'A table to keep track of orders.'
```
```
CREATE TABLE bigger_orders (
  another_orderkey bigint,
  LIKE orders,
  another_orderdate date
)
```


---

### CREATE TABLE AS
Source: https://trino.io/docs/current/sql/create-table-as.html

CREATE TABLE AS# Synopsis# CREATE [ OR REPLACE ] TABLE [ IF NOT EXISTS ] table_name [ ( column_alias, ... ) ] [ COMMENT table_comment ] [ WITH ( property_name = expression [, ...] ) ] AS query [ WITH [ NO ] DATA ] Description# Create a new table containing the result of a SELECT query. Use CREATE TABLE to create an empty table. The optional OR REPLACE clause causes an existing table with the specified name to be replaced with the new table definition. Support for table replacement varies across connectors. Refer to the connector documentation for details. The optional IF NOT EXISTS clause causes the error to be suppressed if the table already exists. OR REPLACE and IF NOT EXISTS cannot be used together. The optional WITH clause can be used to set properties on the newly created table. To list all available table properties, run the following query: SELECT * FROM system.metadata.table_properties Examples# Create a new table orders_column_aliased with the results of a query and the given column names: CREATE TABLE orders_column_aliased (order_date, total_price) AS SELECT orderdate, totalprice FROM orders Create a new table orders_by_date that summarizes orders: CREATE TABLE orders_by_date COMMENT 'Summary of orders by date' WITH (format = 'ORC') AS SELECT orderdate, sum(totalprice) AS price FROM orders GROUP BY orderdate Create the table orders_by_date if it does not already exist: CREATE TABLE IF NOT EXISTS orders_by_date AS SELECT orderdate, sum(totalprice) AS price FROM orders GROUP BY orderdate Create a new empty_nation table with the same schema as nation and no data: CREATE TABLE empty_nation AS SELECT * FROM nation WITH NO DATA See also# CREATE TABLE, SELECT

#### Code Examples

```
CREATE [ OR REPLACE ] TABLE [ IF NOT EXISTS ] table_name [ ( column_alias, ... ) ]
[ COMMENT table_comment ]
[ WITH ( property_name = expression [, ...] ) ]
AS query
[ WITH [ NO ] DATA ]
```
```
SELECT * FROM system.metadata.table_properties
```
```
CREATE TABLE orders_column_aliased (order_date, total_price)
AS
SELECT orderdate, totalprice
FROM orders
```
```
CREATE TABLE orders_by_date
COMMENT 'Summary of orders by date'
WITH (format = 'ORC')
AS
SELECT orderdate, sum(totalprice) AS price
FROM orders
GROUP BY orderdate
```
```
CREATE TABLE IF NOT EXISTS orders_by_date AS
SELECT orderdate, sum(totalprice) AS price
FROM orders
GROUP BY orderdate
```
```
CREATE TABLE empty_nation AS
SELECT *
FROM nation
WITH NO DATA
```


---

### CREATE VIEW
Source: https://trino.io/docs/current/sql/create-view.html

CREATE VIEW# Synopsis# CREATE [ OR REPLACE ] VIEW view_name [ COMMENT view_comment ] [ SECURITY { DEFINER | INVOKER } ] AS query Description# Create a new view of a SELECT query. The view is a logical table that can be referenced by future queries. Views do not contain any data. Instead, the query stored by the view is executed every time the view is referenced by another query. The optional OR REPLACE clause causes the view to be replaced if it already exists rather than raising an error. Security# In the default DEFINER security mode, tables referenced in the view are accessed using the permissions of the view owner (the creator or definer of the view) rather than the user executing the query. This allows providing restricted access to the underlying tables, for which the user may not be allowed to access directly. In the INVOKER security mode, tables referenced in the view are accessed using the permissions of the user executing the query (the invoker of the view). A view created in this mode is simply a stored query. Regardless of the security mode, the current_user function will always return the user executing the query and thus may be used within views to filter out rows or otherwise restrict access. Examples# Create a simple view test over the orders table: CREATE VIEW test AS SELECT orderkey, orderstatus, totalprice / 2 AS half FROM orders Create a view test_with_comment with a view comment: CREATE VIEW test_with_comment COMMENT 'A view to keep track of orders.' AS SELECT orderkey, orderstatus, totalprice FROM orders Create a view orders_by_date that summarizes orders: CREATE VIEW orders_by_date AS SELECT orderdate, sum(totalprice) AS price FROM orders GROUP BY orderdate Create a view that replaces an existing view: CREATE OR REPLACE VIEW test AS SELECT orderkey, orderstatus, totalprice / 4 AS quarter FROM orders See also# ALTER VIEW DROP VIEW SHOW CREATE VIEW SHOW TABLES

#### Code Examples

```
CREATE [ OR REPLACE ] VIEW view_name
[ COMMENT view_comment ]
[ SECURITY { DEFINER | INVOKER } ]
AS query
```
```
CREATE VIEW test AS
SELECT orderkey, orderstatus, totalprice / 2 AS half
FROM orders
```
```
CREATE VIEW test_with_comment
COMMENT 'A view to keep track of orders.'
AS
SELECT orderkey, orderstatus, totalprice
FROM orders
```
```
CREATE VIEW orders_by_date AS
SELECT orderdate, sum(totalprice) AS price
FROM orders
GROUP BY orderdate
```
```
CREATE OR REPLACE VIEW test AS
SELECT orderkey, orderstatus, totalprice / 4 AS quarter
FROM orders
```


---

### DROP CATALOG
Source: https://trino.io/docs/current/sql/drop-catalog.html

DROP CATALOG# Synopsis# DROP CATALOG catalog_name Description# Drops an existing catalog. Dropping a catalog does not interrupt any running queries that use it, but makes it unavailable to any new queries. Warning Some connectors are known not to release all resources when dropping a catalog that uses such connector. This includes all connectors that can read data from HDFS, S3, GCS, or Azure, which are Hive connector, Iceberg connector, Delta Lake connector, and Hudi connector. Note This command requires the catalog management type to be set to dynamic. Examples# Drop the catalog example: DROP CATALOG example; See also# CREATE CATALOG Catalog management properties

#### Code Examples

```
DROP CATALOG catalog_name
```
```
DROP CATALOG example;
```


---

### DROP FUNCTION
Source: https://trino.io/docs/current/sql/drop-function.html

DROP FUNCTION# Synopsis# DROP FUNCTION [ IF EXISTS ] udf_name ( [ [ parameter_name ] data_type [, ...] ] ) Description# Removes a catalog UDF. The value of udf_name must be fully qualified with catalog and schema location of the UDF, unless the default UDF storage catalog and schema are configured. The data_types must be included for UDFs that use parameters to ensure the UDF with the correct name and parameter signature is removed. The optional IF EXISTS clause causes the error to be suppressed if the function does not exist. Examples# The following example removes the meaning_of_life UDF in the default schema of the example catalog: DROP FUNCTION example.default.meaning_of_life(); If the UDF uses a input parameter, the type must be added: DROP FUNCTION multiply_by_two(bigint); If the default catalog and schema for UDF storage is configured, you can use the following more compact syntax: DROP FUNCTION meaning_of_life(); See also# CREATE FUNCTION SHOW CREATE FUNCTION SHOW FUNCTIONS User-defined functions SQL environment properties

#### Code Examples

```
DROP FUNCTION [ IF EXISTS ] udf_name ( [ [ parameter_name ] data_type [, ...] ] )
```
```
DROP FUNCTION example.default.meaning_of_life();
```
```
DROP FUNCTION multiply_by_two(bigint);
```
```
DROP FUNCTION meaning_of_life();
```


---

### DROP MATERIALIZED VIEW
Source: https://trino.io/docs/current/sql/drop-materialized-view.html

DROP MATERIALIZED VIEW# Synopsis# DROP MATERIALIZED VIEW [ IF EXISTS ] view_name Description# Drop an existing materialized view view_name. The optional IF EXISTS clause causes the error to be suppressed if the materialized view does not exist. Examples# Drop the materialized view orders_by_date: DROP MATERIALIZED VIEW orders_by_date; Drop the materialized view orders_by_date if it exists: DROP MATERIALIZED VIEW IF EXISTS orders_by_date; See also# CREATE MATERIALIZED VIEW SHOW CREATE MATERIALIZED VIEW REFRESH MATERIALIZED VIEW

#### Code Examples

```
DROP MATERIALIZED VIEW [ IF EXISTS ] view_name
```
```
DROP MATERIALIZED VIEW orders_by_date;
```
```
DROP MATERIALIZED VIEW IF EXISTS orders_by_date;
```


---

### DROP ROLE
Source: https://trino.io/docs/current/sql/drop-role.html

DROP ROLE# Synopsis# DROP ROLE [ IF EXISTS ] role_name [ IN catalog ] Description# DROP ROLE drops the specified role. For DROP ROLE statement to succeed, the user executing it should possess admin privileges for the given role. The optional IF EXISTS prevents the statement from failing if the role isn’t found. The optional IN catalog clause drops the role in a catalog as opposed to a system role. Examples# Drop role admin DROP ROLE admin; Limitations# Some connectors do not support role management. See connector documentation for more details. See also# CREATE ROLE, SET ROLE, GRANT role, REVOKE role

#### Code Examples

```
DROP ROLE [ IF EXISTS ] role_name
[ IN catalog ]
```
```
DROP ROLE admin;
```


---

### DROP SCHEMA
Source: https://trino.io/docs/current/sql/drop-schema.html

DROP SCHEMA# Synopsis# DROP SCHEMA [ IF EXISTS ] schema_name [ CASCADE | RESTRICT ] Description# Drop an existing schema. The schema must be empty. The optional IF EXISTS clause causes the error to be suppressed if the schema does not exist. Examples# Drop the schema web: DROP SCHEMA web Drop the schema sales if it exists: DROP SCHEMA IF EXISTS sales Drop the schema archive, along with everything it contains: DROP SCHEMA archive CASCADE Drop the schema archive, only if there are no objects contained in the schema: DROP SCHEMA archive RESTRICT See also# ALTER SCHEMA, CREATE SCHEMA

#### Code Examples

```
DROP SCHEMA [ IF EXISTS ] schema_name [ CASCADE | RESTRICT ]
```
```
DROP SCHEMA web
```
```
DROP SCHEMA IF EXISTS sales
```
```
DROP SCHEMA archive CASCADE
```
```
DROP SCHEMA archive RESTRICT
```


---

### DROP TABLE
Source: https://trino.io/docs/current/sql/drop-table.html

DROP TABLE# Synopsis# DROP TABLE [ IF EXISTS ] table_name Description# Drops an existing table. The optional IF EXISTS clause causes the error to be suppressed if the table does not exist. The error is not suppressed if a Trino view with the same name exists. Examples# Drop the table orders_by_date: DROP TABLE orders_by_date Drop the table orders_by_date if it exists: DROP TABLE IF EXISTS orders_by_date See also# ALTER TABLE, CREATE TABLE

#### Code Examples

```
DROP TABLE  [ IF EXISTS ] table_name
```
```
DROP TABLE orders_by_date
```
```
DROP TABLE IF EXISTS orders_by_date
```


---

### DROP VIEW
Source: https://trino.io/docs/current/sql/drop-view.html

DROP VIEW# Synopsis# DROP VIEW [ IF EXISTS ] view_name Description# Drop an existing view. The optional IF EXISTS clause causes the error to be suppressed if the view does not exist. Examples# Drop the view orders_by_date: DROP VIEW orders_by_date Drop the view orders_by_date if it exists: DROP VIEW IF EXISTS orders_by_date See also# CREATE VIEW

#### Code Examples

```
DROP VIEW [ IF EXISTS ] view_name
```
```
DROP VIEW orders_by_date
```
```
DROP VIEW IF EXISTS orders_by_date
```


---

### DELETE
Source: https://trino.io/docs/current/sql/delete.html

DELETE# Synopsis# DELETE FROM table_name [ WHERE condition ] Description# Delete rows from a table. If the WHERE clause is specified, only the matching rows are deleted. Otherwise, all rows from the table are deleted. Examples# Delete all line items shipped by air: DELETE FROM lineitem WHERE shipmode = 'AIR'; Delete all line items for low priority orders: DELETE FROM lineitem WHERE orderkey IN (SELECT orderkey FROM orders WHERE priority = 'LOW'); Delete all orders: DELETE FROM orders; Limitations# Some connectors have limited or no support for DELETE. See connector documentation for more details.

#### Code Examples

```
DELETE FROM table_name [ WHERE condition ]
```
```
DELETE FROM lineitem WHERE shipmode = 'AIR';
```
```
DELETE FROM lineitem
WHERE orderkey IN (SELECT orderkey FROM orders WHERE priority = 'LOW');
```
```
DELETE FROM orders;
```


---

### INSERT
Source: https://trino.io/docs/current/sql/insert.html

INSERT# Synopsis# INSERT INTO table_name [ ( column [, ... ] ) ] query Description# Insert new rows into a table. If the list of column names is specified, they must exactly match the list of columns produced by the query. Each column in the table not present in the column list will be filled with a null value. Otherwise, if the list of columns is not specified, the columns produced by the query must exactly match the columns in the table being inserted into. Examples# Load additional rows into the orders table from the new_orders table: INSERT INTO orders SELECT * FROM new_orders; Insert a single row into the cities table: INSERT INTO cities VALUES (1, 'San Francisco'); Insert multiple rows into the cities table: INSERT INTO cities VALUES (2, 'San Jose'), (3, 'Oakland'); Insert a single row into the nation table with the specified column list: INSERT INTO nation (nationkey, name, regionkey, comment) VALUES (26, 'POLAND', 3, 'no comment'); Insert a row without specifying the comment column. That column will be null: INSERT INTO nation (nationkey, name, regionkey) VALUES (26, 'POLAND', 3); See also# VALUES

#### Code Examples

```
INSERT INTO table_name [ ( column [, ... ] ) ] query
```
```
INSERT INTO orders
SELECT * FROM new_orders;
```
```
INSERT INTO cities VALUES (1, 'San Francisco');
```
```
INSERT INTO cities VALUES (2, 'San Jose'), (3, 'Oakland');
```
```
INSERT INTO nation (nationkey, name, regionkey, comment)
VALUES (26, 'POLAND', 3, 'no comment');
```
```
INSERT INTO nation (nationkey, name, regionkey)
VALUES (26, 'POLAND', 3);
```


---

### MERGE
Source: https://trino.io/docs/current/sql/merge.html

MERGE# Synopsis# MERGE INTO target_table [ [ AS ] target_alias ] USING { source_table | query } [ [ AS ] source_alias ] ON search_condition when_clause [...] where when_clause is one of WHEN MATCHED [ AND condition ] THEN DELETE WHEN MATCHED [ AND condition ] THEN UPDATE SET ( column = expression [, ...] ) WHEN NOT MATCHED [ AND condition ] THEN INSERT [ column_list ] VALUES (expression, ...) Description# Conditionally update and/or delete rows of a table and/or insert new rows into a table. MERGE changes data in the target_table based on the contents of the source_table. The search_condition defines a condition, such as a relation from identical columns, to associate the source and target data. MERGE supports an arbitrary number of WHEN clauses. MATCHED conditions can execute DELETE or UPDATE operations on the target data, while NOT MATCHED conditions can add data from the source to the target table with INSERT. Additional conditions can narrow down the affected rows. For each source row, the WHEN clauses are processed in order. Only the first matching WHEN clause is executed and subsequent clauses are ignored. The query fails if a single target table row matches more than one source row. In WHEN clauses with UPDATE operations, the column value expressions can depend on any field of the target or the source. In the NOT MATCHED case, the INSERT expressions can depend on any field of the source. Typical usage of MERGE involves two tables with similar structure, containing different data. For example, the source table is part of a transactional usage in a production system, while the target table is located in a data warehouse used for analytics. Periodically, MERGE operations are run to combine recent production data with long-term data in the analytics warehouse. As long as you can define a search condition between the two tables, you can also use very different tables. Examples# Delete all customers mentioned in the source table: MERGE INTO accounts t USING monthly_accounts_update s ON t.customer = s.customer WHEN MATCHED THEN DELETE For matching customer rows, increment the purchases, and if there is no match, insert the row from the source table: MERGE INTO accounts t USING monthly_accounts_update s ON (t.customer = s.customer) WHEN MATCHED THEN UPDATE SET purchases = s.purchases + t.purchases WHEN NOT MATCHED THEN INSERT (customer, purchases, address) VALUES(s.customer, s.purchases, s.address) MERGE into the target table from the source table, deleting any matching target row for which the source address is Centreville. For all other matching rows, add the source purchases and set the address to the source address. If there is no match in the target table, insert the source table row: MERGE INTO accounts t USING monthly_accounts_update s ON (t.customer = s.customer) WHEN MATCHED AND s.address = 'Centreville' THEN DELETE WHEN MATCHED THEN UPDATE SET purchases = s.purchases + t.purchases, address = s.address WHEN NOT MATCHED THEN INSERT (customer, purchases, address) VALUES(s.customer, s.purchases, s.address) Limitations# Any connector can be used as a source table for a MERGE statement. Only connectors which support the MERGE statement can be the target of a merge operation. See the connector documentation for more information.

#### Code Examples

```
MERGE INTO target_table [ [ AS ]  target_alias ]
USING { source_table | query } [ [ AS ] source_alias ]
ON search_condition
when_clause [...]
```
```
WHEN MATCHED [ AND condition ]
    THEN DELETE
```
```
WHEN MATCHED [ AND condition ]
    THEN UPDATE SET ( column = expression [, ...] )
```
```
WHEN NOT MATCHED [ AND condition ]
    THEN INSERT [ column_list ] VALUES (expression, ...)
```
```
MERGE INTO accounts t USING monthly_accounts_update s
    ON t.customer = s.customer
    WHEN MATCHED
        THEN DELETE
```
```
MERGE INTO accounts t USING monthly_accounts_update s
    ON (t.customer = s.customer)
    WHEN MATCHED
        THEN UPDATE SET purchases = s.purchases + t.purchases
    WHEN NOT MATCHED
        THEN INSERT (customer, purchases, address)
              VALUES(s.customer, s.purchases, s.address)
```
```
MERGE INTO accounts t USING monthly_accounts_update s
    ON (t.customer = s.customer)
    WHEN MATCHED AND s.address = 'Centreville'
        THEN DELETE
    WHEN MATCHED
        THEN UPDATE
            SET purchases = s.purchases + t.purchases, address = s.address
    WHEN NOT MATCHED
        THEN INSERT (customer, purchases, address)
              VALUES(s.customer, s.purchases, s.address)
```


---

### SELECT
Source: https://trino.io/docs/current/sql/select.html

SELECT# Synopsis# [ WITH SESSION [ name = expression [, ...] ] [ WITH [ FUNCTION udf ] [, ...] ] [ WITH [ RECURSIVE ] with_query [, ...] ] SELECT [ ALL | DISTINCT ] select_expression [, ...] [ FROM from_item [, ...] ] [ WHERE condition ] [ GROUP BY [ ALL | DISTINCT ] grouping_element [, ...] ] [ HAVING condition] [ WINDOW window_definition_list] [ { UNION | INTERSECT | EXCEPT } [ ALL | DISTINCT ] select ] [ ORDER BY expression [ ASC | DESC ] [, ...] ] [ OFFSET count [ ROW | ROWS ] ] [ LIMIT { count | ALL } ] [ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } { ONLY | WITH TIES } ] where from_item is one of table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ] from_item join_type from_item [ ON join_condition | USING ( join_column [, ...] ) ] table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ] MATCH_RECOGNIZE pattern_recognition_specification [ [ AS ] alias [ ( column_alias [, ...] ) ] ] For detailed description of MATCH_RECOGNIZE clause, see pattern recognition in FROM clause. TABLE (table_function_invocation) [ [ AS ] alias [ ( column_alias [, ...] ) ] ] For description of table functions usage, see table functions. and join_type is one of [ INNER ] JOIN LEFT [ OUTER ] JOIN RIGHT [ OUTER ] JOIN FULL [ OUTER ] JOIN CROSS JOIN and grouping_element is one of () expression GROUPING SETS ( ( column [, ...] ) [, ...] ) CUBE ( column [, ...] ) ROLLUP ( column [, ...] ) Description# Retrieve rows from zero or more tables. WITH SESSION clause# The WITH SESSION clause allows you to set session and catalog session property values applicable for the processing of the current SELECT statement only. The defined values override any other configuration and session property settings. Multiple properties are separated by commas. The following example overrides the global configuration property query.max-execution-time with the session property query_max_execution_time to reduce the time to 2h. It also overrides the catalog property iceberg.query-partition-filter-required from the example catalog using Iceberg connector setting the catalog session property query_partition_filter_required to true: WITH SESSION query_max_execution_time='2h', example.query_partition_filter_required=true SELECT * FROM example.default.thetable LIMIT 100; WITH FUNCTION clause# The WITH FUNCTION clause allows you to define a list of Inline user-defined functions that are available for use in the rest of the query. The following example declares and uses two inline UDFs: WITH FUNCTION hello(name varchar) RETURNS varchar RETURN format('Hello %s!', 'name'), FUNCTION bye(name varchar) RETURNS varchar RETURN format('Bye %s!', 'name') SELECT hello('Finn') || ' and ' || bye('Joe'); -- Hello Finn! and Bye Joe! Find further information about UDFs in general, inline UDFs, all supported statements, and examples in User-defined functions. WITH clause# The WITH clause defines named relations for use within a query. It allows flattening nested queries or simplifying subqueries. For example, the following queries are equivalent: SELECT a, b FROM ( SELECT a, MAX(b) AS b FROM t GROUP BY a ) AS x; WITH x AS (SELECT a, MAX(b) AS b FROM t GROUP BY a) SELECT a, b FROM x; This also works with multiple subqueries: WITH t1 AS (SELECT a, MAX(b) AS b FROM x GROUP BY a), t2 AS (SELECT a, AVG(d) AS d FROM y GROUP BY a) SELECT t1.*, t2.* FROM t1 JOIN t2 ON t1.a = t2.a; Additionally, the relations within a WITH clause can chain: WITH x AS (SELECT a FROM t), y AS (SELECT a AS b FROM x), z AS (SELECT b AS c FROM y) SELECT c FROM z; Warning Currently, the SQL for the WITH clause will be inlined anywhere the named relation is used. This means that if the relation is used more than once and the query is non-deterministic, the results may be different each time. WITH RECURSIVE clause# The WITH RECURSIVE clause is a variant of the WITH clause. It defines a list of queries to process, including recursive processing of suitable queries. Warning This feature is experimental only. Proceed to use it only if you understand potential query failures and the impact of the recursion processing on your workload. A recursive WITH-query must be shaped as a UNION of two relations. The first relation is called the recursion base, and the second relation is called the recursion step. Trino supports recursive WITH-queries with a single recursive reference to a WITH-query from within the query. The name T of the query T can be mentioned once in the FROM clause of the recursion step relation. The following listing shows a simple example, that displays a commonly used form of a single query in the list: WITH RECURSIVE t(n) AS ( VALUES (1) UNION ALL SELECT n + 1 FROM t WHERE n < 4 ) SELECT sum(n) FROM t; In the preceding query the simple assignment VALUES (1) defines the recursion base relation. SELECT n + 1 FROM t WHERE n < 4 defines the recursion step relation. The recursion processing performs these steps: recursive base yields 1 first recursion yields 1 + 1 = 2 second recursion uses the result from the first and adds one: 2 + 1 = 3 third recursion uses the result from the second and adds one again: 3 + 1 = 4 fourth recursion aborts since n = 4 this results in t having values 1, 2, 3 and 4 the final statement performs the sum operation of these elements with the final result value 10 The types of the returned columns are those of the base relation. Therefore it is required that types in the step relation can be coerced to base relation types. The RECURSIVE clause applies to all queries in the WITH list, but not all of them must be recursive. If a WITH-query is not shaped according to the rules mentioned above or it does not contain a recursive reference, it is processed like a regular WITH-query. Column aliases are mandatory for all the queries in the recursive WITH list. The following limitations apply as a result of following the SQL standard and due to implementation choices, in addition to WITH clause limitations: only single-element recursive cycles are supported. Like in regular WITH-queries, references to previous queries in the WITH list are allowed. References to following queries are forbidden. usage of outer joins, set operations, limit clause, and others is not always allowed in the step relation recursion depth is fixed, defaults to 10, and doesn’t depend on the actual query results You can adjust the recursion depth with the session property max_recursion_depth. When changing the value consider that the size of the query plan growth is quadratic with the recursion depth. SELECT clause# The SELECT clause specifies the output of the query. Each select_expression defines a column or columns to be included in the result. SELECT [ ALL | DISTINCT ] select_expression [, ...] The ALL and DISTINCT quantifiers determine whether duplicate rows are included in the result set. If the argument ALL is specified, all rows are included. If the argument DISTINCT is specified, only unique rows are included in the result set. In this case, each output column must be of a type that allows comparison. If neither argument is specified, the behavior defaults to ALL. Select expressions# Each select_expression must be in one of the following forms: expression [ [ AS ] column_alias ] row_expression.* [ AS ( column_alias [, ...] ) ] relation.* * In the case of expression [ [ AS ] column_alias ], a single output column is defined. In the case of row_expression.* [ AS ( column_alias [, ...] ) ], the row_expression is an arbitrary expression of type ROW. All fields of the row define output columns to be included in the result set. In the case of relation.*, all columns of relation are included in the result set. In this case column aliases are not allowed. In the case of *, all columns of the relation defined by the query are included in the result set. In the result set, the order of columns is the same as the order of their specification by the select expressions. If a select expression returns multiple columns, they are ordered the same way they were ordered in the source relation or row type expression. If column aliases are specified, they override any preexisting column or row field names: SELECT (CAST(ROW(1, true) AS ROW(field1 bigint, field2 boolean))).* AS (alias1, alias2); alias1 | alias2 --------+-------- 1 | true (1 row) Otherwise, the existing names are used: SELECT (CAST(ROW(1, true) AS ROW(field1 bigint, field2 boolean))).*; field1 | field2 --------+-------- 1 | true (1 row) and in their absence, anonymous columns are produced: SELECT (ROW(1, true)).*; _col0 | _col1 -------+------- 1 | true (1 row) GROUP BY clause# The GROUP BY clause divides the output of a SELECT statement into groups of rows containing matching values. A simple GROUP BY clause may contain any expression composed of input columns or it may be an ordinal number selecting an output column by position (starting at one). The following queries are equivalent. They both group the output by the nationkey input column with the first query using the ordinal position of the output column and the second query using the input column name: SELECT count(*), nationkey FROM customer GROUP BY 2; SELECT count(*), nationkey FROM customer GROUP BY nationkey; GROUP BY clauses can group output by input column names not appearing in the output of a select statement. For example, the following query generates row counts for the customer table using the input column mktsegment: SELECT count(*) FROM customer GROUP BY mktsegment; _col0 ------- 29968 30142 30189 29949 29752 (5 rows) When a GROUP BY clause is used in a SELECT statement all output expressions must be either aggregate functions or columns present in the GROUP BY clause. Complex grouping operations# Trino also supports complex aggregations using the GROUPING SETS, CUBE and ROLLUP syntax. This syntax allows users to perform analysis that requires aggregation on multiple sets of columns in a single query. Complex grouping operations do not support grouping on expressions composed of input columns. Only column names are allowed. Complex grouping operations are often equivalent to a UNION ALL of simple GROUP BY expressions, as shown in the following examples. This equivalence does not apply, however, when the source of data for the aggregation is non-deterministic. GROUPING SETS# Grouping sets allow users to specify multiple lists of columns to group on. The columns not part of a given sublist of grouping columns are set to NULL. SELECT * FROM shipping; origin_state | origin_zip | destination_state | destination_zip | package_weight --------------+------------+-------------------+-----------------+---------------- California | 94131 | New Jersey | 8648 | 13 California | 94131 | New Jersey | 8540 | 42 New Jersey | 7081 | Connecticut | 6708 | 225 California | 90210 | Connecticut | 6927 | 1337 California | 94131 | Colorado | 80302 | 5 New York | 10002 | New Jersey | 8540 | 3 (6 rows) GROUPING SETS semantics are demonstrated by this example query: SELECT origin_state, origin_zip, destination_state, sum(package_weight) FROM shipping GROUP BY GROUPING SETS ( (origin_state), (origin_state, origin_zip), (destination_state)); origin_state | origin_zip | destination_state | _col0 --------------+------------+-------------------+------- New Jersey | NULL | NULL | 225 California | NULL | NULL | 1397 New York | NULL | NULL | 3 California | 90210 | NULL | 1337 California | 94131 | NULL | 60 New Jersey | 7081 | NULL | 225 New York | 10002 | NULL | 3 NULL | NULL | Colorado | 5 NULL | NULL | New Jersey | 58 NULL | NULL | Connecticut | 1562 (10 rows) The preceding query may be considered logically equivalent to a UNION ALL of multiple GROUP BY queries: SELECT origin_state, NULL, NULL, sum(package_weight) FROM shipping GROUP BY origin_state UNION ALL SELECT origin_state, origin_zip, NULL, sum(package_weight) FROM shipping GROUP BY origin_state, origin_zip UNION ALL SELECT NULL, NULL, destination_state, sum(package_weight) FROM shipping GROUP BY destination_state; However, the query with the complex grouping syntax (GROUPING SETS, CUBE or ROLLUP) will only read from the underlying data source once, while the query with the UNION ALL reads the underlying data three times. This is why queries with a UNION ALL may produce inconsistent results when the data source is not deterministic. CUBE# The CUBE operator generates all possible grouping sets (i.e. a power set) for a given set of columns. For example, the query: SELECT origin_state, destination_state, sum(package_weight) FROM shipping GROUP BY CUBE (origin_state, destination_state); is equivalent to: SELECT origin_state, destination_state, sum(package_weight) FROM shipping GROUP BY GROUPING SETS ( (origin_state, destination_state), (origin_state), (destination_state), () ); origin_state | destination_state | _col0 --------------+-------------------+------- California | New Jersey | 55 California | Colorado | 5 New York | New Jersey | 3 New Jersey | Connecticut | 225 California | Connecticut | 1337 California | NULL | 1397 New York | NULL | 3 New Jersey | NULL | 225 NULL | New Jersey | 58 NULL | Connecticut | 1562 NULL | Colorado | 5 NULL | NULL | 1625 (12 rows) ROLLUP# The ROLLUP operator generates all possible subtotals for a given set of columns. For example, the query: SELECT origin_state, origin_zip, sum(package_weight) FROM shipping GROUP BY ROLLUP (origin_state, origin_zip); origin_state | origin_zip | _col2 --------------+------------+------- California | 94131 | 60 California | 90210 | 1337 New Jersey | 7081 | 225 New York | 10002 | 3 California | NULL | 1397 New York | NULL | 3 New Jersey | NULL | 225 NULL | NULL | 1625 (8 rows) is equivalent to: SELECT origin_state, origin_zip, sum(package_weight) FROM shipping GROUP BY GROUPING SETS ((origin_state, origin_zip), (origin_state), ()); Combining multiple grouping expressions# Multiple grouping expressions in the same query are interpreted as having cross-product semantics. For example, the following query: SELECT origin_state, destination_state, origin_zip, sum(package_weight) FROM shipping GROUP BY GROUPING SETS ((origin_state, destination_state)), ROLLUP (origin_zip); which can be rewritten as: SELECT origin_state, destination_state, origin_zip, sum(package_weight) FROM shipping GROUP BY GROUPING SETS ((origin_state, destination_state)), GROUPING SETS ((origin_zip), ()); is logically equivalent to: SELECT origin_state, destination_state, origin_zip, sum(package_weight) FROM shipping GROUP BY GROUPING SETS ( (origin_state, destination_state, origin_zip), (origin_state, destination_state) ); origin_state | destination_state | origin_zip | _col3 --------------+-------------------+------------+------- New York | New Jersey | 10002 | 3 California | New Jersey | 94131 | 55 New Jersey | Connecticut | 7081 | 225 California | Connecticut | 90210 | 1337 California | Colorado | 94131 | 5 New York | New Jersey | NULL | 3 New Jersey | Connecticut | NULL | 225 California | Colorado | NULL | 5 California | Connecticut | NULL | 1337 California | New Jersey | NULL | 55 (10 rows) The ALL and DISTINCT quantifiers determine whether duplicate grouping sets each produce distinct output rows. This is particularly useful when multiple complex grouping sets are combined in the same query. For example, the following query: SELECT origin_state, destination_state, origin_zip, sum(package_weight) FROM shipping GROUP BY ALL CUBE (origin_state, destination_state), ROLLUP (origin_state, origin_zip); is equivalent to: SELECT origin_state, destination_state, origin_zip, sum(package_weight) FROM shipping GROUP BY GROUPING SETS ( (origin_state, destination_state, origin_zip), (origin_state, origin_zip), (origin_state, destination_state, origin_zip), (origin_state, origin_zip), (origin_state, destination_state), (origin_state), (origin_state, destination_state), (origin_state), (origin_state, destination_state), (origin_state), (destination_state), () ); However, if the query uses the DISTINCT quantifier for the GROUP BY: SELECT origin_state, destination_state, origin_zip, sum(package_weight) FROM shipping GROUP BY DISTINCT CUBE (origin_state, destination_state), ROLLUP (origin_state, origin_zip); only unique grouping sets are generated: SELECT origin_state, destination_state, origin_zip, sum(package_weight) FROM shipping GROUP BY GROUPING SETS ( (origin_state, destination_state, origin_zip), (origin_state, origin_zip), (origin_state, destination_state), (origin_state), (destination_state), () ); The default set quantifier is ALL. GROUPING operation# grouping(col1, ..., colN) -> bigint The grouping operation returns a bit set converted to decimal, indicating which columns are present in a grouping. It must be used in conjunction with GROUPING SETS, ROLLUP, CUBE or GROUP BY and its arguments must match exactly the columns referenced in the corresponding GROUPING SETS, ROLLUP, CUBE or GROUP BY clause. To compute the resulting bit set for a particular row, bits are assigned to the argument columns with the rightmost column being the least significant bit. For a given grouping, a bit is set to 0 if the corresponding column is included in the grouping and to 1 otherwise. For example, consider the query below: SELECT origin_state, origin_zip, destination_state, sum(package_weight), grouping(origin_state, origin_zip, destination_state) FROM shipping GROUP BY GROUPING SETS ( (origin_state), (origin_state, origin_zip), (destination_state) ); origin_state | origin_zip | destination_state | _col3 | _col4 --------------+------------+-------------------+-------+------- California | NULL | NULL | 1397 | 3 New Jersey | NULL | NULL | 225 | 3 New York | NULL | NULL | 3 | 3 California | 94131 | NULL | 60 | 1 New Jersey | 7081 | NULL | 225 | 1 California | 90210 | NULL | 1337 | 1 New York | 10002 | NULL | 3 | 1 NULL | NULL | New Jersey | 58 | 6 NULL | NULL | Connecticut | 1562 | 6 NULL | NULL | Colorado | 5 | 6 (10 rows) The first grouping in the above result only includes the origin_state column and excludes the origin_zip and destination_state columns. The bit set constructed for that grouping is 011 where the most significant bit represents origin_state. HAVING clause# The HAVING clause is used in conjunction with aggregate functions and the GROUP BY clause to control which groups are selected. A HAVING clause eliminates groups that do not satisfy the given conditions. HAVING filters groups after groups and aggregates are computed. The following example queries the customer table and selects groups with an account balance greater than the specified value: SELECT count(*), mktsegment, nationkey, CAST(sum(acctbal) AS bigint) AS totalbal FROM customer GROUP BY mktsegment, nationkey HAVING sum(acctbal) > 5700000 ORDER BY totalbal DESC; _col0 | mktsegment | nationkey | totalbal -------+------------+-----------+---------- 1272 | AUTOMOBILE | 19 | 5856939 1253 | FURNITURE | 14 | 5794887 1248 | FURNITURE | 9 | 5784628 1243 | FURNITURE | 12 | 5757371 1231 | HOUSEHOLD | 3 | 5753216 1251 | MACHINERY | 2 | 5719140 1247 | FURNITURE | 8 | 5701952 (7 rows) WINDOW clause# The WINDOW clause is used to define named window specifications. The defined named window specifications can be referred to in the SELECT and ORDER BY clauses of the enclosing query: SELECT orderkey, clerk, totalprice, rank() OVER w AS rnk FROM orders WINDOW w AS (PARTITION BY clerk ORDER BY totalprice DESC) ORDER BY count() OVER w, clerk, rnk The window definition list of WINDOW clause can contain one or multiple named window specifications of the form window_name AS (window_specification) A window specification has the following components: The existing window name, which refers to a named window specification in the WINDOW clause. The window specification associated with the referenced name is the basis of the current specification. The partition specification, which separates the input rows into different partitions. This is analogous to how the GROUP BY clause separates rows into different groups for aggregate functions. The ordering specification, which determines the order in which input rows will be processed by the window function. The window frame, which specifies a sliding window of rows to be processed by the function for a given row. If the frame is not specified, it defaults to RANGE UNBOUNDED PRECEDING, which is the same as RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW. This frame contains all rows from the start of the partition up to the last peer of the current row. In the absence of ORDER BY, all rows are considered peers, so RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW is equivalent to BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING. The window frame syntax supports additional clauses for row pattern recognition. If the row pattern recognition clauses are specified, the window frame for a particular row consists of the rows matched by a pattern starting from that row. Additionally, if the frame specifies row pattern measures, they can be called over the window, similarly to window functions. For more details, see Row pattern recognition in window structures . Each window component is optional. If a window specification does not specify window partitioning, ordering or frame, those components are obtained from the window specification referenced by the existing window name, or from another window specification in the reference chain. In case when there is no existing window name specified, or none of the referenced window specifications contains the component, the default value is used. Set operations# UNION INTERSECT and EXCEPT are all set operations. These clauses are used to combine the results of more than one select statement into a single result set: query UNION [ALL | DISTINCT] query query INTERSECT [ALL | DISTINCT] query query EXCEPT [ALL | DISTINCT] query The argument ALL or DISTINCT controls which rows are included in the final result set. If the argument ALL is specified all rows are included even if the rows are identical. If the argument DISTINCT is specified only unique rows are included in the combined result set. If neither is specified, the behavior defaults to DISTINCT. Multiple set operations are processed left to right, unless the order is explicitly specified via parentheses. Additionally, INTERSECT binds more tightly than EXCEPT and UNION. That means A UNION B INTERSECT C EXCEPT D is the same as A UNION (B INTERSECT C) EXCEPT D. UNION clause# UNION combines all the rows that are in the result set from the first query with those that are in the result set for the second query. The following is an example of one of the simplest possible UNION clauses. It selects the value 13 and combines this result set with a second query that selects the value 42: SELECT 13 UNION SELECT 42; _col0 ------- 13 42 (2 rows) The following query demonstrates the difference between UNION and UNION ALL. It selects the value 13 and combines this result set with a second query that selects the values 42 and 13: SELECT 13 UNION SELECT * FROM (VALUES 42, 13); _col0 ------- 13 42 (2 rows) SELECT 13 UNION ALL SELECT * FROM (VALUES 42, 13); _col0 ------- 13 42 13 (2 rows) INTERSECT clause# INTERSECT returns only the rows that are in the result sets of both the first and the second queries. The following is an example of one of the simplest possible INTERSECT clauses. It selects the values 13 and 42 and combines this result set with a second query that selects the value 13. Since 42 is only in the result set of the first query, it is not included in the final results.: SELECT * FROM (VALUES 13, 42) INTERSECT SELECT 13; _col0 ------- 13 (2 rows) EXCEPT clause# EXCEPT returns the rows that are in the result set of the first query, but not the second. The following is an example of one of the simplest possible EXCEPT clauses. It selects the values 13 and 42 and combines this result set with a second query that selects the value 13. Since 13 is also in the result set of the second query, it is not included in the final result.: SELECT * FROM (VALUES 13, 42) EXCEPT SELECT 13; _col0 ------- 42 (2 rows) ORDER BY clause# The ORDER BY clause is used to sort a result set by one or more output expressions: ORDER BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] Each expression may be composed of output columns, or it may be an ordinal number selecting an output column by position, starting at one. The ORDER BY clause is evaluated after any GROUP BY or HAVING clause, and before any OFFSET, LIMIT or FETCH FIRST clause. The default null ordering is NULLS LAST, regardless of the ordering direction. Note that, following the SQL specification, an ORDER BY clause only affects the order of rows for queries that immediately contain the clause. Trino follows that specification, and drops redundant usage of the clause to avoid negative performance impacts. In the following example, the clause only applies to the select statement. INSERT INTO some_table SELECT * FROM another_table ORDER BY field; Since tables in SQL are inherently unordered, and the ORDER BY clause in this case does not result in any difference, but negatively impacts performance of running the overall insert statement, Trino skips the sort operation. Another example where the ORDER BY clause is redundant, and does not affect the outcome of the overall statement, is a nested query: SELECT * FROM some_table JOIN (SELECT * FROM another_table ORDER BY field) u ON some_table.key = u.key; More background information and details can be found in a blog post about this optimization. OFFSET clause# The OFFSET clause is used to discard a number of leading rows from the result set: OFFSET count [ ROW | ROWS ] If the ORDER BY clause is present, the OFFSET clause is evaluated over a sorted result set, and the set remains sorted after the leading rows are discarded: SELECT name FROM nation ORDER BY name OFFSET 22; name ---------------- UNITED KINGDOM UNITED STATES VIETNAM (3 rows) Otherwise, it is arbitrary which rows are discarded. If the count specified in the OFFSET clause equals or exceeds the size of the result set, the final result is empty. LIMIT or FETCH FIRST clause# The LIMIT or FETCH FIRST clause restricts the number of rows in the result set. LIMIT { count | ALL } FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } { ONLY | WITH TIES } The following example queries a large table, but the LIMIT clause restricts the output to only have five rows (because the query lacks an ORDER BY, exactly which rows are returned is arbitrary): SELECT orderdate FROM orders LIMIT 5; orderdate ------------ 1994-07-25 1993-11-12 1992-10-06 1994-01-04 1997-12-28 (5 rows) LIMIT ALL is the same as omitting the LIMIT clause. The FETCH FIRST clause supports either the FIRST or NEXT keywords and the ROW or ROWS keywords. These keywords are equivalent and the choice of keyword has no effect on query execution. If the count is not specified in the FETCH FIRST clause, it defaults to 1: SELECT orderdate FROM orders FETCH FIRST ROW ONLY; orderdate ------------ 1994-02-12 (1 row) If the OFFSET clause is present, the LIMIT or FETCH FIRST clause is evaluated after the OFFSET clause: SELECT * FROM (VALUES 5, 2, 4, 1, 3) t(x) ORDER BY x OFFSET 2 LIMIT 2; x --- 3 4 (2 rows) For the FETCH FIRST clause, the argument ONLY or WITH TIES controls which rows are included in the result set. If the argument ONLY is specified, the result set is limited to the exact number of leading rows determined by the count. If the argument WITH TIES is specified, it is required that the ORDER BY clause be present. The result set consists of the same set of leading rows and all of the rows in the same peer group as the last of them (‘ties’) as established by the ordering in the ORDER BY clause. The result set is sorted: SELECT name, regionkey FROM nation ORDER BY regionkey FETCH FIRST ROW WITH TIES; name | regionkey ------------+----------- ETHIOPIA | 0 MOROCCO | 0 KENYA | 0 ALGERIA | 0 MOZAMBIQUE | 0 (5 rows) TABLESAMPLE# There are multiple sample methods: BERNOULLIEach row is selected to be in the table sample with a probability of the sample percentage. When a table is sampled using the Bernoulli method, all physical blocks of the table are scanned and certain rows are skipped (based on a comparison between the sample percentage and a random value calculated at runtime). The probability of a row being included in the result is independent from any other row. This does not reduce the time required to read the sampled table from disk. It may have an impact on the total query time if the sampled output is processed further. SYSTEMThis sampling method divides the table into logical segments of data and samples the table at this granularity. This sampling method either selects all the rows from a particular segment of data or skips it (based on a comparison between the sample percentage and a random value calculated at runtime). The rows selected in a system sampling will be dependent on which connector is used. For example, when used with Hive, it is dependent on how the data is laid out on HDFS. This method does not guarantee independent sampling probabilities. Note Neither of the two methods allow deterministic bounds on the number of rows returned. Examples: SELECT * FROM users TABLESAMPLE BERNOULLI (50); SELECT * FROM users TABLESAMPLE SYSTEM (75); Using sampling with joins: SELECT o.*, i.* FROM orders o TABLESAMPLE SYSTEM (10) JOIN lineitem i TABLESAMPLE BERNOULLI (40) ON o.orderkey = i.orderkey; UNNEST# UNNEST can be used to expand an ARRAY or MAP into a relation. Arrays are expanded into a single column: SELECT * FROM UNNEST(ARRAY[1,2]) AS t(number); number -------- 1 2 (2 rows) Maps are expanded into two columns (key, value): SELECT * FROM UNNEST( map_from_entries( ARRAY[ ('SQL',1974), ('Java', 1995) ] ) ) AS t(language, first_appeared_year); language | first_appeared_year ----------+--------------------- SQL | 1974 Java | 1995 (2 rows) UNNEST can be used in combination with an ARRAY of ROW structures for expanding each field of the ROW into a corresponding column: SELECT * FROM UNNEST( ARRAY[ ROW('Java', 1995), ROW('SQL' , 1974)], ARRAY[ ROW(false), ROW(true)] ) as t(language,first_appeared_year,declarative); language | first_appeared_year | declarative ----------+---------------------+------------- Java | 1995 | false SQL | 1974 | true (2 rows) UNNEST can optionally have a WITH ORDINALITY clause, in which case an additional ordinality column is added to the end: SELECT a, b, rownumber FROM UNNEST ( ARRAY[2, 5], ARRAY[7, 8, 9] ) WITH ORDINALITY AS t(a, b, rownumber); a | b | rownumber ------+---+----------- 2 | 7 | 1 5 | 8 | 2 NULL | 9 | 3 (3 rows) UNNEST returns zero entries when the array/map is empty: SELECT * FROM UNNEST (ARRAY[]) AS t(value); value ------- (0 rows) UNNEST returns zero entries when the array/map is null: SELECT * FROM UNNEST (CAST(null AS ARRAY(integer))) AS t(number); number -------- (0 rows) UNNEST is normally used with a JOIN, and can reference columns from relations on the left side of the join: SELECT student, score FROM ( VALUES ('John', ARRAY[7, 10, 9]), ('Mary', ARRAY[4, 8, 9]) ) AS tests (student, scores) CROSS JOIN UNNEST(scores) AS t(score); student | score ---------+------- John | 7 John | 10 John | 9 Mary | 4 Mary | 8 Mary | 9 (6 rows) UNNEST can also be used with multiple arguments, in which case they are expanded into multiple columns, with as many rows as the highest cardinality argument (the other columns are padded with nulls): SELECT numbers, animals, n, a FROM ( VALUES (ARRAY[2, 5], ARRAY['dog', 'cat', 'bird']), (ARRAY[7, 8, 9], ARRAY['cow', 'pig']) ) AS x (numbers, animals) CROSS JOIN UNNEST(numbers, animals) AS t (n, a); numbers | animals | n | a -----------+------------------+------+------ [2, 5] | [dog, cat, bird] | 2 | dog [2, 5] | [dog, cat, bird] | 5 | cat [2, 5] | [dog, cat, bird] | NULL | bird [7, 8, 9] | [cow, pig] | 7 | cow [7, 8, 9] | [cow, pig] | 8 | pig [7, 8, 9] | [cow, pig] | 9 | NULL (6 rows) LEFT JOIN is preferable in order to avoid losing the row containing the array/map field in question when referenced columns from relations on the left side of the join can be empty or have NULL values: SELECT runner, checkpoint FROM ( VALUES ('Joe', ARRAY[10, 20, 30, 42]), ('Roger', ARRAY[10]), ('Dave', ARRAY[]), ('Levi', NULL) ) AS marathon (runner, checkpoints) LEFT JOIN UNNEST(checkpoints) AS t(checkpoint) ON TRUE; runner | checkpoint --------+------------ Joe | 10 Joe | 20 Joe | 30 Joe | 42 Roger | 10 Dave | NULL Levi | NULL (7 rows) Note that in case of using LEFT JOIN the only condition supported by the current implementation is ON TRUE. JSON_TABLE# JSON_TABLE transforms JSON data into a relational table format. Like UNNEST and LATERAL, use JSON_TABLE in the FROM clause of a SELECT statement. For more information, see JSON_TABLE. Joins# Joins allow you to combine data from multiple relations. CROSS JOIN# A cross join returns the Cartesian product (all combinations) of two relations. Cross joins can either be specified using the explit CROSS JOIN syntax or by specifying multiple relations in the FROM clause. Both of the following queries are equivalent: SELECT * FROM nation CROSS JOIN region; SELECT * FROM nation, region; The nation table contains 25 rows and the region table contains 5 rows, so a cross join between the two tables produces 125 rows: SELECT n.name AS nation, r.name AS region FROM nation AS n CROSS JOIN region AS r ORDER BY 1, 2; nation | region ----------------+------------- ALGERIA | AFRICA ALGERIA | AMERICA ALGERIA | ASIA ALGERIA | EUROPE ALGERIA | MIDDLE EAST ARGENTINA | AFRICA ARGENTINA | AMERICA ... (125 rows) LATERAL# Subqueries appearing in the FROM clause can be preceded by the keyword LATERAL. This allows them to reference columns provided by preceding FROM items. A LATERAL join can appear at the top level in the FROM list, or anywhere within a parenthesized join tree. In the latter case, it can also refer to any items that are on the left-hand side of a JOIN for which it is on the right-hand side. When a FROM item contains LATERAL cross-references, evaluation proceeds as follows: for each row of the FROM item providing the cross-referenced columns, the LATERAL item is evaluated using that row set’s values of the columns. The resulting rows are joined as usual with the rows they were computed from. This is repeated for set of rows from the column source tables. LATERAL is primarily useful when the cross-referenced column is necessary for computing the rows to be joined: SELECT name, x, y FROM nation CROSS JOIN LATERAL (SELECT name || ' :-' AS x) CROSS JOIN LATERAL (SELECT x || ')' AS y); Qualifying column names# When two relations in a join have columns with the same name, the column references must be qualified using the relation alias (if the relation has an alias), or with the relation name: SELECT nation.name, region.name FROM nation CROSS JOIN region; SELECT n.name, r.name FROM nation AS n CROSS JOIN region AS r; SELECT n.name, r.name FROM nation n CROSS JOIN region r; The following query will fail with the error Column 'name' is ambiguous: SELECT name FROM nation CROSS JOIN region; Subqueries# A subquery is an expression which is composed of a query. The subquery is correlated when it refers to columns outside of the subquery. Logically, the subquery will be evaluated for each row in the surrounding query. The referenced columns will thus be constant during any single evaluation of the subquery. Note Support for correlated subqueries is limited. Not every standard form is supported. EXISTS# The EXISTS predicate determines if a subquery returns any rows: SELECT name FROM nation WHERE EXISTS ( SELECT * FROM region WHERE region.regionkey = nation.regionkey ); IN# The IN predicate determines if any values produced by the subquery are equal to the provided expression. The result of IN follows the standard rules for nulls. The subquery must produce exactly one column: SELECT name FROM nation WHERE regionkey IN ( SELECT regionkey FROM region WHERE name = 'AMERICA' OR name = 'AFRICA' ); Scalar subquery# A scalar subquery is a non-correlated subquery that returns zero or one row. It is an error for the subquery to produce more than one row. The returned value is NULL if the subquery produces no rows: SELECT name FROM nation WHERE regionkey = (SELECT max(regionkey) FROM region); Note Currently only single column can be returned from the scalar subquery.

#### Code Examples

```
[ WITH SESSION [ name = expression [, ...] ]
[ WITH [ FUNCTION udf ] [, ...] ]
[ WITH [ RECURSIVE ] with_query [, ...] ]
SELECT [ ALL | DISTINCT ] select_expression [, ...]
[ FROM from_item [, ...] ]
[ WHERE condition ]
[ GROUP BY [ ALL | DISTINCT ] grouping_element [, ...] ]
[ HAVING condition]
[ WINDOW window_definition_list]
[ { UNION | INTERSECT | EXCEPT } [ ALL | DISTINCT ] select ]
[ ORDER BY expression [ ASC | DESC ] [, ...] ]
[ OFFSET count [ ROW | ROWS ] ]
[ LIMIT { count | ALL } ]
[ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } { ONLY | WITH TIES } ]
```
```
table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ]
```
```
from_item join_type from_item
  [ ON join_condition | USING ( join_column [, ...] ) ]
```
```
table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ]
  MATCH_RECOGNIZE pattern_recognition_specification
    [ [ AS ] alias [ ( column_alias [, ...] ) ] ]
```
```
TABLE (table_function_invocation) [ [ AS ] alias [ ( column_alias [, ...] ) ] ]
```
```
[ INNER ] JOIN
LEFT [ OUTER ] JOIN
RIGHT [ OUTER ] JOIN
FULL [ OUTER ] JOIN
CROSS JOIN
```
```
()
expression
GROUPING SETS ( ( column [, ...] ) [, ...] )
CUBE ( column [, ...] )
ROLLUP ( column [, ...] )
```
```
WITH
  SESSION
    query_max_execution_time='2h',
    example.query_partition_filter_required=true
SELECT *
FROM example.default.thetable
LIMIT 100;
```
```
WITH 
  FUNCTION hello(name varchar)
    RETURNS varchar
    RETURN format('Hello %s!', 'name'),
  FUNCTION bye(name varchar)
    RETURNS varchar
    RETURN format('Bye %s!', 'name')
SELECT hello('Finn') || ' and ' || bye('Joe');
-- Hello Finn! and Bye Joe!
```
```
SELECT a, b
FROM (
  SELECT a, MAX(b) AS b FROM t GROUP BY a
) AS x;

WITH x AS (SELECT a, MAX(b) AS b FROM t GROUP BY a)
SELECT a, b FROM x;
```
```
WITH
  t1 AS (SELECT a, MAX(b) AS b FROM x GROUP BY a),
  t2 AS (SELECT a, AVG(d) AS d FROM y GROUP BY a)
SELECT t1.*, t2.*
FROM t1
JOIN t2 ON t1.a = t2.a;
```
```
WITH
  x AS (SELECT a FROM t),
  y AS (SELECT a AS b FROM x),
  z AS (SELECT b AS c FROM y)
SELECT c FROM z;
```
```
WITH RECURSIVE t(n) AS (
    VALUES (1)
    UNION ALL
    SELECT n + 1 FROM t WHERE n < 4
)
SELECT sum(n) FROM t;
```
```
SELECT [ ALL | DISTINCT ] select_expression [, ...]
```
```
expression [ [ AS ] column_alias ]
```
```
row_expression.* [ AS ( column_alias [, ...] ) ]
```
```
relation.*
```
```
*
```
```
SELECT (CAST(ROW(1, true) AS ROW(field1 bigint, field2 boolean))).* AS (alias1, alias2);
```
```
alias1 | alias2
--------+--------
      1 | true
(1 row)
```
```
SELECT (CAST(ROW(1, true) AS ROW(field1 bigint, field2 boolean))).*;
```
```
field1 | field2
--------+--------
      1 | true
(1 row)
```
```
SELECT (ROW(1, true)).*;
```
```
_col0 | _col1
-------+-------
     1 | true
(1 row)
```
```
SELECT count(*), nationkey FROM customer GROUP BY 2;

SELECT count(*), nationkey FROM customer GROUP BY nationkey;
```
```
SELECT count(*) FROM customer GROUP BY mktsegment;
```
```
_col0
-------
 29968
 30142
 30189
 29949
 29752
(5 rows)
```
```
SELECT * FROM shipping;
```
```
origin_state | origin_zip | destination_state | destination_zip | package_weight
--------------+------------+-------------------+-----------------+----------------
 California   |      94131 | New Jersey        |            8648 |             13
 California   |      94131 | New Jersey        |            8540 |             42
 New Jersey   |       7081 | Connecticut       |            6708 |            225
 California   |      90210 | Connecticut       |            6927 |           1337
 California   |      94131 | Colorado          |           80302 |              5
 New York     |      10002 | New Jersey        |            8540 |              3
(6 rows)
```
```
SELECT origin_state, origin_zip, destination_state, sum(package_weight)
FROM shipping
GROUP BY GROUPING SETS (
    (origin_state),
    (origin_state, origin_zip),
    (destination_state));
```
```
origin_state | origin_zip | destination_state | _col0
--------------+------------+-------------------+-------
 New Jersey   | NULL       | NULL              |   225
 California   | NULL       | NULL              |  1397
 New York     | NULL       | NULL              |     3
 California   |      90210 | NULL              |  1337
 California   |      94131 | NULL              |    60
 New Jersey   |       7081 | NULL              |   225
 New York     |      10002 | NULL              |     3
 NULL         | NULL       | Colorado          |     5
 NULL         | NULL       | New Jersey        |    58
 NULL         | NULL       | Connecticut       |  1562
(10 rows)
```
```
SELECT origin_state, NULL, NULL, sum(package_weight)
FROM shipping GROUP BY origin_state

UNION ALL

SELECT origin_state, origin_zip, NULL, sum(package_weight)
FROM shipping GROUP BY origin_state, origin_zip

UNION ALL

SELECT NULL, NULL, destination_state, sum(package_weight)
FROM shipping GROUP BY destination_state;
```
```
SELECT origin_state, destination_state, sum(package_weight)
FROM shipping
GROUP BY CUBE (origin_state, destination_state);
```
```
SELECT origin_state, destination_state, sum(package_weight)
FROM shipping
GROUP BY GROUPING SETS (
    (origin_state, destination_state),
    (origin_state),
    (destination_state),
    ()
);
```
```
origin_state | destination_state | _col0
--------------+-------------------+-------
 California   | New Jersey        |    55
 California   | Colorado          |     5
 New York     | New Jersey        |     3
 New Jersey   | Connecticut       |   225
 California   | Connecticut       |  1337
 California   | NULL              |  1397
 New York     | NULL              |     3
 New Jersey   | NULL              |   225
 NULL         | New Jersey        |    58
 NULL         | Connecticut       |  1562
 NULL         | Colorado          |     5
 NULL         | NULL              |  1625
(12 rows)
```
```
SELECT origin_state, origin_zip, sum(package_weight)
FROM shipping
GROUP BY ROLLUP (origin_state, origin_zip);
```
```
origin_state | origin_zip | _col2
--------------+------------+-------
 California   |      94131 |    60
 California   |      90210 |  1337
 New Jersey   |       7081 |   225
 New York     |      10002 |     3
 California   | NULL       |  1397
 New York     | NULL       |     3
 New Jersey   | NULL       |   225
 NULL         | NULL       |  1625
(8 rows)
```
```
SELECT origin_state, origin_zip, sum(package_weight)
FROM shipping
GROUP BY GROUPING SETS ((origin_state, origin_zip), (origin_state), ());
```
```
SELECT origin_state, destination_state, origin_zip, sum(package_weight)
FROM shipping
GROUP BY
    GROUPING SETS ((origin_state, destination_state)),
    ROLLUP (origin_zip);
```
```
SELECT origin_state, destination_state, origin_zip, sum(package_weight)
FROM shipping
GROUP BY
    GROUPING SETS ((origin_state, destination_state)),
    GROUPING SETS ((origin_zip), ());
```
```
SELECT origin_state, destination_state, origin_zip, sum(package_weight)
FROM shipping
GROUP BY GROUPING SETS (
    (origin_state, destination_state, origin_zip),
    (origin_state, destination_state)
);
```
```
origin_state | destination_state | origin_zip | _col3
--------------+-------------------+------------+-------
 New York     | New Jersey        |      10002 |     3
 California   | New Jersey        |      94131 |    55
 New Jersey   | Connecticut       |       7081 |   225
 California   | Connecticut       |      90210 |  1337
 California   | Colorado          |      94131 |     5
 New York     | New Jersey        | NULL       |     3
 New Jersey   | Connecticut       | NULL       |   225
 California   | Colorado          | NULL       |     5
 California   | Connecticut       | NULL       |  1337
 California   | New Jersey        | NULL       |    55
(10 rows)
```
```
SELECT origin_state, destination_state, origin_zip, sum(package_weight)
FROM shipping
GROUP BY ALL
    CUBE (origin_state, destination_state),
    ROLLUP (origin_state, origin_zip);
```
```
SELECT origin_state, destination_state, origin_zip, sum(package_weight)
FROM shipping
GROUP BY GROUPING SETS (
    (origin_state, destination_state, origin_zip),
    (origin_state, origin_zip),
    (origin_state, destination_state, origin_zip),
    (origin_state, origin_zip),
    (origin_state, destination_state),
    (origin_state),
    (origin_state, destination_state),
    (origin_state),
    (origin_state, destination_state),
    (origin_state),
    (destination_state),
    ()
);
```
```
SELECT origin_state, destination_state, origin_zip, sum(package_weight)
FROM shipping
GROUP BY DISTINCT
    CUBE (origin_state, destination_state),
    ROLLUP (origin_state, origin_zip);
```
```
SELECT origin_state, destination_state, origin_zip, sum(package_weight)
FROM shipping
GROUP BY GROUPING SETS (
    (origin_state, destination_state, origin_zip),
    (origin_state, origin_zip),
    (origin_state, destination_state),
    (origin_state),
    (destination_state),
    ()
);
```
```
SELECT origin_state, origin_zip, destination_state, sum(package_weight),
       grouping(origin_state, origin_zip, destination_state)
FROM shipping
GROUP BY GROUPING SETS (
    (origin_state),
    (origin_state, origin_zip),
    (destination_state)
);
```
```
origin_state | origin_zip | destination_state | _col3 | _col4
--------------+------------+-------------------+-------+-------
California   | NULL       | NULL              |  1397 |     3
New Jersey   | NULL       | NULL              |   225 |     3
New York     | NULL       | NULL              |     3 |     3
California   |      94131 | NULL              |    60 |     1
New Jersey   |       7081 | NULL              |   225 |     1
California   |      90210 | NULL              |  1337 |     1
New York     |      10002 | NULL              |     3 |     1
NULL         | NULL       | New Jersey        |    58 |     6
NULL         | NULL       | Connecticut       |  1562 |     6
NULL         | NULL       | Colorado          |     5 |     6
(10 rows)
```
```
SELECT count(*), mktsegment, nationkey,
       CAST(sum(acctbal) AS bigint) AS totalbal
FROM customer
GROUP BY mktsegment, nationkey
HAVING sum(acctbal) > 5700000
ORDER BY totalbal DESC;
```
```
_col0 | mktsegment | nationkey | totalbal
-------+------------+-----------+----------
  1272 | AUTOMOBILE |        19 |  5856939
  1253 | FURNITURE  |        14 |  5794887
  1248 | FURNITURE  |         9 |  5784628
  1243 | FURNITURE  |        12 |  5757371
  1231 | HOUSEHOLD  |         3 |  5753216
  1251 | MACHINERY  |         2 |  5719140
  1247 | FURNITURE  |         8 |  5701952
(7 rows)
```
```
SELECT orderkey, clerk, totalprice,
      rank() OVER w AS rnk
FROM orders
WINDOW w AS (PARTITION BY clerk ORDER BY totalprice DESC)
ORDER BY count() OVER w, clerk, rnk
```
```
window_name AS (window_specification)
```
```
query UNION [ALL | DISTINCT] query
```
```
query INTERSECT [ALL | DISTINCT] query
```
```
query EXCEPT [ALL | DISTINCT] query
```
```
SELECT 13
UNION
SELECT 42;
```
```
_col0
-------
    13
    42
(2 rows)
```
```
SELECT 13
UNION
SELECT * FROM (VALUES 42, 13);
```
```
_col0
-------
    13
    42
(2 rows)
```
```
SELECT 13
UNION ALL
SELECT * FROM (VALUES 42, 13);
```
```
_col0
-------
    13
    42
    13
(2 rows)
```
```
SELECT * FROM (VALUES 13, 42)
INTERSECT
SELECT 13;
```
```
_col0
-------
    13
(2 rows)
```
```
SELECT * FROM (VALUES 13, 42)
EXCEPT
SELECT 13;
```
```
_col0
-------
   42
(2 rows)
```
```
ORDER BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...]
```
```
INSERT INTO some_table
SELECT * FROM another_table
ORDER BY field;
```
```
SELECT *
FROM some_table
    JOIN (SELECT * FROM another_table ORDER BY field) u
    ON some_table.key = u.key;
```
```
OFFSET count [ ROW | ROWS ]
```
```
SELECT name FROM nation ORDER BY name OFFSET 22;
```
```
name
----------------
 UNITED KINGDOM
 UNITED STATES
 VIETNAM
(3 rows)
```
```
LIMIT { count | ALL }
```
```
FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } { ONLY | WITH TIES }
```
```
SELECT orderdate FROM orders LIMIT 5;
```
```
orderdate
------------
 1994-07-25
 1993-11-12
 1992-10-06
 1994-01-04
 1997-12-28
(5 rows)
```
```
SELECT orderdate FROM orders FETCH FIRST ROW ONLY;
```
```
orderdate
------------
 1994-02-12
(1 row)
```
```
SELECT * FROM (VALUES 5, 2, 4, 1, 3) t(x) ORDER BY x OFFSET 2 LIMIT 2;
```
```
x
---
 3
 4
(2 rows)
```
```
SELECT name, regionkey
FROM nation
ORDER BY regionkey FETCH FIRST ROW WITH TIES;
```
```
name    | regionkey
------------+-----------
 ETHIOPIA   |         0
 MOROCCO    |         0
 KENYA      |         0
 ALGERIA    |         0
 MOZAMBIQUE |         0
(5 rows)
```
```
SELECT *
FROM users TABLESAMPLE BERNOULLI (50);

SELECT *
FROM users TABLESAMPLE SYSTEM (75);
```
```
SELECT o.*, i.*
FROM orders o TABLESAMPLE SYSTEM (10)
JOIN lineitem i TABLESAMPLE BERNOULLI (40)
  ON o.orderkey = i.orderkey;
```
```
SELECT * FROM UNNEST(ARRAY[1,2]) AS t(number);
```
```
number
--------
      1
      2
(2 rows)
```
```
SELECT * FROM UNNEST(
        map_from_entries(
            ARRAY[
                ('SQL',1974),
                ('Java', 1995)
            ]
        )
) AS t(language, first_appeared_year);
```
```
language | first_appeared_year
----------+---------------------
 SQL      |                1974
 Java     |                1995
(2 rows)
```
```
SELECT *
FROM UNNEST(
        ARRAY[
            ROW('Java',  1995),
            ROW('SQL' , 1974)],
        ARRAY[
            ROW(false),
            ROW(true)]
) as t(language,first_appeared_year,declarative);
```
```
language | first_appeared_year | declarative
----------+---------------------+-------------
 Java     |                1995 | false
 SQL      |                1974 | true
(2 rows)
```
```
SELECT a, b, rownumber
FROM UNNEST (
    ARRAY[2, 5],
    ARRAY[7, 8, 9]
     ) WITH ORDINALITY AS t(a, b, rownumber);
```
```
a   | b | rownumber
------+---+-----------
    2 | 7 |         1
    5 | 8 |         2
 NULL | 9 |         3
(3 rows)
```
```
SELECT * FROM UNNEST (ARRAY[]) AS t(value);
```
```
value
-------
(0 rows)
```
```
SELECT * FROM UNNEST (CAST(null AS ARRAY(integer))) AS t(number);
```
```
number
--------
(0 rows)
```
```
SELECT student, score
FROM (
   VALUES
      ('John', ARRAY[7, 10, 9]),
      ('Mary', ARRAY[4, 8, 9])
) AS tests (student, scores)
CROSS JOIN UNNEST(scores) AS t(score);
```
```
student | score
---------+-------
 John    |     7
 John    |    10
 John    |     9
 Mary    |     4
 Mary    |     8
 Mary    |     9
(6 rows)
```
```
SELECT numbers, animals, n, a
FROM (
  VALUES
    (ARRAY[2, 5], ARRAY['dog', 'cat', 'bird']),
    (ARRAY[7, 8, 9], ARRAY['cow', 'pig'])
) AS x (numbers, animals)
CROSS JOIN UNNEST(numbers, animals) AS t (n, a);
```
```
numbers  |     animals      |  n   |  a
-----------+------------------+------+------
 [2, 5]    | [dog, cat, bird] |    2 | dog
 [2, 5]    | [dog, cat, bird] |    5 | cat
 [2, 5]    | [dog, cat, bird] | NULL | bird
 [7, 8, 9] | [cow, pig]       |    7 | cow
 [7, 8, 9] | [cow, pig]       |    8 | pig
 [7, 8, 9] | [cow, pig]       |    9 | NULL
(6 rows)
```
```
SELECT runner, checkpoint
FROM (
   VALUES
      ('Joe', ARRAY[10, 20, 30, 42]),
      ('Roger', ARRAY[10]),
      ('Dave', ARRAY[]),
      ('Levi', NULL)
) AS marathon (runner, checkpoints)
LEFT JOIN UNNEST(checkpoints) AS t(checkpoint) ON TRUE;
```
```
runner | checkpoint
--------+------------
 Joe    |         10
 Joe    |         20
 Joe    |         30
 Joe    |         42
 Roger  |         10
 Dave   |       NULL
 Levi   |       NULL
(7 rows)
```
```
SELECT *
FROM nation
CROSS JOIN region;

SELECT *
FROM nation, region;
```
```
SELECT n.name AS nation, r.name AS region
FROM nation AS n
CROSS JOIN region AS r
ORDER BY 1, 2;
```
```
nation     |   region
----------------+-------------
 ALGERIA        | AFRICA
 ALGERIA        | AMERICA
 ALGERIA        | ASIA
 ALGERIA        | EUROPE
 ALGERIA        | MIDDLE EAST
 ARGENTINA      | AFRICA
 ARGENTINA      | AMERICA
...
(125 rows)
```
```
SELECT name, x, y
FROM nation
CROSS JOIN LATERAL (SELECT name || ' :-' AS x)
CROSS JOIN LATERAL (SELECT x || ')' AS y);
```
```
SELECT nation.name, region.name
FROM nation
CROSS JOIN region;

SELECT n.name, r.name
FROM nation AS n
CROSS JOIN region AS r;

SELECT n.name, r.name
FROM nation n
CROSS JOIN region r;
```
```
SELECT name
FROM nation
CROSS JOIN region;
```
```
SELECT name
FROM nation
WHERE EXISTS (
     SELECT *
     FROM region
     WHERE region.regionkey = nation.regionkey
);
```
```
SELECT name
FROM nation
WHERE regionkey IN (
     SELECT regionkey
     FROM region
     WHERE name = 'AMERICA' OR name = 'AFRICA'
);
```
```
SELECT name
FROM nation
WHERE regionkey = (SELECT max(regionkey) FROM region);
```


---

### TRUNCATE
Source: https://trino.io/docs/current/sql/truncate.html

TRUNCATE# Synopsis# TRUNCATE TABLE table_name Description# Delete all rows from a table. Examples# Truncate the table orders: TRUNCATE TABLE orders;

#### Code Examples

```
TRUNCATE TABLE table_name
```
```
TRUNCATE TABLE orders;
```


---

### UPDATE
Source: https://trino.io/docs/current/sql/update.html

UPDATE# Synopsis# UPDATE table_name SET [ ( column = expression [, ... ] ) ] [ WHERE condition ] Description# Update selected columns values in existing rows in a table. The columns named in the column = expression assignments will be updated for all rows that match the WHERE condition. The values of all column update expressions for a matching row are evaluated before any column value is changed. When the type of the expression and the type of the column differ, the usual implicit CASTs, such as widening numeric fields, are applied to the UPDATE expression values. Examples# Update the status of all purchases that haven’t been assigned a ship date: UPDATE purchases SET status = 'OVERDUE' WHERE ship_date IS NULL; Update the account manager and account assign date for all customers: UPDATE customers SET account_manager = 'John Henry', assign_date = now(); Update the manager to be the name of the employee who matches the manager ID: UPDATE new_hires SET manager = ( SELECT e.name FROM employees e WHERE e.employee_id = new_hires.manager_id ); Limitations# Some connectors have limited or no support for UPDATE. See connector documentation for more details.

#### Code Examples

```
UPDATE table_name SET [ ( column = expression [, ... ] ) ] [ WHERE condition ]
```
```
UPDATE
  purchases
SET
  status = 'OVERDUE'
WHERE
  ship_date IS NULL;
```
```
UPDATE
  customers
SET
  account_manager = 'John Henry',
  assign_date = now();
```
```
UPDATE
  new_hires
SET
  manager = (
    SELECT
      e.name
    FROM
      employees e
    WHERE
      e.employee_id = new_hires.manager_id
  );
```


---

### VALUES
Source: https://trino.io/docs/current/sql/values.html

VALUES# Synopsis# VALUES row [, ...] where row is a single expression or ( column_expression [, ...] ) Description# Defines a literal inline table. VALUES can be used anywhere a query can be used (e.g., the FROM clause of a SELECT, an INSERT, or even at the top level). VALUES creates an anonymous table without column names, but the table and columns can be named using an AS clause with column aliases. Examples# Return a table with one column and three rows: VALUES 1, 2, 3 Return a table with two columns and three rows: VALUES (1, 'a'), (2, 'b'), (3, 'c') Return table with column id and name: SELECT * FROM ( VALUES (1, 'a'), (2, 'b'), (3, 'c') ) AS t (id, name) Create a new table with column id and name: CREATE TABLE example AS SELECT * FROM ( VALUES (1, 'a'), (2, 'b'), (3, 'c') ) AS t (id, name) See also# INSERT, SELECT

#### Code Examples

```
VALUES row [, ...]
```
```
( column_expression [, ...] )
```
```
VALUES 1, 2, 3
```
```
VALUES
    (1, 'a'),
    (2, 'b'),
    (3, 'c')
```
```
SELECT * FROM (
    VALUES
        (1, 'a'),
        (2, 'b'),
        (3, 'c')
) AS t (id, name)
```
```
CREATE TABLE example AS
SELECT * FROM (
    VALUES
        (1, 'a'),
        (2, 'b'),
        (3, 'c')
) AS t (id, name)
```


---

### COMMIT
Source: https://trino.io/docs/current/sql/commit.html

COMMIT# Synopsis# COMMIT [ WORK ] Description# Commit the current transaction. Examples# COMMIT; COMMIT WORK; See also# ROLLBACK, START TRANSACTION

#### Code Examples

```
COMMIT [ WORK ]
```
```
COMMIT;
COMMIT WORK;
```


---

### ROLLBACK
Source: https://trino.io/docs/current/sql/rollback.html

ROLLBACK# Synopsis# ROLLBACK [ WORK ] Description# Rollback the current transaction. Examples# ROLLBACK; ROLLBACK WORK; See also# COMMIT, START TRANSACTION

#### Code Examples

```
ROLLBACK [ WORK ]
```
```
ROLLBACK;
ROLLBACK WORK;
```


---

### START TRANSACTION
Source: https://trino.io/docs/current/sql/start-transaction.html

START TRANSACTION# Synopsis# START TRANSACTION [ mode [, ...] ] where mode is one of ISOLATION LEVEL { READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE } READ { ONLY | WRITE } Description# Start a new transaction for the current session. Examples# START TRANSACTION; START TRANSACTION ISOLATION LEVEL REPEATABLE READ; START TRANSACTION READ WRITE; START TRANSACTION ISOLATION LEVEL READ COMMITTED, READ ONLY; START TRANSACTION READ WRITE, ISOLATION LEVEL SERIALIZABLE; See also# COMMIT, ROLLBACK

#### Code Examples

```
START TRANSACTION [ mode [, ...] ]
```
```
ISOLATION LEVEL { READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE }
READ { ONLY | WRITE }
```
```
START TRANSACTION;
START TRANSACTION ISOLATION LEVEL REPEATABLE READ;
START TRANSACTION READ WRITE;
START TRANSACTION ISOLATION LEVEL READ COMMITTED, READ ONLY;
START TRANSACTION READ WRITE, ISOLATION LEVEL SERIALIZABLE;
```


---

### RESET SESSION
Source: https://trino.io/docs/current/sql/reset-session.html

RESET SESSION# Synopsis# RESET SESSION name RESET SESSION catalog.name Description# Reset a session property value to the default value. Examples# RESET SESSION query_max_run_time; RESET SESSION hive.optimized_reader_enabled; See also# SET SESSION, SHOW SESSION

#### Code Examples

```
RESET SESSION name
RESET SESSION catalog.name
```
```
RESET SESSION query_max_run_time;
RESET SESSION hive.optimized_reader_enabled;
```


---

### RESET SESSION AUTHORIZATION
Source: https://trino.io/docs/current/sql/reset-session-authorization.html

RESET SESSION AUTHORIZATION# Synopsis# RESET SESSION AUTHORIZATION Description# Resets the current authorization user back to the original user. The original user is usually the authenticated user (principal), or it can be the session user when the session user is provided by the client. See also# SET SESSION AUTHORIZATION

#### Code Examples

```
RESET SESSION AUTHORIZATION
```


---

### SET PATH
Source: https://trino.io/docs/current/sql/set-path.html

SET PATH# Synopsis# SET PATH path-element[, ...] Description# Define a collection of paths to functions or table functions in specific catalogs and schemas for the current session. Each path-element uses a period-separated syntax to specify the catalog name and schema location <catalog>.<schema> of the function, or only the schema location <schema> in the current catalog. The current catalog is set with USE, or as part of a client tool connection. Catalog and schema must exist. Examples# The following example sets a path to access functions in the system schema of the example catalog: SET PATH example.system; The catalog uses the PostgreSQL connector, and you can therefore use the query table function directly, without the full catalog and schema qualifiers: SELECT * FROM TABLE( query( query => 'SELECT * FROM tpch.nation' ) ); See also# USE SQL environment properties

#### Code Examples

```
SET PATH path-element[, ...]
```
```
SET PATH example.system;
```
```
SELECT
  *
FROM
  TABLE(
    query(
      query => 'SELECT
        *
      FROM
        tpch.nation'
    )
  );
```


---

### SET ROLE
Source: https://trino.io/docs/current/sql/set-role.html

SET ROLE# Synopsis# SET ROLE ( role | ALL | NONE ) [ IN catalog ] Description# SET ROLE sets the enabled role for the current session. SET ROLE role enables a single specified role for the current session. For the SET ROLE role statement to succeed, the user executing it should have a grant for the given role. SET ROLE ALL enables all roles that the current user has been granted for the current session. SET ROLE NONE disables all the roles granted to the current user for the current session. The optional IN catalog clause sets the role in a catalog as opposed to a system role. Limitations# Some connectors do not support role management. See connector documentation for more details. See also# CREATE ROLE, DROP ROLE, GRANT role, REVOKE role

#### Code Examples

```
SET ROLE ( role | ALL | NONE )
[ IN catalog ]
```


---

### SET SESSION
Source: https://trino.io/docs/current/sql/set-session.html

SET SESSION# Synopsis# SET SESSION name = expression SET SESSION catalog.name = expression Description# Set a session property value or a catalog session property. Session properties# A session property is a configuration property that can be temporarily modified by a user for the duration of the current connection session to the Trino cluster. Many configuration properties have a corresponding session property that accepts the same values as the config property. There are two types of session properties: System session properties apply to the whole cluster. Most session properties are system session properties unless specified otherwise. Catalog session properties are connector-defined session properties that can be set on a per-catalog basis. These properties must be set separately for each catalog by including the catalog name as a prefix, such as catalogname.property_name. Session properties are tied to the current session, so a user can have multiple connections to a cluster that each have different values for the same session properties. Once a session ends, either by disconnecting or creating a new session, any changes made to session properties during the previous session are lost. Examples# The following example sets a system session property change maximum query run time: SET SESSION query_max_run_time = '10m'; The following example sets the incremental_refresh_enabled catalog session property for a catalog using the Iceberg connector named example: SET SESSION example.incremental_refresh_enabled=false; The related catalog configuration property iceberg.incremental-refresh-enabled defaults to true, and the session property allows you to override this setting in for specific catalog and the current session. The example.incremental_refresh_enabled catalog session property does not apply to any other catalog, even if another catalog also uses the Iceberg connector. See also# RESET SESSION, SHOW SESSION

#### Code Examples

```
SET SESSION name = expression
SET SESSION catalog.name = expression
```
```
SET SESSION query_max_run_time = '10m';
```
```
SET SESSION example.incremental_refresh_enabled=false;
```


---

### SET SESSION AUTHORIZATION
Source: https://trino.io/docs/current/sql/set-session-authorization.html

SET SESSION AUTHORIZATION# Synopsis# SET SESSION AUTHORIZATION username Description# Changes the current user of the session. For the SET SESSION AUTHORIZATION username statement to succeed, the original user (that the client connected with) must be able to impersonate the specified user. User impersonation can be enabled in the system access control. Examples# In the following example, the original user when the connection to Trino is made is Kevin. The following sets the session authorization user to John: SET SESSION AUTHORIZATION 'John'; Queries will now execute as John instead of Kevin. All supported syntax to change the session authorization users are shown below. Changing the session authorization with single quotes: SET SESSION AUTHORIZATION 'John'; Changing the session authorization with double quotes: SET SESSION AUTHORIZATION "John"; Changing the session authorization without quotes: SET SESSION AUTHORIZATION John; See also# RESET SESSION AUTHORIZATION

#### Code Examples

```
SET SESSION AUTHORIZATION username
```
```
SET SESSION AUTHORIZATION 'John';
```
```
SET SESSION AUTHORIZATION 'John';
```
```
SET SESSION AUTHORIZATION "John";
```
```
SET SESSION AUTHORIZATION John;
```


---

### SET TIME ZONE
Source: https://trino.io/docs/current/sql/set-time-zone.html

SET TIME ZONE# Synopsis# SET TIME ZONE LOCAL SET TIME ZONE expression Description# Sets the default time zone for the current session. If the LOCAL option is specified, the time zone for the current session is set to the initial time zone of the session. If the expression option is specified: if the type of the expression is a string, the time zone for the current session is set to the corresponding region-based time zone ID or the corresponding zone offset. if the type of the expression is an interval, the time zone for the current session is set to the corresponding zone offset relative to UTC. It must be in the range of [-14,14] hours. Examples# Use the default time zone for the current session: SET TIME ZONE LOCAL; Use a zone offset for specifying the time zone: SET TIME ZONE '-08:00'; Use an interval literal for specifying the time zone: SET TIME ZONE INTERVAL '10' HOUR; SET TIME ZONE INTERVAL -'08:00' HOUR TO MINUTE; Use a region-based time zone identifier for specifying the time zone: SET TIME ZONE 'America/Los_Angeles'; The time zone identifier to be used can be passed as the output of a function call: SET TIME ZONE concat_ws('/', 'America', 'Los_Angeles'); Limitations# Setting the default time zone for the session has no effect if the sql.forced-session-time-zone configuration property is already set. See also# current_timezone()

#### Code Examples

```
SET TIME ZONE LOCAL
SET TIME ZONE expression
```
```
SET TIME ZONE LOCAL;
```
```
SET TIME ZONE '-08:00';
```
```
SET TIME ZONE INTERVAL '10' HOUR;
SET TIME ZONE INTERVAL -'08:00' HOUR TO MINUTE;
```
```
SET TIME ZONE 'America/Los_Angeles';
```
```
SET TIME ZONE concat_ws('/', 'America', 'Los_Angeles');
```


---

### DESCRIBE
Source: https://trino.io/docs/current/sql/describe.html

DESCRIBE# Synopsis# DESCRIBE table_name Description# DESCRIBE is an alias for SHOW COLUMNS.

#### Code Examples

```
DESCRIBE table_name
```


---

### SHOW CATALOGS
Source: https://trino.io/docs/current/sql/show-catalogs.html

SHOW CATALOGS# Synopsis# SHOW CATALOGS [ LIKE pattern ] Description# List the available catalogs. Specify a pattern in the optional LIKE clause to filter the results to the desired subset. For example, the following query allows you to find catalogs that begin with t: SHOW CATALOGS LIKE 't%'

#### Code Examples

```
SHOW CATALOGS [ LIKE pattern ]
```
```
SHOW CATALOGS LIKE 't%'
```


---

### SHOW COLUMNS
Source: https://trino.io/docs/current/sql/show-columns.html

SHOW COLUMNS# Synopsis# SHOW COLUMNS FROM table [ LIKE pattern ] Description# List the columns in a table along with their data type and other attributes: SHOW COLUMNS FROM nation; Column | Type | Extra | Comment -----------+--------------+-------+--------- nationkey | bigint | | name | varchar(25) | | regionkey | bigint | | comment | varchar(152) | | Specify a pattern in the optional LIKE clause to filter the results to the desired subset. For example, the following query allows you to find columns ending in key: SHOW COLUMNS FROM nation LIKE '%key'; Column | Type | Extra | Comment -----------+--------------+-------+--------- nationkey | bigint | | regionkey | bigint | |

#### Code Examples

```
SHOW COLUMNS FROM table [ LIKE pattern ]
```
```
SHOW COLUMNS FROM nation;
```
```
Column   |     Type     | Extra | Comment
-----------+--------------+-------+---------
 nationkey | bigint       |       |
 name      | varchar(25)  |       |
 regionkey | bigint       |       |
 comment   | varchar(152) |       |
```
```
SHOW COLUMNS FROM nation LIKE '%key';
```
```
Column   |     Type     | Extra | Comment
-----------+--------------+-------+---------
 nationkey | bigint       |       |
 regionkey | bigint       |       |
```


---

### SHOW CREATE FUNCTION
Source: https://trino.io/docs/current/sql/show-create-function.html

SHOW CREATE FUNCTION# Synopsis# SHOW CREATE FUNCTION function_name Description# Show the SQL statement that creates the specified function. Examples# Show the SQL that can be run to create the meaning_of_life function: SHOW CREATE FUNCTION example.default.meaning_of_life; See also# CREATE FUNCTION DROP FUNCTION SHOW FUNCTIONS User-defined functions SQL environment properties

#### Code Examples

```
SHOW CREATE FUNCTION function_name
```
```
SHOW CREATE FUNCTION example.default.meaning_of_life;
```


---

### SHOW CREATE MATERIALIZED VIEW
Source: https://trino.io/docs/current/sql/show-create-materialized-view.html

SHOW CREATE MATERIALIZED VIEW# Synopsis# SHOW CREATE MATERIALIZED VIEW view_name Description# Show the SQL statement that creates the specified materialized view view_name. See also# CREATE MATERIALIZED VIEW DROP MATERIALIZED VIEW REFRESH MATERIALIZED VIEW

#### Code Examples

```
SHOW CREATE MATERIALIZED VIEW view_name
```


---

### SHOW CREATE SCHEMA
Source: https://trino.io/docs/current/sql/show-create-schema.html

SHOW CREATE SCHEMA# Synopsis# SHOW CREATE SCHEMA schema_name Description# Show the SQL statement that creates the specified schema. See also# CREATE SCHEMA

#### Code Examples

```
SHOW CREATE SCHEMA schema_name
```


---

### SHOW CREATE TABLE
Source: https://trino.io/docs/current/sql/show-create-table.html

SHOW CREATE TABLE# Synopsis# SHOW CREATE TABLE table_name Description# Show the SQL statement that creates the specified table. Examples# Show the SQL that can be run to create the orders table: SHOW CREATE TABLE sf1.orders; Create Table ----------------------------------------- CREATE TABLE tpch.sf1.orders ( orderkey bigint, orderstatus varchar, totalprice double, orderdate varchar ) WITH ( format = 'ORC', partitioned_by = ARRAY['orderdate'] ) (1 row) See also# CREATE TABLE

#### Code Examples

```
SHOW CREATE TABLE table_name
```
```
SHOW CREATE TABLE sf1.orders;
```
```
Create Table
-----------------------------------------
 CREATE TABLE tpch.sf1.orders (
    orderkey bigint,
    orderstatus varchar,
    totalprice double,
    orderdate varchar
 )
 WITH (
    format = 'ORC',
    partitioned_by = ARRAY['orderdate']
 )
(1 row)
```


---

### SHOW CREATE VIEW
Source: https://trino.io/docs/current/sql/show-create-view.html

SHOW CREATE VIEW# Synopsis# SHOW CREATE VIEW view_name Description# Show the SQL statement that creates the specified view. See also# CREATE VIEW

#### Code Examples

```
SHOW CREATE VIEW view_name
```


---

### SHOW FUNCTIONS
Source: https://trino.io/docs/current/sql/show-functions.html

SHOW FUNCTIONS# Synopsis# SHOW FUNCTIONS [ FROM schema ] [ LIKE pattern ] Description# List functions in schema or all functions in the current session path. This can include built-in functions, functions from a custom plugin, and User-defined functions. For each function returned, the following information is displayed: Function name Return type Argument types Function type Deterministic Description Use the optional FROM keyword to only list functions in a specific catalog and schema. The location in schema must be specified as cataglog_name.schema_name. Specify a pattern in the optional LIKE clause to filter the results to the desired subset. Examples# List all UDFs and plugin functions in the default schema of the example catalog: SHOW FUNCTIONS FROM example.default; List all functions with a name beginning with array: SHOW FUNCTIONS LIKE 'array%'; List all functions with a name beginning with cf: SHOW FUNCTIONS LIKE 'cf%'; Example output: Function | Return Type | Argument Types | Function Type | Deterministic | Description ------------------+-------------+----------------+---------------+---------------+----------------------------------------- cf_getgroups | varchar | | scalar | true | Returns the current session's groups cf_getprincipal | varchar | | scalar | true | Returns the current session's principal cf_getuser | varchar | | scalar | true | Returns the current session's user See also# Functions and operators User-defined functions Functions CREATE FUNCTION DROP FUNCTION SHOW CREATE FUNCTION

#### Code Examples

```
SHOW FUNCTIONS [ FROM schema ] [ LIKE pattern ]
```
```
SHOW FUNCTIONS FROM example.default;
```
```
SHOW FUNCTIONS LIKE 'array%';
```
```
SHOW FUNCTIONS LIKE 'cf%';
```
```
Function      | Return Type | Argument Types | Function Type | Deterministic |               Description
 ------------------+-------------+----------------+---------------+---------------+-----------------------------------------
 cf_getgroups      | varchar     |                | scalar        | true          | Returns the current session's groups
 cf_getprincipal   | varchar     |                | scalar        | true          | Returns the current session's principal
 cf_getuser        | varchar     |                | scalar        | true          | Returns the current session's user
```


---

### SHOW GRANTS
Source: https://trino.io/docs/current/sql/show-grants.html

SHOW GRANTS# Synopsis# SHOW GRANTS [ ON [ TABLE ] table_name ] Description# List the grants for the current user on the specified table in the current catalog. If no table name is specified, the command lists the grants for the current user on all the tables in all schemas of the current catalog. The command requires the current catalog to be set. Note Ensure that authentication has been enabled before running any of the authorization commands. Examples# List the grants for the current user on table orders: SHOW GRANTS ON TABLE orders; List the grants for the current user on all the tables in all schemas of the current catalog: SHOW GRANTS; Limitations# Some connectors have no support for SHOW GRANTS. See connector documentation for more details. See also# GRANT privilege, REVOKE privilege

#### Code Examples

```
SHOW GRANTS [ ON [ TABLE ] table_name ]
```
```
SHOW GRANTS ON TABLE orders;
```
```
SHOW GRANTS;
```


---

### SHOW ROLE GRANTS
Source: https://trino.io/docs/current/sql/show-role-grants.html

SHOW ROLE GRANTS# Synopsis# SHOW ROLE GRANTS [ FROM catalog ] Description# List non-recursively the system roles or roles in catalog that have been granted to the session user.

#### Code Examples

```
SHOW ROLE GRANTS [ FROM catalog ]
```


---

### SHOW ROLES
Source: https://trino.io/docs/current/sql/show-roles.html

SHOW ROLES# Synopsis# SHOW [CURRENT] ROLES [ FROM catalog ] Description# SHOW ROLES lists all the system roles or all the roles in catalog. SHOW CURRENT ROLES lists the enabled system roles or roles in catalog.

#### Code Examples

```
SHOW [CURRENT] ROLES [ FROM catalog ]
```


---

### SHOW SCHEMAS
Source: https://trino.io/docs/current/sql/show-schemas.html

SHOW SCHEMAS# Synopsis# SHOW SCHEMAS [ FROM catalog ] [ LIKE pattern ] Description# List the schemas in catalog or in the current catalog. Specify a pattern in the optional LIKE clause to filter the results to the desired subset. For example, the following query allows you to find schemas that have 3 as the third character: SHOW SCHEMAS FROM tpch LIKE '__3%'

#### Code Examples

```
SHOW SCHEMAS [ FROM catalog ] [ LIKE pattern ]
```
```
SHOW SCHEMAS FROM tpch LIKE '__3%'
```


---

### SHOW SESSION
Source: https://trino.io/docs/current/sql/show-session.html

SHOW SESSION# Synopsis# SHOW SESSION [ LIKE pattern ] Description# List the current session properties. Specify a pattern in the optional LIKE clause to filter the results to the desired subset. For example, the following query allows you to find session properties that begin with query: SHOW SESSION LIKE 'query%' See also# RESET SESSION, SET SESSION

#### Code Examples

```
SHOW SESSION [ LIKE pattern ]
```
```
SHOW SESSION LIKE 'query%'
```


---

### SHOW STATS
Source: https://trino.io/docs/current/sql/show-stats.html

SHOW STATS# Synopsis# SHOW STATS FOR table SHOW STATS FOR ( query ) Description# Returns approximated statistics for the named table or for the results of a query. Returns NULL for any statistics that are not populated or unavailable on the data source. Statistics are returned as a row for each column, plus a summary row for the table (identifiable by a NULL value for column_name). The following table lists the returned columns and what statistics they represent. Any additional statistics collected on the data source, other than those listed here, are not included. Statistics# Column Description Notes column_name The name of the column NULL in the table summary row data_size The total size in bytes of all of the values in the column NULL in the table summary row. Available for columns of string data types with variable widths. distinct_values_count The estimated number of distinct values in the column NULL in the table summary row nulls_fractions The portion of the values in the column that are NULL NULL in the table summary row. row_count The estimated number of rows in the table NULL in column statistic rows low_value The lowest value found in this column NULL in the table summary row. Available for columns of DATE, integer, floating-point, and exact numeric data types. high_value The highest value found in this column NULL in the table summary row. Available for columns of DATE, integer, floating-point, and exact numeric data types.

#### Code Examples

```
SHOW STATS FOR table
SHOW STATS FOR ( query )
```


---

### SHOW TABLES
Source: https://trino.io/docs/current/sql/show-tables.html

SHOW TABLES# Synopsis# SHOW TABLES [ FROM schema ] [ LIKE pattern ] Description# List the tables and views in the current schema, for example set with USE or by a client connection. Use a fully qualified path to a schema in the form of catalog_name.schema_name to specify any schema in any catalog in the FROM clause. Specify a pattern in the optional LIKE clause to filter the results to the desired subset. Examples# The following query lists tables and views that begin with p in the tiny schema of the tpch catalog: SHOW TABLES FROM tpch.tiny LIKE 'p%'; See also# Schema and table management View management

#### Code Examples

```
SHOW TABLES [ FROM schema ] [ LIKE pattern ]
```
```
SHOW TABLES FROM tpch.tiny LIKE 'p%';
```


---

### DEALLOCATE PREPARE
Source: https://trino.io/docs/current/sql/deallocate-prepare.html

DEALLOCATE PREPARE# Synopsis# DEALLOCATE PREPARE statement_name Description# Removes a statement with the name statement_name from the list of prepared statements in a session. Examples# Deallocate a statement with the name my_query: DEALLOCATE PREPARE my_query; See also# PREPARE, EXECUTE, EXECUTE IMMEDIATE

#### Code Examples

```
DEALLOCATE PREPARE statement_name
```
```
DEALLOCATE PREPARE my_query;
```


---

### DESCRIBE INPUT
Source: https://trino.io/docs/current/sql/describe-input.html

DESCRIBE INPUT# Synopsis# DESCRIBE INPUT statement_name Description# Lists the input parameters of a prepared statement along with the position and type of each parameter. Parameter types that cannot be determined will appear as unknown. Examples# Prepare and describe a query with three parameters: PREPARE my_select1 FROM SELECT ? FROM nation WHERE regionkey = ? AND name < ?; DESCRIBE INPUT my_select1; Position | Type -------------------- 0 | unknown 1 | bigint 2 | varchar (3 rows) Prepare and describe a query with no parameters: PREPARE my_select2 FROM SELECT * FROM nation; DESCRIBE INPUT my_select2; Position | Type ----------------- (0 rows) See also# PREPARE

#### Code Examples

```
DESCRIBE INPUT statement_name
```
```
PREPARE my_select1 FROM
SELECT ? FROM nation WHERE regionkey = ? AND name < ?;
```
```
DESCRIBE INPUT my_select1;
```
```
Position | Type
--------------------
        0 | unknown
        1 | bigint
        2 | varchar
(3 rows)
```
```
PREPARE my_select2 FROM
SELECT * FROM nation;
```
```
DESCRIBE INPUT my_select2;
```
```
Position | Type
-----------------
(0 rows)
```


---

### DESCRIBE OUTPUT
Source: https://trino.io/docs/current/sql/describe-output.html

DESCRIBE OUTPUT# Synopsis# DESCRIBE OUTPUT statement_name Description# List the output columns of a prepared statement, including the column name (or alias), catalog, schema, table, type, type size in bytes, and a boolean indicating if the column is aliased. Examples# Prepare and describe a query with four output columns: PREPARE my_select1 FROM SELECT * FROM nation; DESCRIBE OUTPUT my_select1; Column Name | Catalog | Schema | Table | Type | Type Size | Aliased -------------+---------+--------+--------+---------+-----------+--------- nationkey | tpch | sf1 | nation | bigint | 8 | false name | tpch | sf1 | nation | varchar | 0 | false regionkey | tpch | sf1 | nation | bigint | 8 | false comment | tpch | sf1 | nation | varchar | 0 | false (4 rows) Prepare and describe a query whose output columns are expressions: PREPARE my_select2 FROM SELECT count(*) as my_count, 1+2 FROM nation; DESCRIBE OUTPUT my_select2; Column Name | Catalog | Schema | Table | Type | Type Size | Aliased -------------+---------+--------+-------+--------+-----------+--------- my_count | | | | bigint | 8 | true _col1 | | | | bigint | 8 | false (2 rows) Prepare and describe a row count query: PREPARE my_create FROM CREATE TABLE foo AS SELECT * FROM nation; DESCRIBE OUTPUT my_create; Column Name | Catalog | Schema | Table | Type | Type Size | Aliased -------------+---------+--------+-------+--------+-----------+--------- rows | | | | bigint | 8 | false (1 row) See also# PREPARE

#### Code Examples

```
DESCRIBE OUTPUT statement_name
```
```
PREPARE my_select1 FROM
SELECT * FROM nation;
```
```
DESCRIBE OUTPUT my_select1;
```
```
Column Name | Catalog | Schema | Table  |  Type   | Type Size | Aliased
-------------+---------+--------+--------+---------+-----------+---------
 nationkey   | tpch    | sf1    | nation | bigint  |         8 | false
 name        | tpch    | sf1    | nation | varchar |         0 | false
 regionkey   | tpch    | sf1    | nation | bigint  |         8 | false
 comment     | tpch    | sf1    | nation | varchar |         0 | false
(4 rows)
```
```
PREPARE my_select2 FROM
SELECT count(*) as my_count, 1+2 FROM nation;
```
```
DESCRIBE OUTPUT my_select2;
```
```
Column Name | Catalog | Schema | Table |  Type  | Type Size | Aliased
-------------+---------+--------+-------+--------+-----------+---------
 my_count    |         |        |       | bigint |         8 | true
 _col1       |         |        |       | bigint |         8 | false
(2 rows)
```
```
PREPARE my_create FROM
CREATE TABLE foo AS SELECT * FROM nation;
```
```
DESCRIBE OUTPUT my_create;
```
```
Column Name | Catalog | Schema | Table |  Type  | Type Size | Aliased
-------------+---------+--------+-------+--------+-----------+---------
 rows        |         |        |       | bigint |         8 | false
(1 row)
```


---

### EXECUTE
Source: https://trino.io/docs/current/sql/execute.html

EXECUTE# Synopsis# EXECUTE statement_name [ USING parameter1 [ , parameter2, ... ] ] Description# Executes a prepared statement with the name statement_name. Parameter values are defined in the USING clause. Examples# Prepare and execute a query with no parameters: PREPARE my_select1 FROM SELECT name FROM nation; EXECUTE my_select1; Prepare and execute a query with two parameters: PREPARE my_select2 FROM SELECT name FROM nation WHERE regionkey = ? and nationkey < ?; EXECUTE my_select2 USING 1, 3; This is equivalent to: SELECT name FROM nation WHERE regionkey = 1 AND nationkey < 3; See also# PREPARE, DEALLOCATE PREPARE, EXECUTE IMMEDIATE

#### Code Examples

```
EXECUTE statement_name [ USING parameter1 [ , parameter2, ... ] ]
```
```
PREPARE my_select1 FROM
SELECT name FROM nation;
```
```
EXECUTE my_select1;
```
```
PREPARE my_select2 FROM
SELECT name FROM nation WHERE regionkey = ? and nationkey < ?;
```
```
EXECUTE my_select2 USING 1, 3;
```
```
SELECT name FROM nation WHERE regionkey = 1 AND nationkey < 3;
```


---

### EXECUTE IMMEDIATE
Source: https://trino.io/docs/current/sql/execute-immediate.html

EXECUTE IMMEDIATE# Synopsis# EXECUTE IMMEDIATE `statement` [ USING parameter1 [ , parameter2, ... ] ] Description# Executes a statement without the need to prepare or deallocate the statement. Parameter values are defined in the USING clause. Examples# Execute a query with no parameters: EXECUTE IMMEDIATE 'SELECT name FROM nation'; Execute a query with two parameters: EXECUTE IMMEDIATE 'SELECT name FROM nation WHERE regionkey = ? and nationkey < ?' USING 1, 3; This is equivalent to: PREPARE statement_name FROM SELECT name FROM nation WHERE regionkey = ? and nationkey < ? EXECUTE statement_name USING 1, 3 DEALLOCATE PREPARE statement_name See also# EXECUTE, PREPARE, DEALLOCATE PREPARE

#### Code Examples

```
EXECUTE IMMEDIATE `statement` [ USING parameter1 [ , parameter2, ... ] ]
```
```
EXECUTE IMMEDIATE
'SELECT name FROM nation';
```
```
EXECUTE IMMEDIATE
'SELECT name FROM nation WHERE regionkey = ? and nationkey < ?'
USING 1, 3;
```
```
PREPARE statement_name FROM SELECT name FROM nation WHERE regionkey = ? and nationkey < ?
EXECUTE statement_name USING 1, 3
DEALLOCATE PREPARE statement_name
```


---

### PREPARE
Source: https://trino.io/docs/current/sql/prepare.html

PREPARE# Synopsis# PREPARE statement_name FROM statement Description# Prepares a statement for execution at a later time. Prepared statements are queries that are saved in a session with a given name. The statement can include parameters in place of literals to be replaced at execution time. Parameters are represented by question marks. Examples# Prepare a select query: PREPARE my_select1 FROM SELECT * FROM nation; Prepare a select query that includes parameters. The values to compare with regionkey and nationkey will be filled in with the EXECUTE statement: PREPARE my_select2 FROM SELECT name FROM nation WHERE regionkey = ? AND nationkey < ?; Prepare an insert query: PREPARE my_insert FROM INSERT INTO cities VALUES (1, 'San Francisco'); See also# EXECUTE, DEALLOCATE PREPARE, EXECUTE IMMEDIATE, DESCRIBE INPUT, DESCRIBE OUTPUT

#### Code Examples

```
PREPARE statement_name FROM statement
```
```
PREPARE my_select1 FROM
SELECT * FROM nation;
```
```
PREPARE my_select2 FROM
SELECT name FROM nation WHERE regionkey = ? AND nationkey < ?;
```
```
PREPARE my_insert FROM
INSERT INTO cities VALUES (1, 'San Francisco');
```


---

### GRANT privilege
Source: https://trino.io/docs/current/sql/grant.html

GRANT privilege# Synopsis# GRANT ( privilege [, ...] | ( ALL PRIVILEGES ) ) ON ( table_name | TABLE table_name | SCHEMA schema_name) TO ( user | USER user | ROLE role ) [ WITH GRANT OPTION ] Description# Grants the specified privileges to the specified grantee. Specifying ALL PRIVILEGES grants DELETE, INSERT, UPDATE and SELECT privileges. Specifying ROLE PUBLIC grants privileges to the PUBLIC role and hence to all users. The optional WITH GRANT OPTION clause allows the grantee to grant these same privileges to others. For GRANT statement to succeed, the user executing it should possess the specified privileges as well as the GRANT OPTION for those privileges. Grant on a table grants the specified privilege on all current and future columns of the table. Grant on a schema grants the specified privilege on all current and future columns of all current and future tables of the schema. Examples# Grant INSERT and SELECT privileges on the table orders to user alice: GRANT INSERT, SELECT ON orders TO alice; Grant DELETE privilege on the schema finance to user bob: GRANT DELETE ON SCHEMA finance TO bob; Grant SELECT privilege on the table nation to user alice, additionally allowing alice to grant SELECT privilege to others: GRANT SELECT ON nation TO alice WITH GRANT OPTION; Grant SELECT privilege on the table orders to everyone: GRANT SELECT ON orders TO ROLE PUBLIC; Limitations# Some connectors have no support for GRANT. See connector documentation for more details. See also# DENY, REVOKE privilege, SHOW GRANTS

#### Code Examples

```
GRANT ( privilege [, ...] | ( ALL PRIVILEGES ) )
ON ( table_name | TABLE table_name | SCHEMA schema_name)
TO ( user | USER user | ROLE role )
[ WITH GRANT OPTION ]
```
```
GRANT INSERT, SELECT ON orders TO alice;
```
```
GRANT DELETE ON SCHEMA finance TO bob;
```
```
GRANT SELECT ON nation TO alice WITH GRANT OPTION;
```
```
GRANT SELECT ON orders TO ROLE PUBLIC;
```


---

### GRANT role
Source: https://trino.io/docs/current/sql/grant-roles.html

GRANT role# Synopsis# GRANT role_name [, ...] TO ( user | USER user_name | ROLE role_name) [, ...] [ GRANTED BY ( user | USER user | ROLE role | CURRENT_USER | CURRENT_ROLE ) ] [ WITH ADMIN OPTION ] [ IN catalog ] Description# Grants the specified role(s) to the specified principal(s). If the WITH ADMIN OPTION clause is specified, the role(s) are granted to the users with GRANT option. For the GRANT statement for roles to succeed, the user executing it either should be the role admin or should possess the GRANT option for the given role. The optional GRANTED BY clause causes the role(s) to be granted with the specified principal as a grantor. If the GRANTED BY clause is not specified, the roles are granted with the current user as a grantor. The optional IN catalog clause grants the roles in a catalog as opposed to a system roles. Examples# Grant role bar to user foo GRANT bar TO USER foo; Grant roles bar and foo to user baz and role qux with admin option GRANT bar, foo TO USER baz, ROLE qux WITH ADMIN OPTION; Limitations# Some connectors do not support role management. See connector documentation for more details. See also# CREATE ROLE, DROP ROLE, SET ROLE, REVOKE role

#### Code Examples

```
GRANT role_name [, ...]
TO ( user | USER user_name | ROLE role_name) [, ...]
[ GRANTED BY ( user | USER user | ROLE role | CURRENT_USER | CURRENT_ROLE ) ]
[ WITH ADMIN OPTION ]
[ IN catalog ]
```
```
GRANT bar TO USER foo;
```
```
GRANT bar, foo TO USER baz, ROLE qux WITH ADMIN OPTION;
```


---

### DENY
Source: https://trino.io/docs/current/sql/deny.html

DENY# Synopsis# DENY ( privilege [, ...] | ( ALL PRIVILEGES ) ) ON ( table_name | TABLE table_name | SCHEMA schema_name) TO ( user | USER user | ROLE role ) Description# Denies the specified privileges to the specified grantee. Deny on a table rejects the specified privilege on all current and future columns of the table. Deny on a schema rejects the specified privilege on all current and future columns of all current and future tables of the schema. Examples# Deny INSERT and SELECT privileges on the table orders to user alice: DENY INSERT, SELECT ON orders TO alice; Deny DELETE privilege on the schema finance to user bob: DENY DELETE ON SCHEMA finance TO bob; Deny SELECT privilege on the table orders to everyone: DENY SELECT ON orders TO ROLE PUBLIC; Limitations# The system access controls as well as the connectors provided by default in Trino have no support for DENY. See also# GRANT privilege, REVOKE privilege, SHOW GRANTS

#### Code Examples

```
DENY ( privilege [, ...] | ( ALL PRIVILEGES ) )
ON ( table_name | TABLE table_name | SCHEMA schema_name)
TO ( user | USER user | ROLE role )
```
```
DENY INSERT, SELECT ON orders TO alice;
```
```
DENY DELETE ON SCHEMA finance TO bob;
```
```
DENY SELECT ON orders TO ROLE PUBLIC;
```


---

### REVOKE privilege
Source: https://trino.io/docs/current/sql/revoke.html

REVOKE privilege# Synopsis# REVOKE [ GRANT OPTION FOR ] ( privilege [, ...] | ALL PRIVILEGES ) ON ( table_name | TABLE table_name | SCHEMA schema_name ) FROM ( user | USER user | ROLE role ) Description# Revokes the specified privileges from the specified grantee. Specifying ALL PRIVILEGES revokes DELETE, INSERT and SELECT privileges. Specifying ROLE PUBLIC revokes privileges from the PUBLIC role. Users will retain privileges assigned to them directly or via other roles. If the optional GRANT OPTION FOR clause is specified, only the GRANT OPTION is removed. Otherwise, both the GRANT and GRANT OPTION are revoked. For REVOKE statement to succeed, the user executing it should possess the specified privileges as well as the GRANT OPTION for those privileges. Revoke on a table revokes the specified privilege on all columns of the table. Revoke on a schema revokes the specified privilege on all columns of all tables of the schema. Examples# Revoke INSERT and SELECT privileges on the table orders from user alice: REVOKE INSERT, SELECT ON orders FROM alice; Revoke DELETE privilege on the schema finance from user bob: REVOKE DELETE ON SCHEMA finance FROM bob; Revoke SELECT privilege on the table nation from everyone, additionally revoking the privilege to grant SELECT privilege: REVOKE GRANT OPTION FOR SELECT ON nation FROM ROLE PUBLIC; Revoke all privileges on the table test from user alice: REVOKE ALL PRIVILEGES ON test FROM alice; Limitations# Some connectors have no support for REVOKE. See connector documentation for more details. See also# DENY, GRANT privilege, SHOW GRANTS

#### Code Examples

```
REVOKE [ GRANT OPTION FOR ]
( privilege [, ...] | ALL PRIVILEGES )
ON ( table_name | TABLE table_name | SCHEMA schema_name )
FROM ( user | USER user | ROLE role )
```
```
REVOKE INSERT, SELECT ON orders FROM alice;
```
```
REVOKE DELETE ON SCHEMA finance FROM bob;
```
```
REVOKE GRANT OPTION FOR SELECT ON nation FROM ROLE PUBLIC;
```
```
REVOKE ALL PRIVILEGES ON test FROM alice;
```


---

### REVOKE role
Source: https://trino.io/docs/current/sql/revoke-roles.html

REVOKE role# Synopsis# REVOKE [ ADMIN OPTION FOR ] role_name [, ...] FROM ( user | USER user | ROLE role) [, ...] [ GRANTED BY ( user | USER user | ROLE role | CURRENT_USER | CURRENT_ROLE ) ] [ IN catalog ] Description# Revokes the specified role(s) from the specified principal(s). If the ADMIN OPTION FOR clause is specified, the GRANT permission is revoked instead of the role. For the REVOKE statement for roles to succeed, the user executing it either should be the role admin or should possess the GRANT option for the given role. The optional GRANTED BY clause causes the role(s) to be revoked with the specified principal as a revoker. If the GRANTED BY clause is not specified, the roles are revoked by the current user as a revoker. The optional IN catalog clause revokes the roles in a catalog as opposed to a system roles. Examples# Revoke role bar from user foo REVOKE bar FROM USER foo; Revoke admin option for roles bar and foo from user baz and role qux REVOKE ADMIN OPTION FOR bar, foo FROM USER baz, ROLE qux; Limitations# Some connectors do not support role management. See connector documentation for more details. See also# CREATE ROLE, DROP ROLE, SET ROLE, GRANT role

#### Code Examples

```
REVOKE
[ ADMIN OPTION FOR ]
role_name [, ...]
FROM ( user | USER user | ROLE role) [, ...]
[ GRANTED BY ( user | USER user | ROLE role | CURRENT_USER | CURRENT_ROLE ) ]
[ IN catalog ]
```
```
REVOKE bar FROM USER foo;
```
```
REVOKE ADMIN OPTION FOR bar, foo FROM USER baz, ROLE qux;
```


---

### ANALYZE
Source: https://trino.io/docs/current/sql/analyze.html

ANALYZE# Synopsis# ANALYZE table_name [ WITH ( property_name = expression [, ...] ) ] Description# Collects table and column statistics for a given table. The optional WITH clause can be used to provide connector-specific properties. To list all available properties, run the following query: SELECT * FROM system.metadata.analyze_properties Examples# Analyze table web to collect table and column statistics: ANALYZE web; Analyze table stores in catalog hive and schema default: ANALYZE hive.default.stores; Analyze partitions '1992-01-01', '1992-01-02' from a Hive partitioned table sales: ANALYZE hive.default.sales WITH (partitions = ARRAY[ARRAY['1992-01-01'], ARRAY['1992-01-02']]); Analyze partitions with complex partition key (state and city columns) from a Hive partitioned table customers: ANALYZE hive.default.customers WITH (partitions = ARRAY[ARRAY['CA', 'San Francisco'], ARRAY['NY', 'NY']]); Analyze only columns department and product_id for partitions '1992-01-01', '1992-01-02' from a Hive partitioned table sales: ANALYZE hive.default.sales WITH ( partitions = ARRAY[ARRAY['1992-01-01'], ARRAY['1992-01-02']], columns = ARRAY['department', 'product_id']);

#### Code Examples

```
ANALYZE table_name [ WITH ( property_name = expression [, ...] ) ]
```
```
SELECT * FROM system.metadata.analyze_properties
```
```
ANALYZE web;
```
```
ANALYZE hive.default.stores;
```
```
ANALYZE hive.default.sales WITH (partitions = ARRAY[ARRAY['1992-01-01'], ARRAY['1992-01-02']]);
```
```
ANALYZE hive.default.customers WITH (partitions = ARRAY[ARRAY['CA', 'San Francisco'], ARRAY['NY', 'NY']]);
```
```
ANALYZE hive.default.sales WITH (
    partitions = ARRAY[ARRAY['1992-01-01'], ARRAY['1992-01-02']],
    columns = ARRAY['department', 'product_id']);
```


---

### CALL
Source: https://trino.io/docs/current/sql/call.html

CALL# Synopsis# CALL procedure_name ( [ name => ] expression [, ...] ) Description# Call a procedure. Procedures can be provided by connectors to perform data manipulation or administrative tasks. For example, the System connector defines a procedure for killing a running query. Some connectors, such as the PostgreSQL connector, are for systems that have their own stored procedures. These stored procedures are separate from the connector-defined procedures discussed here and thus are not directly callable via CALL. See connector documentation for details on available procedures. Examples# Call a procedure using positional arguments: CALL test(123, 'apple'); Call a procedure using named arguments: CALL test(name => 'apple', id => 123); Call a procedure using a fully qualified name: CALL catalog.schema.test();

#### Code Examples

```
CALL procedure_name ( [ name => ] expression [, ...] )
```
```
CALL test(123, 'apple');
```
```
CALL test(name => 'apple', id => 123);
```
```
CALL catalog.schema.test();
```


---

### COMMENT
Source: https://trino.io/docs/current/sql/comment.html

COMMENT# Synopsis# COMMENT ON ( TABLE | VIEW | COLUMN ) name IS 'comments' Description# Set the comment for a object. The comment can be removed by setting the comment to NULL. Examples# Change the comment for the users table to be master table: COMMENT ON TABLE users IS 'master table'; Change the comment for the users view to be master view: COMMENT ON VIEW users IS 'master view'; Change the comment for the users.name column to be full name: COMMENT ON COLUMN users.name IS 'full name'; See also# Comments

#### Code Examples

```
COMMENT ON ( TABLE | VIEW | COLUMN ) name IS 'comments'
```
```
COMMENT ON TABLE users IS 'master table';
```
```
COMMENT ON VIEW users IS 'master view';
```
```
COMMENT ON COLUMN users.name IS 'full name';
```


---

### EXPLAIN
Source: https://trino.io/docs/current/sql/explain.html

EXPLAIN# Synopsis# EXPLAIN [ ( option [, ...] ) ] statement where option can be one of: FORMAT { TEXT | GRAPHVIZ | JSON } TYPE { LOGICAL | DISTRIBUTED | VALIDATE | IO } Description# Show the logical or distributed execution plan of a statement, or validate the statement. The distributed plan is shown by default. Each plan fragment of the distributed plan is executed by a single or multiple Trino nodes. Fragments separation represent the data exchange between Trino nodes. Fragment type specifies how the fragment is executed by Trino nodes and how the data is distributed between fragments: SINGLEFragment is executed on a single node. HASHFragment is executed on a fixed number of nodes with the input data distributed using a hash function. ROUND_ROBINFragment is executed on a fixed number of nodes with the input data distributed in a round-robin fashion. BROADCASTFragment is executed on a fixed number of nodes with the input data broadcasted to all nodes. SOURCEFragment is executed on nodes where input splits are accessed. Examples# EXPLAIN (TYPE LOGICAL)# Process the supplied query statement and create a logical plan in text format: EXPLAIN (TYPE LOGICAL) SELECT regionkey, count(*) FROM nation GROUP BY 1; Query Plan ----------------------------------------------------------------------------------------------------------------- Trino version: version Output[regionkey, _col1] │ Layout: [regionkey:bigint, count:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} │ _col1 := count └─ RemoteExchange[GATHER] │ Layout: [regionkey:bigint, count:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} └─ Aggregate(FINAL)[regionkey] │ Layout: [regionkey:bigint, count:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} │ count := count("count_8") └─ LocalExchange[HASH][$hashvalue] ("regionkey") │ Layout: [regionkey:bigint, count_8:bigint, $hashvalue:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} └─ RemoteExchange[REPARTITION][$hashvalue_9] │ Layout: [regionkey:bigint, count_8:bigint, $hashvalue_9:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} └─ Project[] │ Layout: [regionkey:bigint, count_8:bigint, $hashvalue_10:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} │ $hashvalue_10 := "combine_hash"(bigint '0', COALESCE("$operator$hash_code"("regionkey"), 0)) └─ Aggregate(PARTIAL)[regionkey] │ Layout: [regionkey:bigint, count_8:bigint] │ count_8 := count(*) └─ TableScan[tpch:nation:sf0.01] Layout: [regionkey:bigint] Estimates: {rows: 25 (225B), cpu: 225, memory: 0B, network: 0B} regionkey := tpch:regionkey EXPLAIN (TYPE LOGICAL, FORMAT JSON)# Warning The output format is not guaranteed to be backward compatible across Trino versions. Process the supplied query statement and create a logical plan in JSON format: EXPLAIN (TYPE LOGICAL, FORMAT JSON) SELECT regionkey, count(*) FROM nation GROUP BY 1; { "id": "9", "name": "Output", "descriptor": { "columnNames": "[regionkey, _col1]" }, "outputs": [ { "symbol": "regionkey", "type": "bigint" }, { "symbol": "count", "type": "bigint" } ], "details": [ "_col1 := count" ], "estimates": [ { "outputRowCount": "NaN", "outputSizeInBytes": "NaN", "cpuCost": "NaN", "memoryCost": "NaN", "networkCost": "NaN" } ], "children": [ { "id": "145", "name": "RemoteExchange", "descriptor": { "type": "GATHER", "isReplicateNullsAndAny": "", "hashColumn": "" }, "outputs": [ { "symbol": "regionkey", "type": "bigint" }, { "symbol": "count", "type": "bigint" } ], "details": [ ], "estimates": [ { "outputRowCount": "NaN", "outputSizeInBytes": "NaN", "cpuCost": "NaN", "memoryCost": "NaN", "networkCost": "NaN" } ], "children": [ { "id": "4", "name": "Aggregate", "descriptor": { "type": "FINAL", "keys": "[regionkey]", "hash": "" }, "outputs": [ { "symbol": "regionkey", "type": "bigint" }, { "symbol": "count", "type": "bigint" } ], "details": [ "count := count(\"count_0\")" ], "estimates": [ { "outputRowCount": "NaN", "outputSizeInBytes": "NaN", "cpuCost": "NaN", "memoryCost": "NaN", "networkCost": "NaN" } ], "children": [ { "id": "194", "name": "LocalExchange", "descriptor": { "partitioning": "HASH", "isReplicateNullsAndAny": "", "hashColumn": "[$hashvalue]", "arguments": "[\"regionkey\"]" }, "outputs": [ { "symbol": "regionkey", "type": "bigint" }, { "symbol": "count_0", "type": "bigint" }, { "symbol": "$hashvalue", "type": "bigint" } ], "details":[], "estimates": [ { "outputRowCount": "NaN", "outputSizeInBytes": "NaN", "cpuCost": "NaN", "memoryCost": "NaN", "networkCost": "NaN" } ], "children": [ { "id": "200", "name": "RemoteExchange", "descriptor": { "type": "REPARTITION", "isReplicateNullsAndAny": "", "hashColumn": "[$hashvalue_1]" }, "outputs": [ { "symbol": "regionkey", "type": "bigint" }, { "symbol": "count_0", "type": "bigint" }, { "symbol": "$hashvalue_1", "type": "bigint" } ], "details":[], "estimates": [ { "outputRowCount": "NaN", "outputSizeInBytes": "NaN", "cpuCost": "NaN", "memoryCost": "NaN", "networkCost": "NaN" } ], "children": [ { "id": "226", "name": "Project", "descriptor": {} "outputs": [ { "symbol": "regionkey", "type": "bigint" }, { "symbol": "count_0", "type": "bigint" }, { "symbol": "$hashvalue_2", "type": "bigint" } ], "details": [ "$hashvalue_2 := combine_hash(bigint '0', COALESCE(\"$operator$hash_code\"(\"regionkey\"), 0))" ], "estimates": [ { "outputRowCount": "NaN", "outputSizeInBytes": "NaN", "cpuCost": "NaN", "memoryCost": "NaN", "networkCost": "NaN" } ], "children": [ { "id": "198", "name": "Aggregate", "descriptor": { "type": "PARTIAL", "keys": "[regionkey]", "hash": "" }, "outputs": [ { "symbol": "regionkey", "type": "bigint" }, { "symbol": "count_0", "type": "bigint" } ], "details": [ "count_0 := count(*)" ], "estimates":[], "children": [ { "id": "0", "name": "TableScan", "descriptor": { "table": "hive:tpch_sf1_orc_part:nation" }, "outputs": [ { "symbol": "regionkey", "type": "bigint" } ], "details": [ "regionkey := regionkey:bigint:REGULAR" ], "estimates": [ { "outputRowCount": 25, "outputSizeInBytes": 225, "cpuCost": 225, "memoryCost": 0, "networkCost": 0 } ], "children": [] } ] } ] } ] } ] } ] } ] } ] } EXPLAIN (TYPE DISTRIBUTED)# Process the supplied query statement and create a distributed plan in text format. The distributed plan splits the logical plan into stages, and therefore explicitly shows the data exchange between workers: EXPLAIN (TYPE DISTRIBUTED) SELECT regionkey, count(*) FROM nation GROUP BY 1; Query Plan ------------------------------------------------------------------------------------------------------ Trino version: version Fragment 0 [SINGLE] Output layout: [regionkey, count] Output partitioning: SINGLE [] Output[regionkey, _col1] │ Layout: [regionkey:bigint, count:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} │ _col1 := count └─ RemoteSource[1] Layout: [regionkey:bigint, count:bigint] Fragment 1 [HASH] Output layout: [regionkey, count] Output partitioning: SINGLE [] Aggregate(FINAL)[regionkey] │ Layout: [regionkey:bigint, count:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} │ count := count("count_8") └─ LocalExchange[HASH][$hashvalue] ("regionkey") │ Layout: [regionkey:bigint, count_8:bigint, $hashvalue:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} └─ RemoteSource[2] Layout: [regionkey:bigint, count_8:bigint, $hashvalue_9:bigint] Fragment 2 [SOURCE] Output layout: [regionkey, count_8, $hashvalue_10] Output partitioning: HASH [regionkey][$hashvalue_10] Project[] │ Layout: [regionkey:bigint, count_8:bigint, $hashvalue_10:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?} │ $hashvalue_10 := "combine_hash"(bigint '0', COALESCE("$operator$hash_code"("regionkey"), 0)) └─ Aggregate(PARTIAL)[regionkey] │ Layout: [regionkey:bigint, count_8:bigint] │ count_8 := count(*) └─ TableScan[tpch:nation:sf0.01, grouped = false] Layout: [regionkey:bigint] Estimates: {rows: 25 (225B), cpu: 225, memory: 0B, network: 0B} regionkey := tpch:regionkey EXPLAIN (TYPE DISTRIBUTED, FORMAT JSON)# Warning The output format is not guaranteed to be backward compatible across Trino versions. Process the supplied query statement and create a distributed plan in JSON format. The distributed plan splits the logical plan into stages, and therefore explicitly shows the data exchange between workers: EXPLAIN (TYPE DISTRIBUTED, FORMAT JSON) SELECT regionkey, count(*) FROM nation GROUP BY 1; { "0" : { "id" : "9", "name" : "Output", "descriptor" : { "columnNames" : "[regionkey, _col1]" }, "outputs" : [ { "symbol" : "regionkey", "type" : "bigint" }, { "symbol" : "count", "type" : "bigint" } ], "details" : [ "_col1 := count" ], "estimates" : [ { "outputRowCount" : "NaN", "outputSizeInBytes" : "NaN", "cpuCost" : "NaN", "memoryCost" : "NaN", "networkCost" : "NaN" } ], "children" : [ { "id" : "145", "name" : "RemoteSource", "descriptor" : { "sourceFragmentIds" : "[1]" }, "outputs" : [ { "symbol" : "regionkey", "type" : "bigint" }, { "symbol" : "count", "type" : "bigint" } ], "details" : [ ], "estimates" : [ ], "children" : [ ] } ] }, "1" : { "id" : "4", "name" : "Aggregate", "descriptor" : { "type" : "FINAL", "keys" : "[regionkey]", "hash" : "[]" }, "outputs" : [ { "symbol" : "regionkey", "type" : "bigint" }, { "symbol" : "count", "type" : "bigint" } ], "details" : [ "count := count(\"count_0\")" ], "estimates" : [ { "outputRowCount" : "NaN", "outputSizeInBytes" : "NaN", "cpuCost" : "NaN", "memoryCost" : "NaN", "networkCost" : "NaN" } ], "children" : [ { "id" : "194", "name" : "LocalExchange", "descriptor" : { "partitioning" : "SINGLE", "isReplicateNullsAndAny" : "", "hashColumn" : "[]", "arguments" : "[]" }, "outputs" : [ { "symbol" : "regionkey", "type" : "bigint" }, { "symbol" : "count_0", "type" : "bigint" } ], "details" : [ ], "estimates" : [ { "outputRowCount" : "NaN", "outputSizeInBytes" : "NaN", "cpuCost" : "NaN", "memoryCost" : "NaN", "networkCost" : "NaN" } ], "children" : [ { "id" : "227", "name" : "Project", "descriptor" : { }, "outputs" : [ { "symbol" : "regionkey", "type" : "bigint" }, { "symbol" : "count_0", "type" : "bigint" } ], "details" : [ ], "estimates" : [ { "outputRowCount" : "NaN", "outputSizeInBytes" : "NaN", "cpuCost" : "NaN", "memoryCost" : "NaN", "networkCost" : "NaN" } ], "children" : [ { "id" : "200", "name" : "RemoteSource", "descriptor" : { "sourceFragmentIds" : "[2]" }, "outputs" : [ { "symbol" : "regionkey", "type" : "bigint" }, { "symbol" : "count_0", "type" : "bigint" }, { "symbol" : "$hashvalue", "type" : "bigint" } ], "details" : [ ], "estimates" : [ ], "children" : [ ] } ] } ] } ] }, "2" : { "id" : "226", "name" : "Project", "descriptor" : { }, "outputs" : [ { "symbol" : "regionkey", "type" : "bigint" }, { "symbol" : "count_0", "type" : "bigint" }, { "symbol" : "$hashvalue_1", "type" : "bigint" } ], "details" : [ "$hashvalue_1 := combine_hash(bigint '0', COALESCE(\"$operator$hash_code\"(\"regionkey\"), 0))" ], "estimates" : [ { "outputRowCount" : "NaN", "outputSizeInBytes" : "NaN", "cpuCost" : "NaN", "memoryCost" : "NaN", "networkCost" : "NaN" } ], "children" : [ { "id" : "198", "name" : "Aggregate", "descriptor" : { "type" : "PARTIAL", "keys" : "[regionkey]", "hash" : "[]" }, "outputs" : [ { "symbol" : "regionkey", "type" : "bigint" }, { "symbol" : "count_0", "type" : "bigint" } ], "details" : [ "count_0 := count(*)" ], "estimates" : [ ], "children" : [ { "id" : "0", "name" : "TableScan", "descriptor" : { "table" : "tpch:tiny:nation" }, "outputs" : [ { "symbol" : "regionkey", "type" : "bigint" } ], "details" : [ "regionkey := tpch:regionkey" ], "estimates" : [ { "outputRowCount" : 25.0, "outputSizeInBytes" : 225.0, "cpuCost" : 225.0, "memoryCost" : 0.0, "networkCost" : 0.0 } ], "children" : [ ] } ] } ] } } EXPLAIN (TYPE VALIDATE)# Validate the supplied query statement for syntactical and semantic correctness. Returns true if the statement is valid: EXPLAIN (TYPE VALIDATE) SELECT regionkey, count(*) FROM nation GROUP BY 1; Valid ------- true If the statement is not correct because a syntax error, such as an unknown keyword, is found the error message details the problem: EXPLAIN (TYPE VALIDATE) SELET 1=0; Query 20220929_234840_00001_vjwxj failed: line 1:25: mismatched input 'SELET'. Expecting: 'ALTER', 'ANALYZE', 'CALL', 'COMMENT', 'COMMIT', 'CREATE', 'DEALLOCATE', 'DELETE', 'DENY', 'DESC', 'DESCRIBE', 'DROP', 'EXECUTE', 'EXPLAIN', 'GRANT', 'INSERT', 'MERGE', 'PREPARE', 'REFRESH', 'RESET', 'REVOKE', 'ROLLBACK', 'SET', 'SHOW', 'START', 'TRUNCATE', 'UPDATE', 'USE', <query> Similarly if semantic issues are detected, such as an invalid object name nations instead of nation, the error message returns useful information: EXPLAIN(TYPE VALIDATE) SELECT * FROM tpch.tiny.nations; Query 20220929_235059_00003_vjwxj failed: line 1:15: Table 'tpch.tiny.nations' does not exist SELECT * FROM tpch.tiny.nations EXPLAIN (TYPE IO)# Process the supplied query statement and create a plan with input and output details about the accessed objects in JSON format: EXPLAIN (TYPE IO, FORMAT JSON) INSERT INTO test_lineitem SELECT * FROM lineitem WHERE shipdate = '2020-02-01' AND quantity > 10; Query Plan ----------------------------------- { inputTableColumnInfos: [ { table: { catalog: "hive", schemaTable: { schema: "tpch", table: "test_orders" } }, columnConstraints: [ { columnName: "orderkey", type: "bigint", domain: { nullsAllowed: false, ranges: [ { low: { value: "1", bound: "EXACTLY" }, high: { value: "1", bound: "EXACTLY" } }, { low: { value: "2", bound: "EXACTLY" }, high: { value: "2", bound: "EXACTLY" } } ] } }, { columnName: "processing", type: "boolean", domain: { nullsAllowed: false, ranges: [ { low: { value: "false", bound: "EXACTLY" }, high: { value: "false", bound: "EXACTLY" } } ] } }, { columnName: "custkey", type: "bigint", domain: { nullsAllowed: false, ranges: [ { low: { bound: "ABOVE" }, high: { value: "10", bound: "EXACTLY" } } ] } } ], estimate: { outputRowCount: 2, outputSizeInBytes: 40, cpuCost: 40, maxMemory: 0, networkCost: 0 } } ], outputTable: { catalog: "hive", schemaTable: { schema: "tpch", table: "test_orders" } }, estimate: { outputRowCount: "NaN", outputSizeInBytes: "NaN", cpuCost: "NaN", maxMemory: "NaN", networkCost: "NaN" } } See also# EXPLAIN ANALYZE

#### Code Examples

```
EXPLAIN [ ( option [, ...] ) ] statement
```
```
FORMAT { TEXT | GRAPHVIZ | JSON }
TYPE { LOGICAL | DISTRIBUTED | VALIDATE | IO }
```
```
EXPLAIN (TYPE LOGICAL) SELECT regionkey, count(*) FROM nation GROUP BY 1;
```
```
Query Plan
-----------------------------------------------------------------------------------------------------------------
 Trino version: version
 Output[regionkey, _col1]
 │   Layout: [regionkey:bigint, count:bigint]
 │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
 │   _col1 := count
 └─ RemoteExchange[GATHER]
    │   Layout: [regionkey:bigint, count:bigint]
    │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
    └─ Aggregate(FINAL)[regionkey]
       │   Layout: [regionkey:bigint, count:bigint]
       │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
       │   count := count("count_8")
       └─ LocalExchange[HASH][$hashvalue] ("regionkey")
          │   Layout: [regionkey:bigint, count_8:bigint, $hashvalue:bigint]
          │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
          └─ RemoteExchange[REPARTITION][$hashvalue_9]
             │   Layout: [regionkey:bigint, count_8:bigint, $hashvalue_9:bigint]
             │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
             └─ Project[]
                │   Layout: [regionkey:bigint, count_8:bigint, $hashvalue_10:bigint]
                │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
                │   $hashvalue_10 := "combine_hash"(bigint '0', COALESCE("$operator$hash_code"("regionkey"), 0))
                └─ Aggregate(PARTIAL)[regionkey]
                   │   Layout: [regionkey:bigint, count_8:bigint]
                   │   count_8 := count(*)
                   └─ TableScan[tpch:nation:sf0.01]
                          Layout: [regionkey:bigint]
                          Estimates: {rows: 25 (225B), cpu: 225, memory: 0B, network: 0B}
                          regionkey := tpch:regionkey
```
```
EXPLAIN (TYPE LOGICAL, FORMAT JSON) SELECT regionkey, count(*) FROM nation GROUP BY 1;
```
```
{
   "id": "9",
   "name": "Output",
   "descriptor": {
      "columnNames": "[regionkey, _col1]"
   },
   "outputs": [
      {
         "symbol": "regionkey",
         "type": "bigint"
      },
      {
         "symbol": "count",
         "type": "bigint"
      }
   ],
   "details": [
      "_col1 := count"
   ],
   "estimates": [
      {
         "outputRowCount": "NaN",
         "outputSizeInBytes": "NaN",
         "cpuCost": "NaN",
         "memoryCost": "NaN",
         "networkCost": "NaN"
      }
   ],
   "children": [
      {
         "id": "145",
         "name": "RemoteExchange",
         "descriptor": {
            "type": "GATHER",
            "isReplicateNullsAndAny": "",
            "hashColumn": ""
         },
         "outputs": [
            {
               "symbol": "regionkey",
               "type": "bigint"
            },
            {
               "symbol": "count",
               "type": "bigint"
            }
         ],
         "details": [

         ],
         "estimates": [
            {
               "outputRowCount": "NaN",
               "outputSizeInBytes": "NaN",
               "cpuCost": "NaN",
               "memoryCost": "NaN",
               "networkCost": "NaN"
            }
         ],
         "children": [
            {
               "id": "4",
               "name": "Aggregate",
               "descriptor": {
                  "type": "FINAL",
                  "keys": "[regionkey]",
                  "hash": ""
               },
               "outputs": [
                  {
                     "symbol": "regionkey",
                     "type": "bigint"
                  },
                  {
                     "symbol": "count",
                     "type": "bigint"
                  }
               ],
               "details": [
                  "count := count(\"count_0\")"
               ],
               "estimates": [
                  {
                     "outputRowCount": "NaN",
                     "outputSizeInBytes": "NaN",
                     "cpuCost": "NaN",
                     "memoryCost": "NaN",
                     "networkCost": "NaN"
                  }
               ],
               "children": [
                  {
                     "id": "194",
                     "name": "LocalExchange",
                     "descriptor": {
                        "partitioning": "HASH",
                        "isReplicateNullsAndAny": "",
                        "hashColumn": "[$hashvalue]",
                        "arguments": "[\"regionkey\"]"
                     },
                     "outputs": [
                        {
                           "symbol": "regionkey",
                           "type": "bigint"
                        },
                        {
                           "symbol": "count_0",
                           "type": "bigint"
                        },
                        {
                           "symbol": "$hashvalue",
                           "type": "bigint"
                        }
                     ],
                     "details":[],
                     "estimates": [
                        {
                           "outputRowCount": "NaN",
                           "outputSizeInBytes": "NaN",
                           "cpuCost": "NaN",
                           "memoryCost": "NaN",
                           "networkCost": "NaN"
                        }
                     ],
                     "children": [
                        {
                           "id": "200",
                           "name": "RemoteExchange",
                           "descriptor": {
                              "type": "REPARTITION",
                              "isReplicateNullsAndAny": "",
                              "hashColumn": "[$hashvalue_1]"
                           },
                           "outputs": [
                              {
                                 "symbol": "regionkey",
                                 "type": "bigint"
                              },
                              {
                                 "symbol": "count_0",
                                 "type": "bigint"
                              },
                              {
                                 "symbol": "$hashvalue_1",
                                 "type": "bigint"
                              }
                           ],
                           "details":[],
                           "estimates": [
                              {
                                 "outputRowCount": "NaN",
                                 "outputSizeInBytes": "NaN",
                                 "cpuCost": "NaN",
                                 "memoryCost": "NaN",
                                 "networkCost": "NaN"
                              }
                           ],
                           "children": [
                              {
                                 "id": "226",
                                 "name": "Project",
                                 "descriptor": {}
                                 "outputs": [
                                    {
                                       "symbol": "regionkey",
                                       "type": "bigint"
                                    },
                                    {
                                       "symbol": "count_0",
                                       "type": "bigint"
                                    },
                                    {
                                       "symbol": "$hashvalue_2",
                                       "type": "bigint"
                                    }
                                 ],
                                 "details": [
                                    "$hashvalue_2 := combine_hash(bigint '0', COALESCE(\"$operator$hash_code\"(\"regionkey\"), 0))"
                                 ],
                                 "estimates": [
                                    {
                                       "outputRowCount": "NaN",
                                       "outputSizeInBytes": "NaN",
                                       "cpuCost": "NaN",
                                       "memoryCost": "NaN",
                                       "networkCost": "NaN"
                                    }
                                 ],
                                 "children": [
                                    {
                                       "id": "198",
                                       "name": "Aggregate",
                                       "descriptor": {
                                          "type": "PARTIAL",
                                          "keys": "[regionkey]",
                                          "hash": ""
                                       },
                                       "outputs": [
                                          {
                                             "symbol": "regionkey",
                                             "type": "bigint"
                                          },
                                          {
                                             "symbol": "count_0",
                                             "type": "bigint"
                                          }
                                       ],
                                       "details": [
                                          "count_0 := count(*)"
                                       ],
                                       "estimates":[],
                                       "children": [
                                          {
                                             "id": "0",
                                             "name": "TableScan",
                                             "descriptor": {
                                                "table": "hive:tpch_sf1_orc_part:nation"
                                             },
                                             "outputs": [
                                                {
                                                   "symbol": "regionkey",
                                                   "type": "bigint"
                                                }
                                             ],
                                             "details": [
                                                "regionkey := regionkey:bigint:REGULAR"
                                             ],
                                             "estimates": [
                                                {
                                                   "outputRowCount": 25,
                                                   "outputSizeInBytes": 225,
                                                   "cpuCost": 225,
                                                   "memoryCost": 0,
                                                   "networkCost": 0
                                                }
                                             ],
                                             "children": []
                                          }
                                       ]
                                    }
                                 ]
                              }
                           ]
                        }
                     ]
                  }
               ]
            }
         ]
      }
   ]
}
```
```
EXPLAIN (TYPE DISTRIBUTED) SELECT regionkey, count(*) FROM nation GROUP BY 1;
```
```
Query Plan
------------------------------------------------------------------------------------------------------
 Trino version: version
 Fragment 0 [SINGLE]
     Output layout: [regionkey, count]
     Output partitioning: SINGLE []
     Output[regionkey, _col1]
     │   Layout: [regionkey:bigint, count:bigint]
     │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
     │   _col1 := count
     └─ RemoteSource[1]
            Layout: [regionkey:bigint, count:bigint]

 Fragment 1 [HASH]
     Output layout: [regionkey, count]
     Output partitioning: SINGLE []
     Aggregate(FINAL)[regionkey]
     │   Layout: [regionkey:bigint, count:bigint]
     │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
     │   count := count("count_8")
     └─ LocalExchange[HASH][$hashvalue] ("regionkey")
        │   Layout: [regionkey:bigint, count_8:bigint, $hashvalue:bigint]
        │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
        └─ RemoteSource[2]
               Layout: [regionkey:bigint, count_8:bigint, $hashvalue_9:bigint]

 Fragment 2 [SOURCE]
     Output layout: [regionkey, count_8, $hashvalue_10]
     Output partitioning: HASH [regionkey][$hashvalue_10]
     Project[]
     │   Layout: [regionkey:bigint, count_8:bigint, $hashvalue_10:bigint]
     │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: ?}
     │   $hashvalue_10 := "combine_hash"(bigint '0', COALESCE("$operator$hash_code"("regionkey"), 0))
     └─ Aggregate(PARTIAL)[regionkey]
        │   Layout: [regionkey:bigint, count_8:bigint]
        │   count_8 := count(*)
        └─ TableScan[tpch:nation:sf0.01, grouped = false]
               Layout: [regionkey:bigint]
               Estimates: {rows: 25 (225B), cpu: 225, memory: 0B, network: 0B}
               regionkey := tpch:regionkey
```
```
EXPLAIN (TYPE DISTRIBUTED, FORMAT JSON) SELECT regionkey, count(*) FROM nation GROUP BY 1;
```
```
{
   "0" : {
      "id" : "9",
      "name" : "Output",
      "descriptor" : {
         "columnNames" : "[regionkey, _col1]"
      },
      "outputs" : [ {
         "symbol" : "regionkey",
         "type" : "bigint"
      }, {
         "symbol" : "count",
         "type" : "bigint"
      } ],
      "details" : [ "_col1 := count" ],
      "estimates" : [ {
         "outputRowCount" : "NaN",
         "outputSizeInBytes" : "NaN",
         "cpuCost" : "NaN",
         "memoryCost" : "NaN",
         "networkCost" : "NaN"
      } ],
      "children" : [ {
         "id" : "145",
         "name" : "RemoteSource",
         "descriptor" : {
            "sourceFragmentIds" : "[1]"
         },
         "outputs" : [ {
            "symbol" : "regionkey",
            "type" : "bigint"
         }, {
            "symbol" : "count",
            "type" : "bigint"
         } ],
         "details" : [ ],
         "estimates" : [ ],
         "children" : [ ]
      } ]
   },
   "1" : {
      "id" : "4",
      "name" : "Aggregate",
      "descriptor" : {
         "type" : "FINAL",
         "keys" : "[regionkey]",
         "hash" : "[]"
      },
      "outputs" : [ {
         "symbol" : "regionkey",
         "type" : "bigint"
      }, {
         "symbol" : "count",
         "type" : "bigint"
      } ],
      "details" : [ "count := count(\"count_0\")" ],
      "estimates" : [ {
         "outputRowCount" : "NaN",
         "outputSizeInBytes" : "NaN",
         "cpuCost" : "NaN",
         "memoryCost" : "NaN",
         "networkCost" : "NaN"
      } ],
      "children" : [ {
         "id" : "194",
         "name" : "LocalExchange",
         "descriptor" : {
            "partitioning" : "SINGLE",
            "isReplicateNullsAndAny" : "",
            "hashColumn" : "[]",
            "arguments" : "[]"
         },
         "outputs" : [ {
            "symbol" : "regionkey",
            "type" : "bigint"
         }, {
            "symbol" : "count_0",
            "type" : "bigint"
         } ],
         "details" : [ ],
         "estimates" : [ {
            "outputRowCount" : "NaN",
            "outputSizeInBytes" : "NaN",
            "cpuCost" : "NaN",
            "memoryCost" : "NaN",
            "networkCost" : "NaN"
         } ],
         "children" : [ {
            "id" : "227",
            "name" : "Project",
            "descriptor" : { },
            "outputs" : [ {
               "symbol" : "regionkey",
               "type" : "bigint"
            }, {
               "symbol" : "count_0",
               "type" : "bigint"
            } ],
            "details" : [ ],
            "estimates" : [ {
               "outputRowCount" : "NaN",
               "outputSizeInBytes" : "NaN",
               "cpuCost" : "NaN",
               "memoryCost" : "NaN",
               "networkCost" : "NaN"
            } ],
            "children" : [ {
               "id" : "200",
               "name" : "RemoteSource",
               "descriptor" : {
                  "sourceFragmentIds" : "[2]"
               },
               "outputs" : [ {
                  "symbol" : "regionkey",
                  "type" : "bigint"
               }, {
                  "symbol" : "count_0",
                  "type" : "bigint"
               }, {
                  "symbol" : "$hashvalue",
                  "type" : "bigint"
               } ],
               "details" : [ ],
               "estimates" : [ ],
               "children" : [ ]
            } ]
         } ]
      } ]
   },
   "2" : {
      "id" : "226",
      "name" : "Project",
      "descriptor" : { },
      "outputs" : [ {
         "symbol" : "regionkey",
         "type" : "bigint"
      }, {
         "symbol" : "count_0",
         "type" : "bigint"
      }, {
         "symbol" : "$hashvalue_1",
         "type" : "bigint"
      } ],
      "details" : [ "$hashvalue_1 := combine_hash(bigint '0', COALESCE(\"$operator$hash_code\"(\"regionkey\"), 0))" ],
      "estimates" : [ {
         "outputRowCount" : "NaN",
         "outputSizeInBytes" : "NaN",
         "cpuCost" : "NaN",
         "memoryCost" : "NaN",
         "networkCost" : "NaN"
      } ],
      "children" : [ {
         "id" : "198",
         "name" : "Aggregate",
         "descriptor" : {
            "type" : "PARTIAL",
            "keys" : "[regionkey]",
            "hash" : "[]"
         },
         "outputs" : [ {
            "symbol" : "regionkey",
            "type" : "bigint"
         }, {
            "symbol" : "count_0",
            "type" : "bigint"
         } ],
         "details" : [ "count_0 := count(*)" ],
         "estimates" : [ ],
         "children" : [ {
            "id" : "0",
            "name" : "TableScan",
            "descriptor" : {
               "table" : "tpch:tiny:nation"
            },
            "outputs" : [ {
               "symbol" : "regionkey",
               "type" : "bigint"
            } ],
            "details" : [ "regionkey := tpch:regionkey" ],
            "estimates" : [ {
               "outputRowCount" : 25.0,
               "outputSizeInBytes" : 225.0,
               "cpuCost" : 225.0,
               "memoryCost" : 0.0,
               "networkCost" : 0.0
            } ],
            "children" : [ ]
         } ]
      } ]
   }
}
```
```
EXPLAIN (TYPE VALIDATE) SELECT regionkey, count(*) FROM nation GROUP BY 1;
```
```
Valid
-------
 true
```
```
EXPLAIN (TYPE VALIDATE) SELET 1=0;
```
```
Query 20220929_234840_00001_vjwxj failed: line 1:25: mismatched input 'SELET'.
Expecting: 'ALTER', 'ANALYZE', 'CALL', 'COMMENT', 'COMMIT', 'CREATE',
'DEALLOCATE', 'DELETE', 'DENY', 'DESC', 'DESCRIBE', 'DROP', 'EXECUTE',
'EXPLAIN', 'GRANT', 'INSERT', 'MERGE', 'PREPARE', 'REFRESH', 'RESET',
'REVOKE', 'ROLLBACK', 'SET', 'SHOW', 'START', 'TRUNCATE', 'UPDATE', 'USE',
<query>
```
```
EXPLAIN(TYPE VALIDATE) SELECT * FROM tpch.tiny.nations;
```
```
Query 20220929_235059_00003_vjwxj failed: line 1:15: Table 'tpch.tiny.nations' does not exist
SELECT * FROM tpch.tiny.nations
```
```
EXPLAIN (TYPE IO, FORMAT JSON) INSERT INTO test_lineitem
SELECT * FROM lineitem WHERE shipdate = '2020-02-01' AND quantity > 10;
```
```
Query Plan
-----------------------------------
{
   inputTableColumnInfos: [
      {
         table: {
            catalog: "hive",
            schemaTable: {
               schema: "tpch",
               table: "test_orders"
            }
         },
         columnConstraints: [
            {
               columnName: "orderkey",
               type: "bigint",
               domain: {
                  nullsAllowed: false,
                  ranges: [
                     {
                        low: {
                           value: "1",
                           bound: "EXACTLY"
                        },
                        high: {
                           value: "1",
                           bound: "EXACTLY"
                        }
                     },
                     {
                        low: {
                           value: "2",
                           bound: "EXACTLY"
                        },
                        high: {
                           value: "2",
                           bound: "EXACTLY"
                        }
                     }
                  ]
               }
            },
            {
               columnName: "processing",
               type: "boolean",
               domain: {
                  nullsAllowed: false,
                  ranges: [
                     {
                        low: {
                           value: "false",
                           bound: "EXACTLY"
                        },
                        high: {
                           value: "false",
                           bound: "EXACTLY"
                        }
                     }
                  ]
               }
            },
            {
               columnName: "custkey",
               type: "bigint",
               domain: {
                  nullsAllowed: false,
                  ranges: [
                     {
                        low: {
                           bound: "ABOVE"
                        },
                        high: {
                           value: "10",
                           bound: "EXACTLY"
                        }
                     }
                  ]
               }
            }
         ],
         estimate: {
            outputRowCount: 2,
            outputSizeInBytes: 40,
            cpuCost: 40,
            maxMemory: 0,
            networkCost: 0
         }
      }
   ],
   outputTable: {
      catalog: "hive",
      schemaTable: {
         schema: "tpch",
         table: "test_orders"
      }
   },
   estimate: {
      outputRowCount: "NaN",
      outputSizeInBytes: "NaN",
      cpuCost: "NaN",
      maxMemory: "NaN",
      networkCost: "NaN"
   }
}
```


---

### EXPLAIN ANALYZE
Source: https://trino.io/docs/current/sql/explain-analyze.html

EXPLAIN ANALYZE# Synopsis# EXPLAIN ANALYZE [VERBOSE] statement Description# Execute the statement and show the distributed execution plan of the statement along with the cost of each operation. The VERBOSE option will give more detailed information and low-level statistics; understanding these may require knowledge of Trino internals and implementation details. Note The stats may not be entirely accurate, especially for queries that complete quickly. Examples# In the example below, you can see the CPU time spent in each stage, as well as the relative cost of each plan node in the stage. Note that the relative cost of the plan nodes is based on wall time, which may or may not be correlated to CPU time. For each plan node you can see some additional statistics (e.g: average input per node instance). Such statistics are useful when one wants to detect data anomalies for a query (e.g: skewness). EXPLAIN ANALYZE SELECT count(*), clerk FROM orders WHERE orderdate > date '1995-01-01' GROUP BY clerk; Query Plan ----------------------------------------------------------------------------------------------- Trino version: version Queued: 374.17us, Analysis: 190.96ms, Planning: 179.03ms, Execution: 3.06s Fragment 1 [HASH] CPU: 22.58ms, Scheduled: 96.72ms, Blocked 46.21s (Input: 23.06s, Output: 0.00ns), Input: 1000 rows (37.11kB); per task: avg.: 1000.00 std.dev.: 0.00, Output: 1000 rows (28.32kB) Output layout: [clerk, count] Output partitioning: SINGLE [] Project[] │ Layout: [clerk:varchar(15), count:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B} │ CPU: 8.00ms (3.51%), Scheduled: 63.00ms (15.11%), Blocked: 0.00ns (0.00%), Output: 1000 rows (28.32kB) │ Input avg.: 15.63 rows, Input std.dev.: 24.36% └─ Aggregate[type = FINAL, keys = [clerk], hash = [$hashvalue]] │ Layout: [clerk:varchar(15), $hashvalue:bigint, count:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: ?, network: 0B} │ CPU: 8.00ms (3.51%), Scheduled: 22.00ms (5.28%), Blocked: 0.00ns (0.00%), Output: 1000 rows (37.11kB) │ Input avg.: 15.63 rows, Input std.dev.: 24.36% │ count := count("count_0") └─ LocalExchange[partitioning = HASH, hashColumn = [$hashvalue], arguments = ["clerk"]] │ Layout: [clerk:varchar(15), count_0:bigint, $hashvalue:bigint] │ Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B} │ CPU: 2.00ms (0.88%), Scheduled: 4.00ms (0.96%), Blocked: 23.15s (50.10%), Output: 1000 rows (37.11kB) │ Input avg.: 15.63 rows, Input std.dev.: 793.73% └─ RemoteSource[sourceFragmentIds = [2]] Layout: [clerk:varchar(15), count_0:bigint, $hashvalue_1:bigint] CPU: 0.00ns (0.00%), Scheduled: 0.00ns (0.00%), Blocked: 23.06s (49.90%), Output: 1000 rows (37.11kB) Input avg.: 15.63 rows, Input std.dev.: 793.73% Fragment 2 [SOURCE] CPU: 210.60ms, Scheduled: 327.92ms, Blocked 0.00ns (Input: 0.00ns, Output: 0.00ns), Input: 1500000 rows (18.17MB); per task: avg.: 1500000.00 std.dev.: 0.00, Output: 1000 rows (37.11kB) Output layout: [clerk, count_0, $hashvalue_2] Output partitioning: HASH [clerk][$hashvalue_2] Aggregate[type = PARTIAL, keys = [clerk], hash = [$hashvalue_2]] │ Layout: [clerk:varchar(15), $hashvalue_2:bigint, count_0:bigint] │ CPU: 30.00ms (13.16%), Scheduled: 30.00ms (7.19%), Blocked: 0.00ns (0.00%), Output: 1000 rows (37.11kB) │ Input avg.: 818058.00 rows, Input std.dev.: 0.00% │ count_0 := count(*) └─ ScanFilterProject[table = hive:sf1:orders, filterPredicate = ("orderdate" > DATE '1995-01-01')] Layout: [clerk:varchar(15), $hashvalue_2:bigint] Estimates: {rows: 1500000 (41.48MB), cpu: 35.76M, memory: 0B, network: 0B}/{rows: 816424 (22.58MB), cpu: 35.76M, memory: 0B, network: 0B}/{rows: 816424 (22.58MB), cpu: 22.58M, memory: 0B, network: 0B} CPU: 180.00ms (78.95%), Scheduled: 298.00ms (71.46%), Blocked: 0.00ns (0.00%), Output: 818058 rows (12.98MB) Input avg.: 1500000.00 rows, Input std.dev.: 0.00% $hashvalue_2 := combine_hash(bigint '0', COALESCE("$operator$hash_code"("clerk"), 0)) clerk := clerk:varchar(15):REGULAR orderdate := orderdate:date:REGULAR Input: 1500000 rows (18.17MB), Filtered: 45.46%, Physical Input: 4.51MB When the VERBOSE option is used, some operators may report additional information. For example, the window function operator will output the following: EXPLAIN ANALYZE VERBOSE SELECT count(clerk) OVER() FROM orders WHERE orderdate > date '1995-01-01'; Query Plan ----------------------------------------------------------------------------------------------- ... ─ Window[] │ Layout: [clerk:varchar(15), count:bigint] │ CPU: 157.00ms (53.40%), Scheduled: 158.00ms (37.71%), Blocked: 0.00ns (0.00%), Output: 818058 rows (22.62MB) │ metrics: │ 'CPU time distribution (s)' = {count=1.00, p01=0.16, p05=0.16, p10=0.16, p25=0.16, p50=0.16, p75=0.16, p90=0.16, p95=0.16, p99=0.16, min=0.16, max=0.16} │ 'Input rows distribution' = {count=1.00, p01=818058.00, p05=818058.00, p10=818058.00, p25=818058.00, p50=818058.00, p75=818058.00, p90=818058.00, p95=818058.00, p99=818058.00, min=818058.00, max=818058.00} │ 'Scheduled time distribution (s)' = {count=1.00, p01=0.16, p05=0.16, p10=0.16, p25=0.16, p50=0.16, p75=0.16, p90=0.16, p95=0.16, p99=0.16, min=0.16, max=0.16} │ Input avg.: 818058.00 rows, Input std.dev.: 0.00% │ Active Drivers: [ 1 / 1 ] │ Index size: std.dev.: 0.00 bytes, 0.00 rows │ Index count per driver: std.dev.: 0.00 │ Rows per driver: std.dev.: 0.00 │ Size of partition: std.dev.: 0.00 │ count := count("clerk") RANGE UNBOUNDED_PRECEDING CURRENT_ROW ... See also# EXPLAIN

#### Code Examples

```
EXPLAIN ANALYZE [VERBOSE] statement
```
```
EXPLAIN ANALYZE SELECT count(*), clerk FROM orders
WHERE orderdate > date '1995-01-01' GROUP BY clerk;
```
```
Query Plan
-----------------------------------------------------------------------------------------------
Trino version: version
Queued: 374.17us, Analysis: 190.96ms, Planning: 179.03ms, Execution: 3.06s
Fragment 1 [HASH]
    CPU: 22.58ms, Scheduled: 96.72ms, Blocked 46.21s (Input: 23.06s, Output: 0.00ns), Input: 1000 rows (37.11kB); per task: avg.: 1000.00 std.dev.: 0.00, Output: 1000 rows (28.32kB)
    Output layout: [clerk, count]
    Output partitioning: SINGLE []
    Project[]
    │   Layout: [clerk:varchar(15), count:bigint]
    │   Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B}
    │   CPU: 8.00ms (3.51%), Scheduled: 63.00ms (15.11%), Blocked: 0.00ns (0.00%), Output: 1000 rows (28.32kB)
    │   Input avg.: 15.63 rows, Input std.dev.: 24.36%
    └─ Aggregate[type = FINAL, keys = [clerk], hash = [$hashvalue]]
       │   Layout: [clerk:varchar(15), $hashvalue:bigint, count:bigint]
       │   Estimates: {rows: ? (?), cpu: ?, memory: ?, network: 0B}
       │   CPU: 8.00ms (3.51%), Scheduled: 22.00ms (5.28%), Blocked: 0.00ns (0.00%), Output: 1000 rows (37.11kB)
       │   Input avg.: 15.63 rows, Input std.dev.: 24.36%
       │   count := count("count_0")
       └─ LocalExchange[partitioning = HASH, hashColumn = [$hashvalue], arguments = ["clerk"]]
          │   Layout: [clerk:varchar(15), count_0:bigint, $hashvalue:bigint]
          │   Estimates: {rows: ? (?), cpu: ?, memory: 0B, network: 0B}
          │   CPU: 2.00ms (0.88%), Scheduled: 4.00ms (0.96%), Blocked: 23.15s (50.10%), Output: 1000 rows (37.11kB)
          │   Input avg.: 15.63 rows, Input std.dev.: 793.73%
          └─ RemoteSource[sourceFragmentIds = [2]]
                 Layout: [clerk:varchar(15), count_0:bigint, $hashvalue_1:bigint]
                 CPU: 0.00ns (0.00%), Scheduled: 0.00ns (0.00%), Blocked: 23.06s (49.90%), Output: 1000 rows (37.11kB)
                 Input avg.: 15.63 rows, Input std.dev.: 793.73%

Fragment 2 [SOURCE]
    CPU: 210.60ms, Scheduled: 327.92ms, Blocked 0.00ns (Input: 0.00ns, Output: 0.00ns), Input: 1500000 rows (18.17MB); per task: avg.: 1500000.00 std.dev.: 0.00, Output: 1000 rows (37.11kB)
    Output layout: [clerk, count_0, $hashvalue_2]
    Output partitioning: HASH [clerk][$hashvalue_2]
    Aggregate[type = PARTIAL, keys = [clerk], hash = [$hashvalue_2]]
    │   Layout: [clerk:varchar(15), $hashvalue_2:bigint, count_0:bigint]
    │   CPU: 30.00ms (13.16%), Scheduled: 30.00ms (7.19%), Blocked: 0.00ns (0.00%), Output: 1000 rows (37.11kB)
    │   Input avg.: 818058.00 rows, Input std.dev.: 0.00%
    │   count_0 := count(*)
    └─ ScanFilterProject[table = hive:sf1:orders, filterPredicate = ("orderdate" > DATE '1995-01-01')]
           Layout: [clerk:varchar(15), $hashvalue_2:bigint]
           Estimates: {rows: 1500000 (41.48MB), cpu: 35.76M, memory: 0B, network: 0B}/{rows: 816424 (22.58MB), cpu: 35.76M, memory: 0B, network: 0B}/{rows: 816424 (22.58MB), cpu: 22.58M, memory: 0B, network: 0B}
           CPU: 180.00ms (78.95%), Scheduled: 298.00ms (71.46%), Blocked: 0.00ns (0.00%), Output: 818058 rows (12.98MB)
           Input avg.: 1500000.00 rows, Input std.dev.: 0.00%
           $hashvalue_2 := combine_hash(bigint '0', COALESCE("$operator$hash_code"("clerk"), 0))
           clerk := clerk:varchar(15):REGULAR
           orderdate := orderdate:date:REGULAR
           Input: 1500000 rows (18.17MB), Filtered: 45.46%, Physical Input: 4.51MB
```
```
EXPLAIN ANALYZE VERBOSE SELECT count(clerk) OVER() FROM orders
WHERE orderdate > date '1995-01-01';
```
```
Query Plan
-----------------------------------------------------------------------------------------------
  ...
         ─ Window[]
           │   Layout: [clerk:varchar(15), count:bigint]
           │   CPU: 157.00ms (53.40%), Scheduled: 158.00ms (37.71%), Blocked: 0.00ns (0.00%), Output: 818058 rows (22.62MB)
           │   metrics:
           │     'CPU time distribution (s)' = {count=1.00, p01=0.16, p05=0.16, p10=0.16, p25=0.16, p50=0.16, p75=0.16, p90=0.16, p95=0.16, p99=0.16, min=0.16, max=0.16}
           │     'Input rows distribution' = {count=1.00, p01=818058.00, p05=818058.00, p10=818058.00, p25=818058.00, p50=818058.00, p75=818058.00, p90=818058.00, p95=818058.00, p99=818058.00, min=818058.00, max=818058.00}
           │     'Scheduled time distribution (s)' = {count=1.00, p01=0.16, p05=0.16, p10=0.16, p25=0.16, p50=0.16, p75=0.16, p90=0.16, p95=0.16, p99=0.16, min=0.16, max=0.16}
           │   Input avg.: 818058.00 rows, Input std.dev.: 0.00%
           │   Active Drivers: [ 1 / 1 ]
           │   Index size: std.dev.: 0.00 bytes, 0.00 rows
           │   Index count per driver: std.dev.: 0.00
           │   Rows per driver: std.dev.: 0.00
           │   Size of partition: std.dev.: 0.00
           │   count := count("clerk") RANGE UNBOUNDED_PRECEDING CURRENT_ROW
 ...
```


---

### MATCH_RECOGNIZE
Source: https://trino.io/docs/current/sql/match-recognize.html

MATCH_RECOGNIZE# Synopsis# MATCH_RECOGNIZE ( [ PARTITION BY column [, ...] ] [ ORDER BY column [, ...] ] [ MEASURES measure_definition [, ...] ] [ rows_per_match ] [ AFTER MATCH skip_to ] PATTERN ( row_pattern ) [ SUBSET subset_definition [, ...] ] DEFINE variable_definition [, ...] ) Description# The MATCH_RECOGNIZE clause is an optional subclause of the FROM clause. It is used to detect patterns in a set of rows. Patterns of interest are specified using row pattern syntax based on regular expressions. The input to pattern matching is a table, a view or a subquery. For each detected match, one or more rows are returned. They contain requested information about the match. Row pattern matching is a powerful tool when analyzing complex sequences of events. The following examples show some of the typical use cases: in trade applications, tracking trends or identifying customers with specific behavioral patterns in shipping applications, tracking packages through all possible valid paths, in financial applications, detecting unusual incidents, which might signal fraud Example# In the following example, the pattern describes a V-shape over the totalprice column. A match is found whenever orders made by a customer first decrease in price, and then increase past the starting point: SELECT * FROM orders MATCH_RECOGNIZE( PARTITION BY custkey ORDER BY orderdate MEASURES A.totalprice AS starting_price, LAST(B.totalprice) AS bottom_price, LAST(U.totalprice) AS top_price ONE ROW PER MATCH AFTER MATCH SKIP PAST LAST ROW PATTERN (A B+ C+ D+) SUBSET U = (C, D) DEFINE B AS totalprice < PREV(totalprice), C AS totalprice > PREV(totalprice) AND totalprice <= A.totalprice, D AS totalprice > PREV(totalprice) ) In the following sections, all subclauses of the MATCH_RECOGNIZE clause are explained with this example query. Partitioning and ordering# PARTITION BY custkey The PARTITION BY clause allows you to break up the input table into separate sections, that are independently processed for pattern matching. Without a partition declaration, the whole input table is used. This behavior is analogous to the semantics of PARTITION BY clause in window specification. In the example, the orders table is partitioned by the custkey value, so that pattern matching is performed for all orders of a specific customer independently from orders of other customers. ORDER BY orderdate The optional ORDER BY clause is generally useful to allow matching on an ordered data set. For example, sorting the input by orderdate allows for matching on a trend of changes over time. Row pattern measures# The MEASURES clause allows to specify what information is retrieved from a matched sequence of rows. MEASURES measure_expression AS measure_name [, ...] A measure expression is a scalar expression whose value is computed based on a match. In the example, three row pattern measures are specified: A.totalprice AS starting_price returns the price in the first row of the match, which is the only row associated with A according to the pattern. LAST(B.totalprice) AS bottom_price returns the lowest price (corresponding to the bottom of the “V” in the pattern). It is the price in the last row associated with B, which is the last row of the descending section. LAST(U.totalprice) AS top_price returns the highest price in the match. It is the price in the last row associated with C or D, which is also the final row of the match. Measure expressions can refer to the columns of the input table. They also allow special syntax to combine the input information with the details of the match (see Row pattern recognition expressions). Each measure defines an output column of the pattern recognition. The column can be referenced with the measure_name. The MEASURES clause is optional. When no measures are specified, certain input columns (depending on ROWS PER MATCH clause) are the output of the pattern recognition. Rows per match# This clause can be used to specify the quantity of output rows. There are two main options: ONE ROW PER MATCH and ALL ROWS PER MATCH ONE ROW PER MATCH is the default option. For every match, a single row of output is produced. Output consists of PARTITION BY columns and measures. The output is also produced for empty matches, based on their starting rows. Rows that are unmatched (that is, neither included in some non-empty match, nor being the starting row of an empty match), are not included in the output. For ALL ROWS PER MATCH, every row of a match produces an output row, unless it is excluded from the output by the exclusion syntax. Output consists of PARTITION BY columns, ORDER BY columns, measures and remaining columns from the input table. By default, empty matches are shown and unmatched rows are skipped, similarly as with the ONE ROW PER MATCH option. However, this behavior can be changed by modifiers: ALL ROWS PER MATCH SHOW EMPTY MATCHES shows empty matches and skips unmatched rows, like the default. ALL ROWS PER MATCH OMIT EMPTY MATCHES excludes empty matches from the output. ALL ROWS PER MATCH WITH UNMATCHED ROWS shows empty matches and produces additional output row for each unmatched row. There are special rules for computing row pattern measures for empty matches and unmatched rows. They are explained in Evaluating expressions in empty matches and unmatched rows. Unmatched rows can only occur when the pattern does not allow an empty match. Otherwise, they are considered as starting rows of empty matches. The option ALL ROWS PER MATCH WITH UNMATCHED ROWS is recommended when pattern recognition is expected to pass all input rows, and it is not certain whether the pattern allows an empty match. After match skip# The AFTER MATCH SKIP clause specifies where pattern matching resumes after a non-empty match is found. The default option is: AFTER MATCH SKIP PAST LAST ROW With this option, pattern matching starts from the row after the last row of the match. Overlapping matches are not detected. With the following option, pattern matching starts from the second row of the match: AFTER MATCH SKIP TO NEXT ROW In the example, if a V-shape is detected, further overlapping matches are found, starting from consecutive rows on the descending slope of the “V”. Skipping to the next row is the default behavior after detecting an empty match or unmatched row. The following AFTER MATCH SKIP options allow to resume pattern matching based on the components of the pattern. Pattern matching starts from the last (default) or first row matched to a certain row pattern variable. It can be either a primary pattern variable (they are explained in Row pattern syntax) or a union variable: AFTER MATCH SKIP TO [ FIRST | LAST ] pattern_variable It is forbidden to skip to the first row of the current match, because it results in an infinite loop. For example specifying AFTER MATCH SKIP TO A fails, because A is the first element of the pattern, and jumping back to it creates an infinite loop. Similarly, skipping to a pattern variable which is not present in the match causes failure. All other options than the default AFTER MATCH SKIP PAST LAST ROW allow detection of overlapping matches. The combination of ALL ROWS PER MATCH WITH UNMATCHED ROWS with AFTER MATCH SKIP PAST LAST ROW is the only configuration that guarantees exactly one output row for each input row. Row pattern syntax# Row pattern is a form of a regular expression with some syntactical extensions specific to row pattern recognition. It is specified in the PATTERN clause: PATTERN ( row_pattern ) The basic element of row pattern is a primary pattern variable. Like pattern matching in character strings searches for characters, pattern matching in row sequences searches for rows which can be “labeled” with certain primary pattern variables. A primary pattern variable has a form of an identifier and is defined by a boolean condition. This condition determines whether a particular input row can be mapped to this variable and take part in the match. In the example PATTERN (A B+ C+ D+), there are four primary pattern variables: A, B, C, and D. Row pattern syntax includes the following usage: concatenation# A B+ C+ D+ It is a sequence of components without operators between them. All components are matched in the same order as they are specified. alternation# A | B | C It is a sequence of components separated by |. Exactly one of the components is matched. In case when multiple components can be matched, the leftmost matching component is chosen. permutation# PERMUTE(A, B, C) It is equivalent to alternation of all permutations of its components. All components are matched in some order. If multiple matches are possible for different orderings of the components, the match is chosen based on the lexicographical order established by the order of components in the PERMUTE list. In the above example, the most preferred option is A B C, and the least preferred option is C B A. grouping# (A B C) partition start anchor# ^ partition end anchor# $ empty pattern# () exclusion syntax# {- row_pattern -} Exclusion syntax is used to specify portions of the match to exclude from the output. It is useful in combination with the ALL ROWS PER MATCH option, when only certain sections of the match are interesting. If you change the example to use ALL ROWS PER MATCH, and the pattern is modified to PATTERN (A {- B+ C+ -} D+), the result consists of the initial matched row and the trailing section of rows. Specifying pattern exclusions does not affect the computation of expressions in MEASURES and DEFINE clauses. Exclusions also do not affect pattern matching. They have the same semantics as regular grouping with parentheses. It is forbidden to specify pattern exclusions with the option ALL ROWS PER MATCH WITH UNMATCHED ROWS. quantifiers# Pattern quantifiers allow to specify the desired number of repetitions of a sub-pattern in a match. They are appended after the relevant pattern component: (A | B)* There are following row pattern quantifiers: zero or more repetitions: * one or more repetitions: + zero or one repetition: ? exact number of repetitions, specified by a non-negative integer number: {n} number of repetitions ranging between bounds, specified by non-negative integer numbers: {m, n} Specifying bounds is optional. If the left bound is omitted, it defaults to 0. So, {, 5} can be described as “between zero and five repetitions”. If the right bound is omitted, the number of accepted repetitions is unbounded. So, {5, } can be described as “at least five repetitions”. Also, {,} is equivalent to *. Quantifiers are greedy by default. It means that higher number of repetitions is preferred over lower number. This behavior can be changed to reluctant by appending ? immediately after the quantifier. With {3, 5}, 3 repetitions is the least desired option and 5 repetitions – the most desired. With {3, 5}?, 3 repetitions are most desired. Similarly, ? prefers 1 repetition, while ?? prefers 0 repetitions. Row pattern union variables# As explained in Row pattern syntax, primary pattern variables are the basic elements of row pattern. In addition to primary pattern variables, you can define union variables. They are introduced in the SUBSET clause: SUBSET U = (C, D), ... In the preceding example, union variable U is defined as union of primary variables C and D. Union variables are useful in MEASURES, DEFINE and AFTER MATCH SKIP clauses. They allow you to refer to set of rows matched to either primary variable from a subset. With the pattern: PATTERN((A | B){5} C+) it cannot be determined upfront if the match contains any A or any B. A union variable can be used to access the last row matched to either A or B. Define SUBSET U = (A, B), and the expression LAST(U.totalprice) returns the value of the totalprice column from the last row mapped to either A or B. Also, AFTER MATCH SKIP TO LAST A or AFTER MATCH SKIP TO LAST B can result in failure if A or B is not present in the match. AFTER MATCH SKIP TO LAST U does not fail. Row pattern variable definitions# The DEFINE clause is where row pattern primary variables are defined. Each variable is associated with a boolean condition: DEFINE B AS totalprice < PREV(totalprice), ... During pattern matching, when a certain variable is considered for the next step of the match, the boolean condition is evaluated in context of the current match. If the result is true, then the current row, “labeled” with the variable, becomes part of the match. In the preceding example, assume that the pattern allows to match B at some point. There are some rows already matched to some pattern variables. Now, variable B is being considered for the current row. Before the match is made, the defining condition for B is evaluated. In this example, it is only true if the value of the totalprice column in the current row is lower than totalprice in the preceding row. The mechanism of matching variables to rows shows the difference between pattern matching in row sequences and regular expression matching in text. In text, characters remain constantly in their positions. In row pattern matching, a row can be mapped to different variables in different matches, depending on the preceding part of the match, and even on the match number. It is not required that every primary variable has a definition in the DEFINE clause. Variables not mentioned in the DEFINE clause are implicitly associated with true condition, which means that they can be matched to every row. Boolean expressions in the DEFINE clause allow the same special syntax as expressions in the MEASURES clause. Details are explained in Row pattern recognition expressions. Row pattern recognition expressions# Expressions in MEASURES and DEFINE clauses are scalar expressions evaluated over rows of the input table. They support special syntax, specific to pattern recognition context. They can combine input information with the information about the current match. Special syntax allows to access pattern variables assigned to rows, browse rows based on how they are matched, and refer to the sequential number of the match. pattern variable references# A.totalprice U.orderdate orderstatus A column name prefixed with a pattern variable refers to values of this column in all rows matched to this variable, or to any variable from the subset in case of union variable. If a column name is not prefixed, it is considered as prefixed with the universal pattern variable, defined as union of all primary pattern variables. In other words, a non-prefixed column name refers to all rows of the current match. It is forbidden to prefix a column name with a table name in the pattern recognition context. classifier function# CLASSIFIER() CLASSIFIER(A) CLASSIFIER(U) The classifier function returns the primary pattern variable associated with the row. The return type is varchar. The optional argument is a pattern variable. It limits the rows of interest, the same way as with prefixed column references. The classifier function is particularly useful with a union variable as the argument. It allows you to determine which variable from the subset actually matched. match_number function# MATCH_NUMBER() The match_number function returns the sequential number of the match within partition, starting from 1. Empty matches are assigned sequential numbers as well as non-empty matches. The return type is bigint. logical navigation functions# FIRST(A.totalprice, 2) In the above example, the first function navigates to the first row matched to pattern variable A, and then searches forward until it finds two more occurrences of variable A within the match. The result is the value of the totalprice column in that row. LAST(A.totalprice, 2) In the above example, the last function navigates to the last row matched to pattern variable A, and then searches backwards until it finds two more occurrences of variable A within the match. The result is the value of the totalprice column in that row. With the first and last functions the result is null, if the searched row is not found in the mach. The second argument is optional. The default value is 0, which means that by default these functions navigate to the first or last row of interest. If specified, the second argument must be a non-negative integer number. physical navigation functions# PREV(A.totalprice, 2) In the above example, the prev function navigates to the last row matched to pattern variable A, and then searches two rows backward. The result is the value of the totalprice column in that row. NEXT(A.totalprice, 2) In the above example, the next function navigates to the last row matched to pattern variable A, and then searches two rows forward. The result is the value of the totalprice column in that row. With the prev and next functions, it is possible to navigate and retrieve values outside the match. If the navigation goes beyond partition bounds, the result is null. The second argument is optional. The default value is 1, which means that by default these functions navigate to previous or next row. If specified, the second argument must be a non-negative integer number. nesting of navigation functions# It is possible to nest logical navigation functions within physical navigation functions: PREV(FIRST(A.totalprice, 3), 2) In case of nesting, first the logical navigation is performed. It establishes the starting row for the physical navigation. When both navigation operations succeed, the value is retrieved from the designated row. Pattern navigation functions require at least one column reference or classifier function inside of their first argument. The following examples are correct: LAST("pattern_variable_" || CLASSIFIER()) NEXT(U.totalprice + 10) This is incorrect: LAST(1) It is also required that all column references and all classifier calls inside a pattern navigation function are consistent in referred pattern variables. They must all refer either to the same primary variable, the same union variable, or to the implicit universal pattern variable. The following examples are correct: LAST(CLASSIFIER() = 'A' OR totalprice > 10) /* universal pattern variable */ LAST(CLASSIFIER(U) = 'A' OR U.totalprice > 10) /* pattern variable U */ This is incorrect: LAST(A.totalprice + B.totalprice) Aggregate functions# It is allowed to use aggregate functions in a row pattern recognition context. Aggregate functions are evaluated over all rows of the current match or over a subset of rows based on the matched pattern variables. The running and final semantics are supported, with running as the default. The following expression returns the average value of the totalprice column for all rows matched to pattern variable A: avg(A.totalprice) The following expression returns the average value of the totalprice column for all rows matched to pattern variables from subset U: avg(U.totalprice) The following expression returns the average value of the totalprice column for all rows of the match: avg(totalprice) Aggregation arguments# In case when the aggregate function has multiple arguments, it is required that all arguments refer consistently to the same set of rows: max_by(totalprice, tax) /* aggregate over all rows of the match */ max_by(CLASSIFIER(A), A.tax) /* aggregate over all rows matched to A */ This is incorrect: max_by(A.totalprice, tax) max_by(A.totalprice, A.tax + B.tax) If an aggregate argument does not contain any column reference or classifier function, it does not refer to any pattern variable. In such a case other aggregate arguments determine the set of rows to aggregate over. If none of the arguments contains a pattern variable reference, the universal row pattern variable is implicit. This means that the aggregate function applies to all rows of the match: count(1) /* aggregate over all rows of the match */ min_by(1, 2) /* aggregate over all rows of the match */ min_by(1, totalprice) /* aggregate over all rows of the match */ min_by(totalprice, 1) /* aggregate over all rows of the match */ min_by(A.totalprice, 1) /* aggregate over all rows matched to A */ max_by(1, A.totalprice) /* aggregate over all rows matched to A */ Nesting of aggregate functions# Aggregate function arguments must not contain pattern navigation functions. Similarly, aggregate functions cannot be nested in pattern navigation functions. Usage of the classifier and match_number functions# It is allowed to use the classifier and match_number functions in aggregate function arguments. The following expression returns an array containing all matched pattern variables: array_agg(CLASSIFIER()) This is particularly useful in combination with the option ONE ROW PER MATCH. It allows to get all the components of the match while keeping the output size reduced. Row pattern count aggregation# Like other aggregate functions in a row pattern recognition context, the count function can be applied to all rows of the match, or to rows associated with certain row pattern variables: count(*), count() /* count all rows of the match */ count(totalprice) /* count non-null values of the totalprice column in all rows of the match */ count(A.totalprice) /* count non-null values of the totalprice column in all rows matched to A */ The count function in a row pattern recognition context allows special syntax to support the count(*) behavior over a limited set of rows: count(A.*) /* count rows matched to A */ count(U.*) /* count rows matched to pattern variables from subset U */ RUNNING and FINAL semantics# During pattern matching in a sequence of rows, one row after another is examined to determine if it fits the pattern. At any step, a partial match is known, but it is not yet known what rows will be added in the future or what pattern variables they will be mapped to. So, when evaluating a boolean condition in the DEFINE clause for the current row, only the preceding part of the match (plus the current row) is “visible”. This is the running semantics. When evaluating expressions in the MEASURES clause, the match is complete. It is then possible to apply the final semantics. In the final semantics, the whole match is “visible” as from the position of the final row. In the MEASURES clause, the running semantics can also be applied. When outputting information row by row (as in ALL ROWS PER MATCH), the running semantics evaluate expressions from the positions of consecutive rows. The running and final semantics are denoted by the keywords: RUNNING and FINAL, preceding a logical navigation function first or last, or an aggregate function: RUNNING LAST(A.totalprice) FINAL LAST(A.totalprice) RUNNING avg(A.totalprice) FINAL count(A.*) The running semantics is default in MEASURES and DEFINE clauses. FINAL can only be specified in the MEASURES clause. With the option ONE ROW PER MATCH, row pattern measures are evaluated from the position of the final row in the match. Therefore, running and final semantics are the same. Evaluating expressions in empty matches and unmatched rows# An empty match occurs when the row pattern is successfully matched, but no pattern variables are assigned. The following pattern produces an empty match for every row: PATTERN(()) When evaluating row pattern measures for an empty match: all column references return null all navigation operations return null classifier function returns null match_number function returns the sequential number of the match all aggregate functions are evaluated over an empty set of rows Like every match, an empty match has its starting row. All input values which are to be output along with the measures (as explained in Rows per match), are the values from the starting row. An unmatched row is a row that is neither part of any non-empty match nor the starting row of an empty match. With the option ALL ROWS PER MATCH WITH UNMATCHED ROWS, a single output row is produced. In that row, all row pattern measures are null. All input values which are to be output along with the measures (as explained in Rows per match), are the values from the unmatched row. Using the match_number function as a measure can help differentiate between an empty match and unmatched row.

#### Code Examples

```
MATCH_RECOGNIZE (
  [ PARTITION BY column [, ...] ]
  [ ORDER BY column [, ...] ]
  [ MEASURES measure_definition [, ...] ]
  [ rows_per_match ]
  [ AFTER MATCH skip_to ]
  PATTERN ( row_pattern )
  [ SUBSET subset_definition [, ...] ]
  DEFINE variable_definition [, ...]
  )
```
```
SELECT * FROM orders MATCH_RECOGNIZE(
     PARTITION BY custkey
     ORDER BY orderdate
     MEASURES
              A.totalprice AS starting_price,
              LAST(B.totalprice) AS bottom_price,
              LAST(U.totalprice) AS top_price
     ONE ROW PER MATCH
     AFTER MATCH SKIP PAST LAST ROW
     PATTERN (A B+ C+ D+)
     SUBSET U = (C, D)
     DEFINE
              B AS totalprice < PREV(totalprice),
              C AS totalprice > PREV(totalprice) AND totalprice <= A.totalprice,
              D AS totalprice > PREV(totalprice)
     )
```
```
PARTITION BY custkey
```
```
ORDER BY orderdate
```
```
MEASURES measure_expression AS measure_name [, ...]
```
```
ONE ROW PER MATCH
```
```
ALL ROWS PER MATCH
```
```
ALL ROWS PER MATCH SHOW EMPTY MATCHES
```
```
ALL ROWS PER MATCH OMIT EMPTY MATCHES
```
```
ALL ROWS PER MATCH WITH UNMATCHED ROWS
```
```
AFTER MATCH SKIP PAST LAST ROW
```
```
AFTER MATCH SKIP TO NEXT ROW
```
```
AFTER MATCH SKIP TO [ FIRST | LAST ] pattern_variable
```
```
PATTERN ( row_pattern )
```
```
A B+ C+ D+
```
```
A | B | C
```
```
PERMUTE(A, B, C)
```
```
(A B C)
```
```
^
```
```
$
```
```
()
```
```
{- row_pattern -}
```
```
(A | B)*
```
```
*
```
```
+
```
```
?
```
```
{n}
```
```
{m, n}
```
```
SUBSET U = (C, D), ...
```
```
DEFINE B AS totalprice < PREV(totalprice), ...
```
```
A.totalprice

U.orderdate

orderstatus
```
```
CLASSIFIER()

CLASSIFIER(A)

CLASSIFIER(U)
```
```
MATCH_NUMBER()
```
```
FIRST(A.totalprice, 2)
```
```
LAST(A.totalprice, 2)
```
```
PREV(A.totalprice, 2)
```
```
NEXT(A.totalprice, 2)
```
```
PREV(FIRST(A.totalprice, 3), 2)
```
```
LAST("pattern_variable_" || CLASSIFIER())

NEXT(U.totalprice + 10)
```
```
LAST(1)
```
```
LAST(CLASSIFIER() = 'A' OR totalprice > 10) /* universal pattern variable */

LAST(CLASSIFIER(U) = 'A' OR U.totalprice > 10) /* pattern variable U */
```
```
LAST(A.totalprice + B.totalprice)
```
```
avg(A.totalprice)
```
```
avg(U.totalprice)
```
```
avg(totalprice)
```
```
max_by(totalprice, tax) /* aggregate over all rows of the match */

max_by(CLASSIFIER(A), A.tax) /* aggregate over all rows matched to A */
```
```
max_by(A.totalprice, tax)

max_by(A.totalprice, A.tax + B.tax)
```
```
count(1) /* aggregate over all rows of the match */

min_by(1, 2) /* aggregate over all rows of the match */

min_by(1, totalprice) /* aggregate over all rows of the match */

min_by(totalprice, 1) /* aggregate over all rows of the match */

min_by(A.totalprice, 1) /* aggregate over all rows matched to A */

max_by(1, A.totalprice) /* aggregate over all rows matched to A */
```
```
array_agg(CLASSIFIER())
```
```
count(*), count() /* count all rows of the match */

count(totalprice) /* count non-null values of the totalprice column
                     in all rows of the match */

count(A.totalprice) /* count non-null values of the totalprice column
                       in all rows matched to A */
```
```
count(A.*) /* count rows matched to A */

count(U.*) /* count rows matched to pattern variables from subset U */
```
```
RUNNING LAST(A.totalprice)

FINAL LAST(A.totalprice)

RUNNING avg(A.totalprice)

FINAL count(A.*)
```
```
PATTERN(())
```


---

### REFRESH MATERIALIZED VIEW
Source: https://trino.io/docs/current/sql/refresh-materialized-view.html

REFRESH MATERIALIZED VIEW# Synopsis# REFRESH MATERIALIZED VIEW view_name Description# Initially populate or refresh the data stored in the materialized view view_name. The materialized view must be defined with CREATE MATERIALIZED VIEW. Data is retrieved from the underlying tables accessed by the defined query. The initial population of the materialized view is typically processing intensive since it reads the data from the source tables and performs physical write operations. The refresh operation can be less intensive, if the underlying data has not changed and the connector has implemented a mechanism to be aware of that. The specific implementation and performance varies by connector used to create the materialized view. See also# CREATE MATERIALIZED VIEW DROP MATERIALIZED VIEW SHOW CREATE MATERIALIZED VIEW

#### Code Examples

```
REFRESH MATERIALIZED VIEW view_name
```


---

### USE
Source: https://trino.io/docs/current/sql/use.html

USE# Synopsis# USE catalog.schema USE schema Description# Update the session to use the specified catalog and schema. If a catalog is not specified, the schema is resolved relative to the current catalog. Examples# USE hive.finance; USE information_schema;

#### Code Examples

```
USE catalog.schema
USE schema
```
```
USE hive.finance;
USE information_schema;
```


---


## Connectors

### MySQL Connector Documentation
Source: https://trino.io/docs/current/connector/mysql.html

MySQL connector# The MySQL connector allows querying and creating tables in an external MySQL instance. This can be used to join data between different systems like MySQL and Hive, or between two different MySQL instances. Requirements# To connect to MySQL, you need: MySQL 5.7, 8.0 or higher. Network access from the Trino coordinator and workers to MySQL. Port 3306 is the default port. Configuration# To configure the MySQL connector, create a catalog properties file in etc/catalog named, for example, example.properties, to mount the MySQL connector as the mysql catalog. Create the file with the following contents, replacing the connection properties as appropriate for your setup: connector.name=mysql connection-url=jdbc:mysql://example.net:3306 connection-user=root connection-password=secret The connection-url defines the connection information and parameters to pass to the MySQL JDBC driver. The supported parameters for the URL are available in the MySQL Developer Guide. For example, the following connection-url allows you to require encrypted connections to the MySQL server: connection-url=jdbc:mysql://example.net:3306?sslMode=REQUIRED The connection-user and connection-password are typically required and determine the user credentials for the connection, often a service user. You can use secrets to avoid actual values in the catalog properties files. Connection security# If you have TLS configured with a globally-trusted certificate installed on your data source, you can enable TLS between your cluster and the data source by appending a parameter to the JDBC connection string set in the connection-url catalog configuration property. For example, with version 8.0 of MySQL Connector/J, use the sslMode parameter to secure the connection with TLS. By default the parameter is set to PREFERRED which secures the connection if enabled by the server. You can also set this parameter to REQUIRED which causes the connection to fail if TLS is not established. You can set the sslMode parameter in the catalog configuration file by appending it to the connection-url configuration property: connection-url=jdbc:mysql://example.net:3306/?sslMode=REQUIRED For more information on TLS configuration options, see the MySQL JDBC security documentation. Data source authentication# The connector can provide credentials for the data source connection in multiple ways: inline, in the connector configuration file in a separate properties file in a key store file as extra credentials set when connecting to Trino You can use secrets to avoid storing sensitive values in the catalog properties files. The following table describes configuration properties for connection credentials: Property name Description credential-provider.type Type of the credential provider. Must be one of INLINE, FILE, or KEYSTORE; defaults to INLINE. connection-user Connection user name. connection-password Connection password. user-credential-name Name of the extra credentials property, whose value to use as the user name. See extraCredentials in Parameter reference. password-credential-name Name of the extra credentials property, whose value to use as the password. connection-credential-file Location of the properties file where credentials are present. It must contain the connection-user and connection-password properties. keystore-file-path The location of the Java Keystore file, from which to read credentials. keystore-type File format of the keystore file, for example JKS or PEM. keystore-password Password for the key store. keystore-user-credential-name Name of the key store entity to use as the user name. keystore-user-credential-password Password for the user name key store entity. keystore-password-credential-name Name of the key store entity to use as the password. keystore-password-credential-password Password for the password key store entity. Multiple MySQL servers# You can have as many catalogs as you need, so if you have additional MySQL servers, simply add another properties file to etc/catalog with a different name, making sure it ends in .properties. For example, if you name the property file sales.properties, Trino creates a catalog named sales using the configured connector. General configuration properties# The following table describes general catalog configuration properties for the connector: Property name Description case-insensitive-name-matching Support case insensitive schema and table names. Defaults to false. case-insensitive-name-matching.cache-ttl Duration for which case insensitive schema and table names are cached. Defaults to 1m. case-insensitive-name-matching.config-file Path to a name mapping configuration file in JSON format that allows Trino to disambiguate between schemas and tables with similar names in different cases. Defaults to null. case-insensitive-name-matching.config-file.refresh-period Frequency with which Trino checks the name matching configuration file for changes. The duration value defaults to 0s (refresh disabled). metadata.cache-ttl Duration for which metadata, including table and column statistics, is cached. Defaults to 0s (caching disabled). metadata.cache-missing Cache the fact that metadata, including table and column statistics, is not available. Defaults to false. metadata.schemas.cache-ttl Duration for which schema metadata is cached. Defaults to the value of metadata.cache-ttl. metadata.tables.cache-ttl Duration for which table metadata is cached. Defaults to the value of metadata.cache-ttl. metadata.statistics.cache-ttl Duration for which tables statistics are cached. Defaults to the value of metadata.cache-ttl. metadata.cache-maximum-size Maximum number of objects stored in the metadata cache. Defaults to 10000. write.batch-size Maximum number of statements in a batched execution. Do not change this setting from the default. Non-default values may negatively impact performance. Defaults to 1000. dynamic-filtering.enabled Push down dynamic filters into JDBC queries. Defaults to true. dynamic-filtering.wait-timeout Maximum duration for which Trino waits for dynamic filters to be collected from the build side of joins before starting a JDBC query. Using a large timeout can potentially result in more detailed dynamic filters. However, it can also increase latency for some queries. Defaults to 20s. Appending query metadata# The optional parameter query.comment-format allows you to configure a SQL comment that is sent to the datasource with each query. The format of this comment can contain any characters and the following metadata: $QUERY_ID: The identifier of the query. $USER: The name of the user who submits the query to Trino. $SOURCE: The identifier of the client tool used to submit the query, for example trino-cli. $TRACE_TOKEN: The trace token configured with the client tool. The comment can provide more context about the query. This additional information is available in the logs of the datasource. To include environment variables from the Trino cluster with the comment , use the ${ENV:VARIABLE-NAME} syntax. The following example sets a simple comment that identifies each query sent by Trino: query.comment-format=Query sent by Trino. With this configuration, a query such as SELECT * FROM example_table; is sent to the datasource with the comment appended: SELECT * FROM example_table; /*Query sent by Trino.*/ The following example improves on the preceding example by using metadata: query.comment-format=Query $QUERY_ID sent by user $USER from Trino. If Jane sent the query with the query identifier 20230622_180528_00000_bkizg, the following comment string is sent to the datasource: SELECT * FROM example_table; /*Query 20230622_180528_00000_bkizg sent by user Jane from Trino.*/ Note Certain JDBC driver settings and logging configurations might cause the comment to be removed. Domain compaction threshold# Pushing down a large list of predicates to the data source can compromise performance. Trino compacts large predicates into a simpler range predicate by default to ensure a balance between performance and predicate pushdown. If necessary, the threshold for this compaction can be increased to improve performance when the data source is capable of taking advantage of large predicates. Increasing this threshold may improve pushdown of large dynamic filters. The domain-compaction-threshold catalog configuration property or the domain_compaction_threshold catalog session property can be used to adjust the default value of 256 for this threshold. Case insensitive matching# When case-insensitive-name-matching is set to true, Trino is able to query non-lowercase schemas and tables by maintaining a mapping of the lowercase name to the actual name in the remote system. However, if two schemas and/or tables have names that differ only in case (such as “customers” and “Customers”) then Trino fails to query them due to ambiguity. In these cases, use the case-insensitive-name-matching.config-file catalog configuration property to specify a configuration file that maps these remote schemas and tables to their respective Trino schemas and tables. Additionally, the JSON file must include both the schemas and tables properties, even if only as empty arrays. { "schemas": [ { "remoteSchema": "CaseSensitiveName", "mapping": "case_insensitive_1" }, { "remoteSchema": "cASEsENSITIVEnAME", "mapping": "case_insensitive_2" }], "tables": [ { "remoteSchema": "CaseSensitiveName", "remoteTable": "tablex", "mapping": "table_1" }, { "remoteSchema": "CaseSensitiveName", "remoteTable": "TABLEX", "mapping": "table_2" }] } Queries against one of the tables or schemes defined in the mapping attributes are run against the corresponding remote entity. For example, a query against tables in the case_insensitive_1 schema is forwarded to the CaseSensitiveName schema and a query against case_insensitive_2 is forwarded to the cASEsENSITIVEnAME schema. At the table mapping level, a query on case_insensitive_1.table_1 as configured above is forwarded to CaseSensitiveName.tablex, and a query on case_insensitive_1.table_2 is forwarded to CaseSensitiveName.TABLEX. By default, when a change is made to the mapping configuration file, Trino must be restarted to load the changes. Optionally, you can set the case-insensitive-name-matching.config-file.refresh-period to have Trino refresh the properties without requiring a restart: case-insensitive-name-matching.config-file.refresh-period=30s Fault-tolerant execution support# The connector supports Fault-tolerant execution of query processing. Read and write operations are both supported with any retry policy. Type mapping# Because Trino and MySQL each support types that the other does not, this connector modifies some types when reading or writing data. Data types may not map the same way in both directions between Trino and the data source. Refer to the following sections for type mapping in each direction. MySQL to Trino type mapping# The connector maps MySQL types to the corresponding Trino types following this table: MySQL to Trino type mapping# MySQL database type Trino type Notes BIT BOOLEAN BOOLEAN TINYINT TINYINT TINYINT TINYINT UNSIGNED SMALLINT SMALLINT SMALLINT SMALLINT UNSIGNED INTEGER INTEGER INTEGER INTEGER UNSIGNED BIGINT BIGINT BIGINT BIGINT UNSIGNED DECIMAL(20, 0) DOUBLE PRECISION DOUBLE FLOAT REAL REAL REAL DECIMAL(p, s) DECIMAL(p, s) See MySQL DECIMAL type handling CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) TINYTEXT VARCHAR(255) TEXT VARCHAR(65535) MEDIUMTEXT VARCHAR(16777215) LONGTEXT VARCHAR ENUM(n) VARCHAR(n) BINARY, VARBINARY, TINYBLOB, BLOB, MEDIUMBLOB, LONGBLOB VARBINARY JSON JSON DATE DATE TIME(n) TIME(n) DATETIME(n) TIMESTAMP(n) TIMESTAMP(n) TIMESTAMP(n) WITH TIME ZONE No other types are supported. Trino to MySQL type mapping# The connector maps Trino types to the corresponding MySQL types following this table: Trino to MySQL type mapping# Trino type MySQL type Notes BOOLEAN TINYINT TINYINT TINYINT SMALLINT SMALLINT INTEGER INTEGER BIGINT BIGINT REAL REAL DOUBLE DOUBLE PRECISION DECIMAL(p, s) DECIMAL(p, s) MySQL DECIMAL type handling CHAR(n) CHAR(n) VARCHAR(n) VARCHAR(n) JSON JSON DATE DATE TIME(n) TIME(n) TIMESTAMP(n) DATETIME(n) TIMESTAMP(n) WITH TIME ZONE TIMESTAMP(n) No other types are supported. Timestamp type handling# MySQL TIMESTAMP types are mapped to Trino TIMESTAMP WITH TIME ZONE. To preserve time instants, Trino sets the session time zone of the MySQL connection to match the JVM time zone. As a result, error messages similar to the following example occur when a timezone from the JVM does not exist on the MySQL server: com.mysql.cj.exceptions.CJException: Unknown or incorrect time zone: 'UTC' To avoid the errors, you must use a time zone that is known on both systems, or install the missing time zone on the MySQL server. Decimal type handling# DECIMAL types with unspecified precision or scale are ignored unless the decimal-mapping configuration property or the decimal_mapping session property is set to allow_overflow. Then such types are mapped to a Trino DECIMAL with a default precision of 38 and default scale of 0. To change the scale of the resulting type, use the decimal-default-scale configuration property or the decimal_default_scale session property. The precision is always 38. By default, values that require rounding or truncation to fit will cause a failure at runtime. This behavior is controlled via the decimal-rounding-mode configuration property or the decimal_rounding_mode session property, which can be set to UNNECESSARY (the default), UP, DOWN, CEILING, FLOOR, HALF_UP, HALF_DOWN, or HALF_EVEN (see RoundingMode). Type mapping configuration properties# The following properties can be used to configure how data types from the connected data source are mapped to Trino data types and how the metadata is cached in Trino. Property name Description Default value unsupported-type-handling Configure how unsupported column data types are handled: IGNORE, column is not accessible. CONVERT_TO_VARCHAR, column is converted to unbounded VARCHAR. The respective catalog session property is unsupported_type_handling. IGNORE jdbc-types-mapped-to-varchar Allow forced mapping of comma separated lists of data types to convert to unbounded VARCHAR Querying MySQL# The MySQL connector provides a schema for every MySQL database. You can see the available MySQL databases by running SHOW SCHEMAS: SHOW SCHEMAS FROM example; If you have a MySQL database named web, you can view the tables in this database by running SHOW TABLES: SHOW TABLES FROM example.web; You can see a list of the columns in the clicks table in the web database using either of the following: DESCRIBE example.web.clicks; SHOW COLUMNS FROM example.web.clicks; Finally, you can access the clicks table in the web database: SELECT * FROM example.web.clicks; If you used a different name for your catalog properties file, use that catalog name instead of example in the above examples. SQL support# The connector provides read access and write access to data and metadata in the MySQL database. In addition to the globally available and read operation statements, the connector supports the following features: INSERT, see also Non-transactional INSERT UPDATE, see also UPDATE limitation DELETE, see also DELETE limitation MERGE, see also Non-transactional MERGE TRUNCATE CREATE TABLE CREATE TABLE AS DROP TABLE CREATE SCHEMA DROP SCHEMA Procedures Table functions Non-transactional INSERT# The connector supports adding rows using INSERT statements. By default, data insertion is performed by writing data to a temporary table. You can skip this step to improve performance and write directly to the target table. Set the insert.non-transactional-insert.enabled catalog property or the corresponding non_transactional_insert catalog session property to true. Note that with this property enabled, data can be corrupted in rare cases where exceptions occur during the insert operation. With transactions disabled, no rollback can be performed. UPDATE limitation# Only UPDATE statements with constant assignments and predicates are supported. For example, the following statement is supported because the values assigned are constants: UPDATE table SET col1 = 1 WHERE col3 = 1 Arithmetic expressions, function calls, and other non-constant UPDATE statements are not supported. For example, the following statement is not supported because arithmetic expressions cannot be used with the SET command: UPDATE table SET col1 = col2 + 2 WHERE col3 = 1 All column values of a table row cannot be updated simultaneously. For a three column table, the following statement is not supported: UPDATE table SET col1 = 1, col2 = 2, col3 = 3 WHERE col3 = 1 DELETE limitation# If a WHERE clause is specified, the DELETE operation only works if the predicate in the clause can be fully pushed down to the data source. Non-transactional MERGE# The connector supports adding, updating, and deleting rows using MERGE statements, if the merge.non-transactional-merge.enabled catalog property or the corresponding non_transactional_merge_enabled catalog session property is set to true. Merge is only supported for directly modifying target tables. In rare cases, exceptions may occur during the merge operation, potentially resulting in a partial update. Procedures# system.flush_metadata_cache()# Flush JDBC metadata caches. For example, the following system call flushes the metadata caches for all schemas in the example catalog USE example.example_schema; CALL system.flush_metadata_cache(); system.execute('query')# The execute procedure allows you to execute a query in the underlying data source directly. The query must use supported syntax of the connected data source. Use the procedure to access features which are not available in Trino or to execute queries that return no result set and therefore can not be used with the query or raw_query pass-through table function. Typical use cases are statements that create or alter objects, and require native feature such as constraints, default values, automatic identifier creation, or indexes. Queries can also invoke statements that insert, update, or delete data, and do not return any data as a result. The query text is not parsed by Trino, only passed through, and therefore only subject to any security or access control of the underlying data source. The following example sets the current database to the example_schema of the example catalog. Then it calls the procedure in that schema to drop the default value from your_column on your_table table using the standard SQL syntax in the parameter value assigned for query: USE example.example_schema; CALL system.execute(query => 'ALTER TABLE your_table ALTER COLUMN your_column DROP DEFAULT'); Verify that the specific database supports this syntax, and adapt as necessary based on the documentation for the specific connected database and database version. Table functions# The connector provides specific table functions to access MySQL. query(varchar) -> table# The query function allows you to query the underlying database directly. It requires syntax native to MySQL, because the full query is pushed down and processed in MySQL. This can be useful for accessing native features which are not available in Trino or for improving query performance in situations where running a query natively may be faster. The native query passed to the underlying data source is required to return a table as a result set. Only the data source performs validation or security checks for these queries using its own configuration. Trino does not perform these tasks. Only use passthrough queries to read data. For example, query the example catalog and group and concatenate all employee IDs by manager ID: SELECT * FROM TABLE( example.system.query( query => 'SELECT manager_id, GROUP_CONCAT(employee_id) FROM company.employees GROUP BY manager_id' ) ); Note The query engine does not preserve the order of the results of this function. If the passed query contains an ORDER BY clause, the function result may not be ordered as expected. Performance# The connector includes a number of performance improvements, detailed in the following sections. Table statistics# The MySQL connector can use table and column statistics for cost based optimizations, to improve query processing performance based on the actual data in the data source. The statistics are collected by MySQL and retrieved by the connector. The table-level statistics are based on MySQL’s INFORMATION_SCHEMA.TABLES table. The column-level statistics are based on MySQL’s index statistics INFORMATION_SCHEMA.STATISTICS table. The connector can return column-level statistics only when the column is the first column in some index. MySQL database can automatically update its table and index statistics. In some cases, you may want to force statistics update, for example after creating new index, or after changing data in the table. You can do that by executing the following statement in MySQL Database. ANALYZE TABLE table_name; Note MySQL and Trino may use statistics information in different ways. For this reason, the accuracy of table and column statistics returned by the MySQL connector might be lower than than that of others connectors. Improving statistics accuracy You can improve statistics accuracy with histogram statistics (available since MySQL 8.0). To create histogram statistics execute the following statement in MySQL Database. ANALYZE TABLE table_name UPDATE HISTOGRAM ON column_name1, column_name2, ...; Refer to MySQL documentation for information about options, limitations and additional considerations. Pushdown# The connector supports pushdown for a number of operations: Join pushdown Limit pushdown Top-N pushdown Aggregate pushdown for the following functions: avg() count() max() min() sum() stddev() stddev_pop() stddev_samp() variance() var_pop() var_samp() Note The connector performs pushdown where performance may be improved, but in order to preserve correctness an operation may not be pushed down. When pushdown of an operation may result in better performance but risks correctness, the connector prioritizes correctness. Cost-based join pushdown# The connector supports cost-based Join pushdown to make intelligent decisions about whether to push down a join operation to the data source. When cost-based join pushdown is enabled, the connector only pushes down join operations if the available Table statistics suggest that doing so improves performance. Note that if no table statistics are available, join operation pushdown does not occur to avoid a potential decrease in query performance. The following table describes catalog configuration properties for join pushdown: Property name Description Default value join-pushdown.enabled Enable join pushdown. Equivalent catalog session property is join_pushdown_enabled. true join-pushdown.strategy Strategy used to evaluate whether join operations are pushed down. Set to AUTOMATIC to enable cost-based join pushdown, or EAGER to push down joins whenever possible. Note that EAGER can push down joins even when table statistics are unavailable, which may result in degraded query performance. Because of this, EAGER is only recommended for testing and troubleshooting purposes. AUTOMATIC Predicate pushdown support# The connector does not support pushdown of any predicates on columns with textual types like CHAR or VARCHAR. This ensures correctness of results since the data source may compare strings case-insensitively. In the following example, the predicate is not pushed down for either query since name is a column of type VARCHAR: SELECT * FROM nation WHERE name > 'CANADA'; SELECT * FROM nation WHERE name = 'CANADA';

#### Code Examples

```
connector.name=mysql
connection-url=jdbc:mysql://example.net:3306
connection-user=root
connection-password=secret
```
```
connection-url=jdbc:mysql://example.net:3306?sslMode=REQUIRED
```
```
connection-url=jdbc:mysql://example.net:3306/?sslMode=REQUIRED
```
```
query.comment-format=Query sent by Trino.
```
```
SELECT * FROM example_table; /*Query sent by Trino.*/
```
```
query.comment-format=Query $QUERY_ID sent by user $USER from Trino.
```
```
SELECT * FROM example_table; /*Query 20230622_180528_00000_bkizg sent by user Jane from Trino.*/
```
```
{
  "schemas": [
    {
      "remoteSchema": "CaseSensitiveName",
      "mapping": "case_insensitive_1"
    },
    {
      "remoteSchema": "cASEsENSITIVEnAME",
      "mapping": "case_insensitive_2"
    }],
  "tables": [
    {
      "remoteSchema": "CaseSensitiveName",
      "remoteTable": "tablex",
      "mapping": "table_1"
    },
    {
      "remoteSchema": "CaseSensitiveName",
      "remoteTable": "TABLEX",
      "mapping": "table_2"
    }]
}
```
```
case-insensitive-name-matching.config-file.refresh-period=30s
```
```
com.mysql.cj.exceptions.CJException: Unknown or incorrect time zone: 'UTC'
```
```
SHOW SCHEMAS FROM example;
```
```
SHOW TABLES FROM example.web;
```
```
DESCRIBE example.web.clicks;
SHOW COLUMNS FROM example.web.clicks;
```
```
SELECT * FROM example.web.clicks;
```
```
UPDATE table SET col1 = 1 WHERE col3 = 1
```
```
UPDATE table SET col1 = col2 + 2 WHERE col3 = 1
```
```
UPDATE table SET col1 = 1, col2 = 2, col3 = 3 WHERE col3 = 1
```
```
USE example.example_schema;
CALL system.flush_metadata_cache();
```
```
USE example.example_schema;
CALL system.execute(query => 'ALTER TABLE your_table ALTER COLUMN your_column DROP DEFAULT');
```
```
SELECT
  *
FROM
  TABLE(
    example.system.query(
      query => 'SELECT
        manager_id, GROUP_CONCAT(employee_id)
      FROM
        company.employees
      GROUP BY
        manager_id'
    )
  );
```
```
ANALYZE TABLE table_name;
```
```
ANALYZE TABLE table_name UPDATE HISTOGRAM ON column_name1, column_name2, ...;
```
```
SELECT * FROM nation WHERE name > 'CANADA';
SELECT * FROM nation WHERE name = 'CANADA';
```


---

### MongoDB Connector Documentation
Source: https://trino.io/docs/current/connector/mongodb.html

MongoDB connector# The mongodb connector allows the use of MongoDB collections as tables in Trino. Requirements# To connect to MongoDB, you need: MongoDB 4.2 or higher. Network access from the Trino coordinator and workers to MongoDB. Port 27017 is the default port. Write access to the schema information collection in MongoDB. Configuration# To configure the MongoDB connector, create a catalog properties file etc/catalog/example.properties with the following contents, replacing the properties as appropriate: connector.name=mongodb mongodb.connection-url=mongodb://user:[email protected]:27017/ Multiple MongoDB clusters# You can have as many catalogs as you need, so if you have additional MongoDB clusters, simply add another properties file to etc/catalog with a different name, making sure it ends in .properties). For example, if you name the property file sales.properties, Trino will create a catalog named sales using the configured connector. Configuration properties# The following configuration properties are available: Property name Description mongodb.connection-url The connection url that the driver uses to connect to a MongoDB deployment mongodb.schema-collection A collection which contains schema information mongodb.case-insensitive-name-matching Match database and collection names case insensitively mongodb.min-connections-per-host The minimum size of the connection pool per host mongodb.connections-per-host The maximum size of the connection pool per host mongodb.max-wait-time The maximum wait time mongodb.max-connection-idle-time The maximum idle time of a pooled connection mongodb.connection-timeout The socket connect timeout mongodb.socket-timeout The socket timeout mongodb.tls.enabled Use TLS/SSL for connections to mongod/mongos mongodb.tls.keystore-path Path to the or JKS key store mongodb.tls.truststore-path Path to the or JKS trust store mongodb.tls.keystore-password Password for the key store mongodb.tls.truststore-password Password for the trust store mongodb.read-preference The read preference mongodb.write-concern The write concern mongodb.required-replica-set The required replica set name mongodb.cursor-batch-size The number of elements to return in a batch mongodb.allow-local-scheduling Assign MongoDB splits to a specific worker mongodb.dynamic-filtering.wait-timeout Duration to wait for completion of dynamic filters during split generation mongodb.connection-url# A connection string containing the protocol, credential, and host info for use inconnection to your MongoDB deployment. For example, the connection string may use the format mongodb://<user>:<pass>@<host>:<port>/?<options> or mongodb+srv://<user>:<pass>@<host>/?<options>, depending on the protocol used. The user/pass credentials must be for a user with write access to the schema information collection. See the MongoDB Connection URI for more information. This property is required; there is no default. A connection URL must be provided to connect to a MongoDB deployment. mongodb.schema-collection# As MongoDB is a document database, there is no fixed schema information in the system. So a special collection in each MongoDB database should define the schema of all tables. Please refer the Table definition section for the details. At startup, the connector tries to guess the data type of fields based on the type mapping. The initial guess can be incorrect for your specific collection. In that case, you need to modify it manually. Please refer the Table definition section for the details. Creating new tables using CREATE TABLE and CREATE TABLE AS SELECT automatically create an entry for you. This property is optional; the default is _schema. mongodb.case-insensitive-name-matching# Match database and collection names case insensitively. This property is optional; the default is false. mongodb.min-connections-per-host# The minimum number of connections per host for this MongoClient instance. Those connections are kept in a pool when idle, and the pool ensures over time that it contains at least this minimum number. This property is optional; the default is 0. mongodb.connections-per-host# The maximum number of connections allowed per host for this MongoClient instance. Those connections are kept in a pool when idle. Once the pool is exhausted, any operation requiring a connection blocks waiting for an available connection. This property is optional; the default is 100. mongodb.max-wait-time# The maximum wait time in milliseconds, that a thread may wait for a connection to become available. A value of 0 means that it does not wait. A negative value means to wait indefinitely for a connection to become available. This property is optional; the default is 120000. mongodb.max-connection-idle-time# The maximum idle time of a pooled connection in milliseconds. A value of 0 indicates no limit to the idle time. A pooled connection that has exceeded its idle time will be closed and replaced when necessary by a new connection. This property is optional; the default is 0. mongodb.connection-timeout# The connection timeout in milliseconds. A value of 0 means no timeout. It is used solely when establishing a new connection. This property is optional; the default is 10000. mongodb.socket-timeout# The socket timeout in milliseconds. It is used for I/O socket read and write operations. This property is optional; the default is 0 and means no timeout. mongodb.tls.enabled# This flag enables TLS connections to MongoDB servers. This property is optional; the default is false. mongodb.tls.keystore-path# The path to the PEM or JKS key store. This property is optional. mongodb.tls.truststore-path# The path to PEM or JKS trust store. This property is optional. mongodb.tls.keystore-password# The key password for the key store specified by mongodb.tls.keystore-path. This property is optional. mongodb.tls.truststore-password# The key password for the trust store specified by mongodb.tls.truststore-path. This property is optional. mongodb.read-preference# The read preference to use for queries, map-reduce, aggregation, and count. The available values are PRIMARY, PRIMARY_PREFERRED, SECONDARY, SECONDARY_PREFERRED and NEAREST. This property is optional; the default is PRIMARY. mongodb.write-concern# The write concern to use. The available values are ACKNOWLEDGED, JOURNALED, MAJORITY and UNACKNOWLEDGED. This property is optional; the default is ACKNOWLEDGED. mongodb.required-replica-set# The required replica set name. With this option set, the MongoClient instance performs the following actions: #. Connect in replica set mode, and discover all members of the set based on the given servers #. Make sure that the set name reported by all members matches the required set name. #. Refuse to service any requests, if authenticated user is not part of a replica set with the required name. This property is optional; no default value. mongodb.cursor-batch-size# Limits the number of elements returned in one batch. A cursor typically fetches a batch of result objects and stores them locally. If batchSize is 0, Driver’s default are used. If batchSize is positive, it represents the size of each batch of objects retrieved. It can be adjusted to optimize performance and limit data transfer. If batchSize is negative, it limits the number of objects returned, that fit within the max batch size limit (usually 4MB), and the cursor is closed. For example if batchSize is -10, then the server returns a maximum of 10 documents, and as many as can fit in 4MB, then closes the cursor. Note Do not use a batch size of 1. This property is optional; the default is 0. mongodb.allow-local-scheduling# Set the value of this property to true if Trino and MongoDB share the same cluster, and specific MongoDB splits should be processed on the same worker and MongoDB node. Note that a shared deployment is not recommended, and enabling this property can lead to resource contention. This property is optional, and defaults to false. mongodb.dynamic-filtering.wait-timeout# Duration to wait for completion of dynamic filters during split generation. This property is optional; the default is 5s. Table definition# MongoDB maintains table definitions on the special collection where mongodb.schema-collection configuration value specifies. Note The plugin cannot detect that a collection has been deleted. You must delete the entry by executing db.getCollection("_schema").remove( { table: deleted_table_name }) in the MongoDB Shell. You can also drop a collection in Trino by running DROP TABLE table_name. A schema collection consists of a MongoDB document for a table. { "table": ..., "fields": [ { "name" : ..., "type" : "varchar|bigint|boolean|double|date|array(bigint)|...", "hidden" : false }, ... ] } } The connector quotes the fields for a row type when auto-generating the schema; however, the auto-generated schema must be corrected manually in the collection to match the information in the tables. Manually altered fields must be explicitly quoted, for example, row("UpperCase" varchar). Field Required Type Description table required string Trino table name fields required array A list of field definitions. Each field definition creates a new column in the Trino table. Each field definition: { "name": ..., "type": ..., "hidden": ... } Field Required Type Description name required string Name of the column in the Trino table. type required string Trino type of the column. hidden optional boolean Hides the column from DESCRIBE <table name> and SELECT *. Defaults to false. There is no limit on field descriptions for either key or message. ObjectId# MongoDB collection has the special field _id. The connector tries to follow the same rules for this special field, so there will be hidden field _id. CREATE TABLE IF NOT EXISTS orders ( orderkey BIGINT, orderstatus VARCHAR, totalprice DOUBLE, orderdate DATE ); INSERT INTO orders VALUES(1, 'bad', 50.0, current_date); INSERT INTO orders VALUES(2, 'good', 100.0, current_date); SELECT _id, * FROM orders; _id | orderkey | orderstatus | totalprice | orderdate -------------------------------------+----------+-------------+------------+------------ 55 b1 51 63 38 64 d6 43 8c 61 a9 ce | 1 | bad | 50.0 | 2015-07-23 55 b1 51 67 38 64 d6 43 8c 61 a9 cf | 2 | good | 100.0 | 2015-07-23 (2 rows) SELECT _id, * FROM orders WHERE _id = ObjectId('55b151633864d6438c61a9ce'); _id | orderkey | orderstatus | totalprice | orderdate -------------------------------------+----------+-------------+------------+------------ 55 b1 51 63 38 64 d6 43 8c 61 a9 ce | 1 | bad | 50.0 | 2015-07-23 (1 row) You can render the _id field to readable values with a cast to VARCHAR: SELECT CAST(_id AS VARCHAR), * FROM orders WHERE _id = ObjectId('55b151633864d6438c61a9ce'); _id | orderkey | orderstatus | totalprice | orderdate ---------------------------+----------+-------------+------------+------------ 55b151633864d6438c61a9ce | 1 | bad | 50.0 | 2015-07-23 (1 row) ObjectId timestamp functions# The first four bytes of each ObjectId represent an embedded timestamp of its creation time. Trino provides a couple of functions to take advantage of this MongoDB feature. objectid_timestamp(ObjectId) → timestamp# Extracts the TIMESTAMP WITH TIME ZONE from a given ObjectId: SELECT objectid_timestamp(ObjectId('507f191e810c19729de860ea')); -- 2012-10-17 20:46:22.000 UTC timestamp_objectid(timestamp) → ObjectId# Creates an ObjectId from a TIMESTAMP WITH TIME ZONE: SELECT timestamp_objectid(TIMESTAMP '2021-08-07 17:51:36 +00:00'); -- 61 0e c8 28 00 00 00 00 00 00 00 00 In MongoDB, you can filter all the documents created after 2021-08-07 17:51:36 with a query like this: db.collection.find({"_id": {"$gt": ObjectId("610ec8280000000000000000")}}) In Trino, the same can be achieved with this query: SELECT * FROM collection WHERE _id > timestamp_objectid(TIMESTAMP '2021-08-07 17:51:36 +00:00'); Fault-tolerant execution support# The connector supports Fault-tolerant execution of query processing. Read and write operations are both supported with any retry policy. Type mapping# Because Trino and MongoDB each support types that the other does not, this connector modifies some types when reading or writing data. Data types may not map the same way in both directions between Trino and the data source. Refer to the following sections for type mapping in each direction. MongoDB to Trino type mapping# The connector maps MongoDB types to the corresponding Trino types following this table: MongoDB to Trino type mapping# MongoDB type Trino type Notes Boolean BOOLEAN Int32 BIGINT Int64 BIGINT Double DOUBLE Decimal128 DECIMAL(p, s) Date TIMESTAMP(3) String VARCHAR Binary VARBINARY ObjectId ObjectId Object ROW Array ARRAY Map to ROW if the element type is not unique. DBRef ROW No other types are supported. Trino to MongoDB type mapping# The connector maps Trino types to the corresponding MongoDB types following this table: Trino to MongoDB type mapping# Trino type MongoDB type BOOLEAN Boolean BIGINT Int64 DOUBLE Double DECIMAL(p, s) Decimal128 TIMESTAMP(3) Date VARCHAR String VARBINARY Binary ObjectId ObjectId ROW Object ARRAY Array No other types are supported. SQL support# The connector provides read and write access to data and metadata in MongoDB. In addition to the globally available and read operation statements, the connector supports the following features: INSERT DELETE CREATE TABLE CREATE TABLE AS DROP TABLE ALTER TABLE CREATE SCHEMA DROP SCHEMA COMMENT ALTER TABLE# The connector supports ALTER TABLE RENAME TO, ALTER TABLE ADD COLUMN and ALTER TABLE DROP COLUMN operations. Other uses of ALTER TABLE are not supported. Table functions# The connector provides specific table functions to access MongoDB. query(database, collection, filter) -> table# The query function allows you to query the underlying MongoDB directly. It requires syntax native to MongoDB, because the full query is pushed down and processed by MongoDB. This can be useful for accessing native features which are not available in Trino or for improving query performance in situations where running a query natively may be faster. For example, get all rows where regionkey field is 0: SELECT * FROM TABLE( example.system.query( database => 'tpch', collection => 'region', filter => '{ regionkey: 0 }' ) );

#### Code Examples

```
connector.name=mongodb
mongodb.connection-url=mongodb://user:[email protected]:27017/
```
```
#. Connect in replica set mode, and discover all members of the set based on the given servers
#. Make sure that the set name reported by all members matches the required set name.
#. Refuse to service any requests, if authenticated user is not part of a replica set with the required name.
```
```
{
    "table": ...,
    "fields": [
          { "name" : ...,
            "type" : "varchar|bigint|boolean|double|date|array(bigint)|...",
            "hidden" : false },
            ...
        ]
    }
}
```
```
{
    "name": ...,
    "type": ...,
    "hidden": ...
}
```
```
CREATE TABLE IF NOT EXISTS orders (
    orderkey BIGINT,
    orderstatus VARCHAR,
    totalprice DOUBLE,
    orderdate DATE
);

INSERT INTO orders VALUES(1, 'bad', 50.0, current_date);
INSERT INTO orders VALUES(2, 'good', 100.0, current_date);
SELECT _id, * FROM orders;
```
```
_id                 | orderkey | orderstatus | totalprice | orderdate
-------------------------------------+----------+-------------+------------+------------
 55 b1 51 63 38 64 d6 43 8c 61 a9 ce |        1 | bad         |       50.0 | 2015-07-23
 55 b1 51 67 38 64 d6 43 8c 61 a9 cf |        2 | good        |      100.0 | 2015-07-23
(2 rows)
```
```
SELECT _id, * FROM orders WHERE _id = ObjectId('55b151633864d6438c61a9ce');
```
```
_id                 | orderkey | orderstatus | totalprice | orderdate
-------------------------------------+----------+-------------+------------+------------
 55 b1 51 63 38 64 d6 43 8c 61 a9 ce |        1 | bad         |       50.0 | 2015-07-23
(1 row)
```
```
SELECT CAST(_id AS VARCHAR), * FROM orders WHERE _id = ObjectId('55b151633864d6438c61a9ce');
```
```
_id             | orderkey | orderstatus | totalprice | orderdate
---------------------------+----------+-------------+------------+------------
 55b151633864d6438c61a9ce  |        1 | bad         |       50.0 | 2015-07-23
(1 row)
```
```
SELECT objectid_timestamp(ObjectId('507f191e810c19729de860ea'));
-- 2012-10-17 20:46:22.000 UTC
```
```
SELECT timestamp_objectid(TIMESTAMP '2021-08-07 17:51:36 +00:00');
-- 61 0e c8 28 00 00 00 00 00 00 00 00
```
```
db.collection.find({"_id": {"$gt": ObjectId("610ec8280000000000000000")}})
```
```
SELECT *
FROM collection
WHERE _id > timestamp_objectid(TIMESTAMP '2021-08-07 17:51:36 +00:00');
```
```
SELECT
  *
FROM
  TABLE(
    example.system.query(
      database => 'tpch',
      collection => 'region',
      filter => '{ regionkey: 0 }'
    )
  );
```


---


## Usage Examples

### Tutorial: Getting Started with Trino and MySQL
Source: https://github.com/bitsondatadev/trino-getting-started/blob/main/community-tutorials/mysql/trino-mysql/README.md

bitsondatadev / trino-getting-started Public Notifications You must be signed in to change notification settings Fork 101 Star 260 Files main/README.mdCopy path Blame Blame Latest commit HistoryHistory129 lines (100 loc) · 5.25 KB main/README.mdTopFile metadata and controlsPreviewCodeBlame129 lines (100 loc) · 5.25 KBRawRunning MySQL and configuring the MySQL Connector Introduction This tutorial will cover running the MySQL connector Goals In this tutorial, you will: Learn how to run a CTAS (CREATE TABLE AS) statement in Trino. Run a query against the underlying system. Steps Running Services First, you want to start the services. Make sure that you are in the trino-getting-started/mysql/trino-mysql directory. Now run the following command: docker compose up -d You should expect to see the following output (you may also have to download the Docker images before you see the "done" message): Creating network "trino-mysql_trino-network" with driver "bridge" Creating network "trino-mysql_default" with the default driver Creating volume "trino-mysql_mysql-storage" with default driver Creating trino-mysql_trino-coordinator_1 ... done Creating trino-mysql_mysql_1 ... done Open Trino CLI Once this is complete, you can log into the Trino coordinator node. We will do this by using the exec command and run the trino CLI executable as the command we run on that container. Notice the container id is trino-mysql_trino-coordinator_1 so the command you will run is: docker container exec -it trino-mysql-trino-coordinator-1 trino When you start this step, you should see the trino cursor once the startup is complete. It should look like this when it is done: trino> To insert data, run a CTAS (CREATE TABLE AS) query that pushes data from one of the TPC connectors into the hive catalog that points to MySQL. The TPC connectors generate data on the fly so that we can run simple tests like this. First, run a command to show the catalogs to see the tpch and mysql catalogs since these are what we will use in the CTAS query. SHOW CATALOGS; You should see that the mysql catalog is available. Querying Trino If you are familiar with MySQL, you are likely to know that MySQL supports a two-tiered containment hierarchy, though you may have never known it was called that. This containment hierarchy refers to databases and tables. The first tier of the hierarchy are the tables, while the second tier consists of databases. A database contains multiple tables and therefore two tables can have the same name provided they live under a different database. Since Trino has to connect to multiple databases, it supports a three-tiered containment hierarchy. Rather than call the second tier as databases, Trino refers to this tier as schemas. So a database in MySQL is equivalent to a schema in Trino. The third tier that allows Trino to distinguish between multiple underlying data sources is called a catalog. In our case, since the file we provide trino is called etc/catalog/mysql.properties it automatically names the catalog mysql without the .properties file type. There was a database created in MySQL on creation of the container by setting the MYSQL_DATABASE envoironment variable to tiny. Therefore you should be able to run the following command and see that there is a tiny database, or schema as it's referred to by Trino. SHOW SCHEMAS in mysql; Now that we know the name of the schema that will hold our table, we now can create our first table. Optional: To view your queries run, log into the Trino UI and log in using any username (it doesn't matter since no security is set up). Move the customer data from the tiny generated tpch data into MySQL using a CTAS query. Run the following query and if you like, watch it running on the Trino UI: CREATE TABLE mysql.tiny.customer AS SELECT * FROM tpch.tiny.customer; Now there is a table under MySQL, you can query this data by checking the following. SELECT * FROM mysql.tiny.customer LIMIT 5; The results should look like this: trino> SELECT * FROM mysql.tiny.customer LIMIT 5; custkey | name | address | nationkey | phone | acctbal | mktsegment | ---------+--------------------+----------------------------------------+-----------+-----------------+---------+------------+--------------------------- 1126 | Customer#000001126 | 8J bzLWboPqySAWPgHrl4IK4roBvb | 8 | 18-898-994-6389 | 3905.97 | AUTOMOBILE | se carefully asymptotes. u 1127 | Customer#000001127 | nq1w3VhKie4I3ZquEIZuz1 5CWn | 10 | 20-830-875-6204 | 8631.35 | AUTOMOBILE | endencies. express instruc 1128 | Customer#000001128 | 72XUL0qb4,NLmfyrtzyJlR0eP | 0 | 10-392-200-8982 | 8123.99 | BUILDING | odolites according to the 1129 | Customer#000001129 | OMEqYv,hhyBAObDjIkoPL03BvuSRw02AuDPVoe | 8 | 18-313-585-9420 | 6020.02 | HOUSEHOLD | pades affix realms. pendin 1130 | Customer#000001130 | 60zzrBpFXjvHzyv0WObH3h8LhYbOaRID58e | 22 | 32-503-721-8203 | 9519.36 | HOUSEHOLD | s requests nag silently ca (5 rows) So now you have a basic working Trino and MySQL instance up and running. From here you can read more about the Trino MySQL Connector to learn more about the capabilities and limitations of this connector. See trademark and other legal notices.

#### Code Examples

```
docker compose up -d
```
```
Creating network "trino-mysql_trino-network" with driver "bridge"
Creating network "trino-mysql_default" with the default driver
Creating volume "trino-mysql_mysql-storage" with default driver
Creating trino-mysql_trino-coordinator_1 ... done
Creating trino-mysql_mysql_1             ... done
```
```
docker container exec -it trino-mysql-trino-coordinator-1 trino
```
```
trino>
```
```
SHOW CATALOGS;
```
```
SHOW SCHEMAS in mysql;
```
```
CREATE TABLE mysql.tiny.customer
AS SELECT * FROM tpch.tiny.customer;
```
```
SELECT * FROM mysql.tiny.customer LIMIT 5;
```
```
trino> SELECT * FROM mysql.tiny.customer LIMIT 5;
 custkey |        name        |                address                 | nationkey |      phone      | acctbal | mktsegment |
---------+--------------------+----------------------------------------+-----------+-----------------+---------+------------+---------------------------
    1126 | Customer#000001126 | 8J bzLWboPqySAWPgHrl4IK4roBvb          |         8 | 18-898-994-6389 | 3905.97 | AUTOMOBILE | se carefully asymptotes. u
    1127 | Customer#000001127 | nq1w3VhKie4I3ZquEIZuz1 5CWn            |        10 | 20-830-875-6204 | 8631.35 | AUTOMOBILE | endencies. express instruc
    1128 | Customer#000001128 | 72XUL0qb4,NLmfyrtzyJlR0eP              |         0 | 10-392-200-8982 | 8123.99 | BUILDING   | odolites according to the
    1129 | Customer#000001129 | OMEqYv,hhyBAObDjIkoPL03BvuSRw02AuDPVoe |         8 | 18-313-585-9420 | 6020.02 | HOUSEHOLD  | pades affix realms. pendin
    1130 | Customer#000001130 | 60zzrBpFXjvHzyv0WObH3h8LhYbOaRID58e    |        22 | 32-503-721-8203 | 9519.36 | HOUSEHOLD  | s requests nag silently ca
(5 rows)
```


---
